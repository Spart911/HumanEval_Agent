import os
import ast
import torch
import networkx as nx
import subprocess
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from peft import LoraConfig, get_peft_model
from torch.utils.data import Dataset
import gc

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ parallelism
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ---------------- 1. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ----------------
LOCAL_MODEL_DIR = "/home/nyuroprint/Jupyter/Qwen2.5-Coder-3B"  # –ü–∞–ø–∫–∞ —Å —É–∂–µ —Å–∫–∞—á–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
MAX_LEN = 512
EPOCHS = 2
BATCH_SIZE = 1
LR = 5e-6
OUTPUT_DIR = "../../ast_feedback_trained_github"
LIMIT = 10000

REPOS = [
    "https://github.com/psf/requests.git",
    "https://github.com/pallets/flask.git",
    "https://github.com/pandas-dev/pandas.git",
    "https://github.com/numpy/numpy.git",
    "https://github.com/scipy/scipy.git",
    "https://github.com/scikit-learn/scikit-learn.git",
    "https://github.com/matplotlib/matplotlib.git",
    "https://github.com/plotly/plotly.py.git",
    "https://github.com/pytorch/pytorch.git",
    "https://github.com/tensorflow/tensorflow.git"
]

CLONE_DIR = "../../github_repos"
os.makedirs(CLONE_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ
if not os.path.exists(LOCAL_MODEL_DIR):
    print(f"‚ùå –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –ø–∞–ø–∫–µ: {LOCAL_MODEL_DIR}")
    print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞ –≤ —ç—Ç—É –ø–∞–ø–∫—É")
    exit(1)
else:
    print(f"‚úÖ –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞ –≤: {LOCAL_MODEL_DIR}")

# ---------------- 2. –ö–ª–æ–Ω–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ ----------------
print("üìÅ –ö–ª–æ–Ω–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏...")
for repo in REPOS:
    repo_name = repo.split("/")[-1].replace(".git", "")
    dest = os.path.join(CLONE_DIR, repo_name)
    if not os.path.exists(dest):
        print(f"Cloning {repo} ‚Ä¶")
        try:
            result = subprocess.run(
                ["git", "clone", "--depth", "1", repo, dest],
                capture_output=True,
                text=True,
                timeout=300
            )
            if result.returncode != 0:
                print(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ —Å –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º {repo}: {result.stderr}")
            else:
                print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω {repo}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è {repo}: {e}")

# ---------------- 3. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ DFG ----------------


def extract_ast_sequence(code: str) -> str:
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç Python-–∫–æ–¥ –≤ –ª–∏–Ω–µ–π–Ω–æ–µ AST-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
    """
    try:
        tree = ast.parse(code)
        return ast.dump(tree, annotate_fields=True, include_attributes=False)
    except SyntaxError:
        return ""
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ AST: {e}")
        return ""




def visualize_ast_debug(code: str, filename: str):
    """
    –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –ø–µ—á–∞—Ç—å AST –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
    """
    try:
        tree = ast.parse(code)
        print(f"\n AST –¥–ª—è {filename}:")
        print(ast.dump(tree, indent=2, annotate_fields=True, include_attributes=False)[:500])
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ AST: {e}")


# ---------------- 4.1. –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –∫–æ–º–ø–∏–ª—è—Ç–æ—Ä–∞/–∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è ----------------

def get_compiler_feedback(path: str, run_timeout: int = 5) -> str:
    """
    –ü—ã—Ç–∞–µ–º—Å—è —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞—Ç—å –∏ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∫–æ–¥.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫—Ä–∞—Ç–∫—É—é —Ç–µ–∫—Å—Ç–æ–≤—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.
    –§–æ—Ä–º–∞—Ç: –∫–ª—é—á: —Å–æ–æ–±—â–µ–Ω–∏–µ (–±–µ–∑ –¥–ª–∏–Ω–Ω—ã—Ö —Å—Ç–µ–∫-—Ç—Ä–µ–π—Å–æ–≤, —Å—Ç–∞—Ä–∞–µ–º—Å—è —É–∫–æ—Ä–æ—Ç–∏—Ç—å).
    """
    parts = []

    # 1) –ö–æ–º–ø–∏–ª—è—Ü–∏—è –≤ –±–∞–π—Ç–∫–æ–¥ (py_compile)
    try:
        compile_proc = subprocess.run(
            ["python3", "-m", "py_compile", path],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=run_timeout
        )
        stderr = compile_proc.stderr.strip()
        if stderr:
            # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Ç—Ä–∞—Å—Å—ã ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—ã–µ 2 —Å—Ç—Ä–æ–∫–∏
            lines = stderr.splitlines()
            brief = " | ".join(lines[:2])
            parts.append(f"COMPILE_ERROR:{brief}")
    except subprocess.TimeoutExpired:
        parts.append("COMPILE_TIMEOUT")
    except Exception as e:
        parts.append(f"COMPILE_CRASH:{str(e)}")

    # 2) –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–ø—É—Å–∫–∞ (–≤ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ)
    try:
        run_proc = subprocess.run(
            ["python3", path],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=run_timeout
        )
        stdout = run_proc.stdout.strip()
        stderr = run_proc.stderr.strip()
        if stdout:
            s = stdout.replace("\n", " \\n ")
            # —É–∫–æ—Ä–æ—Ç–∏–º –¥–æ 200 —Å–∏–º–≤–æ–ª–æ–≤
            parts.append("RUNTIME_OUT:" + (s[:200] + ("..." if len(s) > 200 else "")))
        if stderr:
            lines = stderr.splitlines()
            brief = " | ".join(lines[:3])
            parts.append("RUNTIME_ERR:" + (brief[:300] + ("..." if len(brief) > 300 else "")))
    except subprocess.TimeoutExpired:
        parts.append("RUNTIME_TIMEOUT")
    except Exception as e:
        parts.append(f"RUNTIME_CRASH:{str(e)}")

    if not parts:
        return "OK"
    return " ".join(parts)


# ---------------- 5. –°–±–æ—Ä Python —Ñ–∞–π–ª–æ–≤ + AST + –∫–æ–º–ø–∏–ª. –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å ----------------
texts = []
count = 0

print("üìÅ –°–±–æ—Ä Python —Ñ–∞–π–ª–æ–≤ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ AST + —Å–±–æ—Ä –∫–æ–º–ø–∏–ª. —Ñ–∏–¥–±–µ–∫–∞...")

for root, dirs, files in os.walk(CLONE_DIR):
    for file in files:
        if file.endswith(".py") and not file.startswith("test_"):
            file_path = os.path.join(root, file)
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    code = f.read()

                ast_sequence = extract_ast_sequence(code)

                # —Ñ–∏–ª—å—Ç—Ä: —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–∏–µ AST
                if ast_sequence and len(ast_sequence.split()) > 20:

                    compiler_seq = get_compiler_feedback(file_path, run_timeout=3)

                    # –§–æ—Ä–º–∞—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                    full_text = f"AST: {ast_sequence} COMPILER: {compiler_seq}"
                    texts.append({"content": full_text})
                    count += 1

                    # –û—Ç–ª–∞–¥–∫–∞ –¥–ª—è –ø–µ—Ä–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
                    if count <= 3:
                        visualize_ast_debug(code, file)
                        print(f"–ü—Ä–∏–º–µ—Ä AST sequence: {ast_sequence[:200]}...")
                        print(f"–ü—Ä–∏–º–µ—Ä compiler_seq: {compiler_seq}")

                if count % 50 == 0 and count > 0:
                    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {count} —Ñ–∞–π–ª–æ–≤")

            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
                continue

        if count >= LIMIT:
            break
    if count >= LIMIT:
        break

print(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(texts)} –ø—Ä–∏–º–µ—Ä–æ–≤ AST+COMPILER")

# –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äî —Å–æ–∑–¥–∞—ë–º —Ç–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã
if len(texts) == 0:
    print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
    print("üîÑ –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –æ–±—ã—á–Ω—ã–º AST...")

    test_codes = [
        """
def calculate_sum(a, b):
    result = a + b
    if result > 10:
        print("Large sum")
        return result * 2
    else:
        print("Small sum")
        return result
        """,

        """
class Calculator:
    def __init__(self, initial_value=0):
        self.value = initial_value

    def add(self, x):
        self.value += x
        return self.value

    def multiply(self, x):
        self.value *= x
        return self.value
        """,

        """
def process_data(data_list):
    results = []
    for item in data_list:
        if item is None:
            continue
        try:
            processed = item * 2
            results.append(processed)
        except Exception as e:
            print(f"Error processing {item}: {e}")
    return results
        """
    ]

    for i, code in enumerate(test_codes):
        seq = extract_ast_sequence(code)
        if seq:
            texts.append({"content": f"AST: {seq}"})
            print(f"–¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä {i+1}: {seq[:200]}...")

    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(texts)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Å AST")

# ---------------- 5. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è ----------------
print("üî§ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏...")
try:
    tokenizer = AutoTokenizer.from_pretrained(
        LOCAL_MODEL_DIR,
        trust_remote_code=True,
        local_files_only=True
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {e}")
    exit(1)

print("üî§ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
tokenized_data = []
for i, text in enumerate(texts):
    try:
        tokenized = tokenizer(
            text["content"],
            truncation=True,
            padding="max_length",
            max_length=MAX_LEN,
            return_tensors="pt"
        )
        tokenized["labels"] = tokenized["input_ids"].clone()
        tokenized_data.append(tokenized)

        if (i + 1) % 100 == 0:
            print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {i + 1} –ø—Ä–∏–º–µ—Ä–æ–≤")

    except Exception as e:
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–π –ø—Ä–∏–º–µ—Ä
        continue

print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(tokenized_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")

if len(tokenized_data) == 0:
    print("‚ùå –ù–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")
    exit(1)

# ---------------- 7. –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ ----------------
class ASTDataset(Dataset):
    def __init__(self, tokenized_data):
        self.tokenized_data = tokenized_data

    def __len__(self):
        return len(self.tokenized_data)

    def __getitem__(self, idx):
        item = self.tokenized_data[idx]
        return {
            'input_ids': item['input_ids'].squeeze(0),
            'attention_mask': item['attention_mask'].squeeze(0),
            'labels': item['labels'].squeeze(0)
        }

dataset = ASTDataset(tokenized_data)
print("‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω")

# ---------------- 8. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏ ----------------
print("ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏...")
# –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Qwen2ForCausalLM –∫–∞–∫ —Ä–∞–Ω—å—à–µ; –µ—Å–ª–∏ —É –≤–∞—Å –¥—Ä—É–≥–∞—è –º–æ–¥–µ–ª—å ‚Äî –∑–∞–º–µ–Ω–∏—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –∫–ª–∞—Å—Å–æ–º.
try:
    from transformers import Qwen2ForCausalLM  # –µ—Å–ª–∏ trust_remote_code —Ä–µ–∞–ª–∏–∑—É–µ—Ç —Ç–∞–∫–æ–π –∫–ª–∞—Å—Å
    ModelClass = Qwen2ForCausalLM
except Exception:
    ModelClass = AutoModelForCausalLM  # fallback

try:
    model = ModelClass.from_pretrained(
        LOCAL_MODEL_DIR,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map="auto",  # 'cuda' -> –∏—Å–ø–æ–ª—å–∑—É–µ–º auto, —á—Ç–æ–±—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—Ç—å
        local_files_only=True
    )
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å float16")

except Exception as e:
    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å float16: {e}")
    try:
        model = ModelClass.from_pretrained(
            LOCAL_MODEL_DIR,
            trust_remote_code=True,
            device_map="auto",
            local_files_only=True
        )
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –±–µ–∑ float16")
    except Exception as e2:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e2}")
        exit(1)

# ---------------- 9. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA ----------------
print("üéõÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA...")
try:
    lora_config = LoraConfig(
        r=8,
        lora_alpha=32,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )

    model = get_peft_model(model, lora_config)
    # –ü–µ—á–∞—Ç—å —Å–∫–æ–ª—å–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–∞–µ–º—ã—Ö
    try:
        model.print_trainable_parameters()
    except Exception:
        pass
    print("‚úÖ LoRA –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞")

except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LoRA: {e}")
    exit(1)

# ---------------- 10. –û–±—É—á–µ–Ω–∏–µ ----------------
print("üöÄ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è...")

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=4,
    num_train_epochs=EPOCHS,
    learning_rate=LR,
    warmup_steps=50,
    logging_steps=10,
    save_steps=200,
    save_total_limit=1,
    fp16=torch.cuda.is_available() and getattr(model, "dtype", None) == torch.float16,
    dataloader_pin_memory=False,
    remove_unused_columns=False,
    report_to="none",
    disable_tqdm=False,
)

def simple_collate_fn(batch):
    return {
        'input_ids': torch.stack([item['input_ids'] for item in batch]),
        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),
        'labels': torch.stack([item['labels'] for item in batch])
    }

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=simple_collate_fn
)

print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ ‚Ä¶")
try:
    trainer.train()
    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")

# ---------------- 11. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ----------------
print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
try:
    trainer.save_model()
    tokenizer.save_pretrained(OUTPUT_DIR)
    print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {OUTPUT_DIR}")

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    print("üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å...")
    test_input = "AST: Function Assign BinOp If Compare Call Return COMPILER: OK"
    test_tokens = tokenizer(test_input, return_tensors="pt", max_length=MAX_LEN, truncation=True)
    if torch.cuda.is_available():
        test_tokens = {k: v.cuda() for k, v in test_tokens.items()}

    with torch.no_grad():
        outputs = model.generate(
            **test_tokens,
            max_length=80,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"üìù –†–µ–∑—É–ª—å—Ç–∞—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {generated_text}")

except Exception as e:
    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")

# –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
del model, trainer, dataset, tokenized_data
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()

print("üéâ –°–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à–µ–Ω!")