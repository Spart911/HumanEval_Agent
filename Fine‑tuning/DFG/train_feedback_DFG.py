import os
import ast
import torch
import networkx as nx
import subprocess
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from peft import LoraConfig, get_peft_model
from torch.utils.data import Dataset
import gc

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ parallelism
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ---------------- 1. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ----------------
LOCAL_MODEL_DIR = "/home/nyuroprint/Jupyter/Qwen2.5-Coder-3B"  # –ü–∞–ø–∫–∞ —Å —É–∂–µ —Å–∫–∞—á–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
MAX_LEN = 512
EPOCHS = 2
BATCH_SIZE = 1
LR = 5e-6
OUTPUT_DIR = "../../dfg_feedback_trained_github"
LIMIT = 10000

REPOS = [
    "https://github.com/psf/requests.git",
    "https://github.com/pallets/flask.git",
    "https://github.com/pandas-dev/pandas.git",
    "https://github.com/numpy/numpy.git",
    "https://github.com/scipy/scipy.git",
    "https://github.com/scikit-learn/scikit-learn.git",
    "https://github.com/matplotlib/matplotlib.git",
    "https://github.com/plotly/plotly.py.git",
    "https://github.com/pytorch/pytorch.git",
    "https://github.com/tensorflow/tensorflow.git"
]

CLONE_DIR = "../../github_repos"
os.makedirs(CLONE_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ
if not os.path.exists(LOCAL_MODEL_DIR):
    print(f"‚ùå –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –ø–∞–ø–∫–µ: {LOCAL_MODEL_DIR}")
    print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞ –≤ —ç—Ç—É –ø–∞–ø–∫—É")
    exit(1)
else:
    print(f"‚úÖ –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞ –≤: {LOCAL_MODEL_DIR}")

# ---------------- 2. –ö–ª–æ–Ω–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ ----------------
print("üìÅ –ö–ª–æ–Ω–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏...")
for repo in REPOS:
    repo_name = repo.split("/")[-1].replace(".git", "")
    dest = os.path.join(CLONE_DIR, repo_name)
    if not os.path.exists(dest):
        print(f"Cloning {repo} ‚Ä¶")
        try:
            result = subprocess.run(
                ["git", "clone", "--depth", "1", repo, dest],
                capture_output=True,
                text=True,
                timeout=300
            )
            if result.returncode != 0:
                print(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ —Å –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º {repo}: {result.stderr}")
            else:
                print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω {repo}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è {repo}: {e}")

# ---------------- 3. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ DFG ----------------

def build_dfg_from_ast(node, graph, def_map=None):
    """
    –°—Ç—Ä–æ–∏—Ç Data Flow Graph (DFG) –∏–∑ AST.
    graph -- networkx.DiGraph(), –Ω–æ–¥—ã –∏–º–µ—é—Ç –∞—Ç—Ä–∏–±—É—Ç—ã: label, type, names (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    def_map -- dict: –∏–º—è –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π -> node_id (–ø–æ—Å–ª–µ–¥–Ω–µ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
    """
    if def_map is None:
        def_map = {}

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—á—ë—Ç—á–∏–∫–∞ id –≤ graph.graph —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –≥–ª–æ–±–æ–≤
    if "next_id" not in graph.graph:
        graph.graph["next_id"] = 0

    def new_id():
        nid = graph.graph["next_id"]
        graph.graph["next_id"] += 1
        return nid

    # –†–µ–∫—É—Ä—Å–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ node_id, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç "–≤—ã—Ö–æ–¥–∞–º" –≤—ã—Ä–∞–∂–µ–Ω–∏—è
    def _walk(n, local_def_map):
        # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ node ids (–æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ) —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —ç—Ç–æ–º—É —É–∑–ª—É (–∫–∞–∫ producer)
        if n is None:
            return []

        # –õ–∏—Å—Ç–æ–≤–æ–π Node: Name, Constant, etc.
        if isinstance(n, ast.Name):
            nid = new_id()
            label = f"Name:{n.id} ({'Load' if isinstance(n.ctx, ast.Load) else 'Store'})"
            graph.add_node(nid, label=label, type="Name", name=n.id, ctx=type(n.ctx).__name__)
            # –ï—Å–ª–∏ —ç—Ç–æ —á—Ç–µ–Ω–∏–µ ‚Äî —Å–æ–∑–¥–∞—ë–º —Ä–µ–±—Ä–æ –æ—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å)
            if isinstance(n.ctx, ast.Load):
                if n.id in local_def_map:
                    graph.add_edge(local_def_map[n.id], nid, label="def->use")
            # –ï—Å–ª–∏ store ‚Äî —ç—Ç–æ –ø–æ–∫–∞ —Ç–æ–ª—å–∫–æ —É–∑–µ–ª, –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π assign –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ def_map
            return [nid]

        elif isinstance(n, ast.Constant):
            nid = new_id()
            label = f"Const:{repr(n.value)}"
            graph.add_node(nid, label=label, type="Const")
            return [nid]

        # Binary op
        elif isinstance(n, ast.BinOp):
            left_ids = _walk(n.left, local_def_map)
            right_ids = _walk(n.right, local_def_map)
            op_id = new_id()
            op_name = type(n.op).__name__
            graph.add_node(op_id, label=f"BinOp:{op_name}", type="BinOp")
            # —Ä–µ–±—Ä–∞ –æ—Ç –æ–ø–µ—Ä–∞–Ω–¥–æ–≤ –∫ –æ–ø–µ—Ä–∞—Ü–∏–∏
            for lid in left_ids + right_ids:
                graph.add_edge(lid, op_id, label="operand")
            return [op_id]

        elif isinstance(n, ast.UnaryOp):
            operand_ids = _walk(n.operand, local_def_map)
            op_id = new_id()
            op_name = type(n.op).__name__
            graph.add_node(op_id, label=f"UnaryOp:{op_name}", type="UnaryOp")
            for oid in operand_ids:
                graph.add_edge(oid, op_id, label="operand")
            return [op_id]

        elif isinstance(n, ast.Call):
            func_ids = _walk(n.func, local_def_map)
            arg_ids = []
            for a in n.args:
                arg_ids.extend(_walk(a, local_def_map))
            call_id = new_id()
            graph.add_node(call_id, label="Call", type="Call")
            for fid in func_ids + arg_ids:
                graph.add_edge(fid, call_id, label="arg")
            # keywords
            for kw in getattr(n, "keywords", []):
                arg_ids = _walk(kw.value, local_def_map)
                for aid in arg_ids:
                    graph.add_edge(aid, call_id, label=f"kw:{kw.arg}")
            return [call_id]

        elif isinstance(n, ast.Assign):
            # RHS ‚Äî value
            value_ids = _walk(n.value, local_def_map)
            assign_id = new_id()
            graph.add_node(assign_id, label="Assign", type="Assign")
            # —Ä–µ–±—Ä–∞ –æ—Ç value –∫ assign
            for vid in value_ids:
                graph.add_edge(vid, assign_id, label="value")
            # –î–ª—è –∫–∞–∂–¥–æ–≥–æ target ‚Äî –µ—Å–ª–∏ —ç—Ç–æ Name, —Å–æ–∑–¥–∞—ë–º —É–∑–µ–ª def –∏ edge assign->def –∏ –æ–±–Ω–æ–≤–ª—è–µ–º def_map
            new_defs = []
            for t in n.targets:
                if isinstance(t, ast.Name):
                    def_id = new_id()
                    graph.add_node(def_id, label=f"Def:{t.id}", type="Def", name=t.id)
                    graph.add_edge(assign_id, def_id, label="assign->def")
                    # —Ä–µ–±—Ä–æ –æ—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å) –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è ‚Äî dataflow –æ—Ç value
                    # –û–±–Ω–æ–≤–ª—è–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –∫–∞—Ä—Ç—É –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
                    local_def_map = dict(local_def_map)  # –∫–ª–æ–Ω–∏—Ä—É–µ–º —á—Ç–æ–±—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–µ —É–¥–∞—Ä—è–ª–∏ –≤–≤–µ—Ä—Ö –ø–æ —Å—Ç–µ–∫—É
                    local_def_map[t.id] = def_id
                    new_defs.append(def_id)
                else:
                    # –ï—Å–ª–∏ target —Å–ª–æ–∂–Ω–µ–µ (tuple, attr, subscript) ‚Äî –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ
                    target_outs = _walk(t, local_def_map)
                    for tout in target_outs:
                        graph.add_edge(assign_id, tout, label="assign->target")
                        # –µ—Å–ª–∏ —É target –µ—Å—Ç—å –∏–º—è –≤–Ω—É—Ç—Ä–∏, –º–æ–∂–Ω–æ –ø–æ–ø—ã—Ç–∞—Ç—å—Å—è –æ–±–Ω–æ–≤–∏—Ç—å def_map ‚Äî –Ω–æ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –ø—Ä–æ–ø—É—Å—Ç–∏–º
            return new_defs if new_defs else [assign_id]

        elif isinstance(n, ast.AugAssign):
            # target op= value  -> —á–∏—Ç–∞–µ–º old def, —Å–æ–∑–¥–∞—ë–º op node, –∏ –Ω–æ–≤—ã–π def
            target_ids = _walk(n.target, local_def_map)
            value_ids = _walk(n.value, local_def_map)
            op_id = new_id()
            op_name = type(n.op).__name__
            graph.add_node(op_id, label=f"AugOp:{op_name}", type="AugOp")
            for tid in target_ids + value_ids:
                graph.add_edge(tid, op_id, label="operand")
            # —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π –¥–µ—Ñ –¥–ª—è target if it's Name
            if isinstance(n.target, ast.Name):
                def_id = new_id()
                graph.add_node(def_id, label=f"Def:{n.target.id}", type="Def", name=n.target.id)
                graph.add_edge(op_id, def_id, label="result")
                local_def_map = dict(local_def_map)
                local_def_map[n.target.id] = def_id
                return [def_id]
            return [op_id]

        elif isinstance(n, ast.If):
            # –î–ª—è DFG: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º test, body –∏ orelse. –¢–µ—Å—Ç ‚Äî producer –¥–ª—è –≤–µ—Ç–æ–∫
            test_ids = _walk(n.test, local_def_map)
            if_id = new_id()
            graph.add_node(if_id, label="If", type="If")
            for tid in test_ids:
                graph.add_edge(tid, if_id, label="cond")
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–ª–∞ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–ø–∏—é def_map (–≤–µ—Ç–≤–∏ –º–æ–≥—É—Ç –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å —Ä–∞–∑–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ)
            body_def_map = dict(local_def_map)
            for stmt in n.body:
                _walk(stmt, body_def_map)
            orelse_def_map = dict(local_def_map)
            for stmt in n.orelse:
                _walk(stmt, orelse_def_map)
            merged_map = dict(local_def_map)
            for var in set(list(body_def_map.keys()) + list(orelse_def_map.keys())):
                b = body_def_map.get(var)
                o = orelse_def_map.get(var)
                if b and o and b != o:
                    merge_id = new_id()
                    graph.add_node(merge_id, label=f"Merge:{var}", type="Merge", name=var)
                    graph.add_edge(b, merge_id, label="merge")
                    graph.add_edge(o, merge_id, label="merge")
                    merged_map[var] = merge_id
                elif b:
                    merged_map[var] = b
                elif o:
                    merged_map[var] = o
            return [if_id], merged_map

        elif isinstance(n, ast.For) or isinstance(n, ast.While):
            # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ: –ø—Ä–æ–∏–∑–≤–æ–¥–∏–º nodes –¥–ª—è —É—Å–ª–æ–≤–∏—è/–∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞ –∏ —Ç–µ–ª–∞
            loop_id = new_id()
            graph.add_node(loop_id, label=type(n).__name__, type=type(n).__name__)
            if isinstance(n, ast.For):
                target_ids = _walk(n.target, local_def_map)
                iter_ids = _walk(n.iter, local_def_map)
                for iid in iter_ids + target_ids:
                    graph.add_edge(iid, loop_id, label="iter")
            else:
                test_ids = _walk(n.test, local_def_map)
                for tid in test_ids:
                    graph.add_edge(tid, loop_id, label="cond")
            body_def_map = dict(local_def_map)
            for stmt in n.body:
                _walk(stmt, body_def_map)
            return [loop_id]

        elif isinstance(n, ast.Return):
            val_ids = _walk(n.value, local_def_map) if n.value else []
            ret_id = new_id()
            graph.add_node(ret_id, label="Return", type="Return")
            for vid in val_ids:
                graph.add_edge(vid, ret_id, label="ret")
            return [ret_id]

        elif isinstance(n, ast.FunctionDef):
            fid = new_id()
            graph.add_node(fid, label=f"Function:{n.name}", type="Function", name=n.name)
            # –∞—Ä–≥—É–º–µ–Ω—Ç—ã ‚Äî —Å—á–∏—Ç–∞—é—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏ –≤–Ω—É—Ç—Ä–∏ —Ñ—É–Ω–∫—Ü–∏–∏
            local_map = dict(local_def_map)
            for arg in n.args.args:
                arg_id = new_id()
                graph.add_node(arg_id, label=f"Arg:{arg.arg}", type="Arg", name=arg.arg)
                local_map[arg.arg] = arg_id
                graph.add_edge(arg_id, fid, label="arg->func")
            # —Ç–µ–ª–æ
            for stmt in n.body:
                _walk(stmt, local_map)
            return [fid]

        elif isinstance(n, ast.ClassDef):
            cid = new_id()
            graph.add_node(cid, label=f"Class:{n.name}", type="Class", name=n.name)
            local_map = dict(local_def_map)
            for stmt in n.body:
                _walk(stmt, local_map)
            return [cid]

        elif isinstance(n, ast.Expr):
            return _walk(n.value, local_def_map)

        elif isinstance(n, ast.Compare):
            left_ids = _walk(n.left, local_def_map)
            comp_id = new_id()
            graph.add_node(comp_id, label="Compare", type="Compare")
            for lid in left_ids:
                graph.add_edge(lid, comp_id, label="left")
            for op, comp in zip(n.ops, n.comparators):
                comp_ids = _walk(comp, local_def_map)
                for cid in comp_ids:
                    graph.add_edge(cid, comp_id, label="comp")
            return [comp_id]

        else:
            out_ids = []
            for field, value in ast.iter_fields(n):
                if isinstance(value, list):
                    for item in value:
                        if isinstance(item, ast.AST):
                            out_ids.extend(_walk(item, local_def_map))
                elif isinstance(value, ast.AST):
                    out_ids.extend(_walk(value, local_def_map))
            return out_ids

    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ö–æ–¥ —Å –ø—É—Å—Ç–æ–π –∫–∞—Ä—Ç–æ–π –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
    _walk(node, def_map)
    return graph


def extract_dfg_sequence(code: str) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —É–∑–ª–æ–≤ DFG –∏–∑ –∫–æ–¥–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–∏–ø–æ–≤ –Ω–æ–¥ (–Ω–∞–ø—Ä–∏–º–µ—Ä: Def Assign Call Name ...)
    """
    try:
        tree = ast.parse(code)
        graph = nx.DiGraph()
        graph.graph["next_id"] = 0

        build_dfg_from_ast(tree, graph, def_map={})

        if len(graph.nodes) == 0:
            return ""

        try:
            topo = list(nx.topological_sort(graph))
        except Exception:
            # fallback: –ø–æ –ø–æ—Ä—è–¥–∫—É –¥–æ–±–∞–≤–ª–µ–Ω–∏—è (id)
            topo = sorted(graph.nodes())

        seq = []
        for nid in topo:
            node_data = graph.nodes[nid]
            t = node_data.get("type", "Unknown")
            name = node_data.get("name")
            if name:
                seq.append(f"{t}:{name}")
            else:
                seq.append(t)
        return " ".join(seq)

    except SyntaxError as e:
        print(f"‚ö†Ô∏è –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –∫–æ–¥–µ: {e}")
        return ""
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è DFG: {e}")
        return ""



def visualize_dfg_debug(code: str, filename: str):
    """
    –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è DFG ‚Äî –ø–µ—á–∞—Ç–∞–µ—Ç –±–∞–∑–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏ –ø–µ—Ä–≤—ã–µ –Ω–æ–¥-–ª–µ–π–±–ª—ã.
    (–ù–µ —Ä–∏—Å—É–µ—Ç –≥—Ä–∞—Ñ–∏–∫—É, –ø—Ä–æ—Å—Ç–æ –≤—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤)
    """
    try:
        tree = ast.parse(code)
        graph = nx.DiGraph()
        graph.graph["next_id"] = 0
        build_dfg_from_ast(tree, graph, def_map={})

        print(f"\nüîç DFG –¥–ª—è {filename}:")
        print(f"–£–∑–ª–æ–≤: {len(graph.nodes)}, –†—ë–±–µ—Ä: {len(graph.edges)}")

        # –ü–æ–∫–∞–∂–µ–º –ø–µ—Ä–≤—ã–µ 12 —É–∑–ª–æ–≤ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        for i, (node_id, node_data) in enumerate(list(graph.nodes(data=True))[:12]):
            label = node_data.get("label", "")
            print(f"  {node_id}: {label}")

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ DFG: {e}")



# ---------------- 4.1. –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –∫–æ–º–ø–∏–ª—è—Ç–æ—Ä–∞/–∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è ----------------

def get_compiler_feedback(path: str, run_timeout: int = 5) -> str:
    """
    –ü—ã—Ç–∞–µ–º—Å—è —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞—Ç—å –∏ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∫–æ–¥.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫—Ä–∞—Ç–∫—É—é —Ç–µ–∫—Å—Ç–æ–≤—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.
    –§–æ—Ä–º–∞—Ç: –∫–ª—é—á: —Å–æ–æ–±—â–µ–Ω–∏–µ (–±–µ–∑ –¥–ª–∏–Ω–Ω—ã—Ö —Å—Ç–µ–∫-—Ç—Ä–µ–π—Å–æ–≤, —Å—Ç–∞—Ä–∞–µ–º—Å—è —É–∫–æ—Ä–æ—Ç–∏—Ç—å).
    """
    parts = []

    # 1) –ö–æ–º–ø–∏–ª—è—Ü–∏—è –≤ –±–∞–π—Ç–∫–æ–¥ (py_compile)
    try:
        compile_proc = subprocess.run(
            ["python3", "-m", "py_compile", path],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=run_timeout
        )
        stderr = compile_proc.stderr.strip()
        if stderr:
            # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Ç—Ä–∞—Å—Å—ã ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—ã–µ 2 —Å—Ç—Ä–æ–∫–∏
            lines = stderr.splitlines()
            brief = " | ".join(lines[:2])
            parts.append(f"COMPILE_ERROR:{brief}")
    except subprocess.TimeoutExpired:
        parts.append("COMPILE_TIMEOUT")
    except Exception as e:
        parts.append(f"COMPILE_CRASH:{str(e)}")

    # 2) –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–ø—É—Å–∫–∞ (–≤ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ)
    try:
        run_proc = subprocess.run(
            ["python3", path],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=run_timeout
        )
        stdout = run_proc.stdout.strip()
        stderr = run_proc.stderr.strip()
        if stdout:
            s = stdout.replace("\n", " \\n ")
            # —É–∫–æ—Ä–æ—Ç–∏–º –¥–æ 200 —Å–∏–º–≤–æ–ª–æ–≤
            parts.append("RUNTIME_OUT:" + (s[:200] + ("..." if len(s) > 200 else "")))
        if stderr:
            lines = stderr.splitlines()
            brief = " | ".join(lines[:3])
            parts.append("RUNTIME_ERR:" + (brief[:300] + ("..." if len(brief) > 300 else "")))
    except subprocess.TimeoutExpired:
        parts.append("RUNTIME_TIMEOUT")
    except Exception as e:
        parts.append(f"RUNTIME_CRASH:{str(e)}")

    if not parts:
        return "OK"
    return " ".join(parts)



# ---------------- 5. –°–±–æ—Ä Python —Ñ–∞–π–ª–æ–≤ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ DFG + –∫–æ–º–ø–∏–ª. –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ ----------------
texts = []
count = 0

print("üìÅ –°–±–æ—Ä Python —Ñ–∞–π–ª–æ–≤ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ DFG + —Å–±–æ—Ä –∫–æ–º–ø–∏–ª. —Ñ–∏–¥–±–µ–∫–∞...")
for root, dirs, files in os.walk(CLONE_DIR):
    for file in files:
        if file.endswith(".py") and not file.startswith("test_"):  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã
            file_path = os.path.join(root, file)
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    code = f.read()

                dfg_sequence = extract_dfg_sequence(code)

                # —Ñ–∏–ª—å—Ç—Ä: —Ç–æ–ª—å–∫–æ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–±–æ–ª—å—à–µ N —Ç–∏–ø–æ–≤)
                if dfg_sequence and len(dfg_sequence.split()) > 20:
                    # —Å–æ–±–∏—Ä–∞–µ–º –∫–æ–º–ø–∏–ª—è—Ç–æ—Ä–Ω—É—é/—Ä–∞–Ω—Ç–∞–π–º –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å
                    compiler_seq = get_compiler_feedback(file_path, run_timeout=3)

                    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–±—É—á–∞—é—â–∏–π —Ç–µ–∫—Å—Ç
                    # –§–æ—Ä–º–∞—Ç: DFG: ... COMPILER: ...
                    full_text = f"DFG: {dfg_sequence} COMPILER: {compiler_seq}"
                    texts.append({"content": full_text})
                    count += 1

                    # –î–ª—è –ø–µ—Ä–≤—ã—Ö –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                    if count <= 3:
                        visualize_dfg_debug(code, file)
                        print(f"–ü—Ä–∏–º–µ—Ä –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {dfg_sequence[:200]}...")
                        print(f"–ü—Ä–∏–º–µ—Ä compiler_seq: {compiler_seq}")

                if count % 50 == 0 and count > 0:
                    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {count} —Ñ–∞–π–ª–æ–≤")

            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
                continue

        if count >= LIMIT:
            break
    if count >= LIMIT:
        break

print(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(texts)} –ø—Ä–∏–º–µ—Ä–æ–≤ DFG+COMPILER")

if len(texts) == 0:
    print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
    print("üîÑ –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º DFG...")

    test_codes = [
        """
def calculate_sum(a, b):
    result = a + b
    if result > 10:
        print("Large sum")
        return result * 2
    else:
        print("Small sum")
        return result
        """,

        """
class Calculator:
    def __init__(self, initial_value=0):
        self.value = initial_value

    def add(self, x):
        self.value += x
        return self.value

    def multiply(self, x):
        self.value *= x
        return self.value
        """,

        """
def process_data(data_list):
    results = []
    for item in data_list:
        if item is None:
            continue
        try:
            processed = item * 2
            results.append(processed)
        except Exception as e:
            print(f"Error processing {item}: {e}")
    return results
        """
    ]

    for i, code in enumerate(test_codes):
        seq = extract_dfg_sequence(code)
        if seq:
            texts.append({"content": seq})
            print(f"–¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä {i+1}: {seq[:200]}...")

    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(texts)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Å DFG")

# ---------------- 5. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è ----------------
print("üî§ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏...")
try:
    tokenizer = AutoTokenizer.from_pretrained(
        LOCAL_MODEL_DIR,
        trust_remote_code=True,
        local_files_only=True
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {e}")
    exit(1)

print("üî§ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
tokenized_data = []
for i, text in enumerate(texts):
    try:
        tokenized = tokenizer(
            text["content"],
            truncation=True,
            padding="max_length",
            max_length=MAX_LEN,
            return_tensors="pt"
        )
        tokenized["labels"] = tokenized["input_ids"].clone()
        tokenized_data.append(tokenized)

        if (i + 1) % 100 == 0:
            print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {i + 1} –ø—Ä–∏–º–µ—Ä–æ–≤")

    except Exception as e:
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–π –ø—Ä–∏–º–µ—Ä
        continue

print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(tokenized_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")

if len(tokenized_data) == 0:
    print("‚ùå –ù–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")
    exit(1)

# ---------------- 7. –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ ----------------
class DFGDataset(Dataset):
    def __init__(self, tokenized_data):
        self.tokenized_data = tokenized_data

    def __len__(self):
        return len(self.tokenized_data)

    def __getitem__(self, idx):
        item = self.tokenized_data[idx]
        return {
            'input_ids': item['input_ids'].squeeze(0),
            'attention_mask': item['attention_mask'].squeeze(0),
            'labels': item['labels'].squeeze(0)
        }

dataset = DFGDataset(tokenized_data)
print("‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω")

# ---------------- 8. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏ ----------------
print("ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏...")
# –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Qwen2ForCausalLM –∫–∞–∫ —Ä–∞–Ω—å—à–µ; –µ—Å–ª–∏ —É –≤–∞—Å –¥—Ä—É–≥–∞—è –º–æ–¥–µ–ª—å ‚Äî –∑–∞–º–µ–Ω–∏—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –∫–ª–∞—Å—Å–æ–º.
try:
    from transformers import Qwen2ForCausalLM  # –µ—Å–ª–∏ trust_remote_code —Ä–µ–∞–ª–∏–∑—É–µ—Ç —Ç–∞–∫–æ–π –∫–ª–∞—Å—Å
    ModelClass = Qwen2ForCausalLM
except Exception:
    ModelClass = AutoModelForCausalLM  # fallback

try:
    model = ModelClass.from_pretrained(
        LOCAL_MODEL_DIR,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map="auto",  # 'cuda' -> –∏—Å–ø–æ–ª—å–∑—É–µ–º auto, —á—Ç–æ–±—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—Ç—å
        local_files_only=True
    )
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å float16")

except Exception as e:
    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å float16: {e}")
    try:
        model = ModelClass.from_pretrained(
            LOCAL_MODEL_DIR,
            trust_remote_code=True,
            device_map="auto",
            local_files_only=True
        )
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –±–µ–∑ float16")
    except Exception as e2:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e2}")
        exit(1)

# ---------------- 9. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA ----------------
print("üéõÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA...")
try:
    lora_config = LoraConfig(
        r=8,
        lora_alpha=32,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )

    model = get_peft_model(model, lora_config)
    # –ü–µ—á–∞—Ç—å —Å–∫–æ–ª—å–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–∞–µ–º—ã—Ö
    try:
        model.print_trainable_parameters()
    except Exception:
        pass
    print("‚úÖ LoRA –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞")

except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LoRA: {e}")
    exit(1)

# ---------------- 10. –û–±—É—á–µ–Ω–∏–µ ----------------
print("üöÄ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è...")

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=4,
    num_train_epochs=EPOCHS,
    learning_rate=LR,
    warmup_steps=50,
    logging_steps=10,
    save_steps=200,
    save_total_limit=1,
    fp16=torch.cuda.is_available() and getattr(model, "dtype", None) == torch.float16,
    dataloader_pin_memory=False,
    remove_unused_columns=False,
    report_to="none",
    disable_tqdm=False,
)

def simple_collate_fn(batch):
    return {
        'input_ids': torch.stack([item['input_ids'] for item in batch]),
        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),
        'labels': torch.stack([item['labels'] for item in batch])
    }

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=simple_collate_fn
)

print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ ‚Ä¶")
try:
    trainer.train()
    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")

# ---------------- 11. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ----------------
print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
try:
    trainer.save_model()
    tokenizer.save_pretrained(OUTPUT_DIR)
    print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {OUTPUT_DIR}")

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    print("üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å...")
    test_input = "DFG: Function Assign BinOp If Compare Call Return COMPILER: OK"
    test_tokens = tokenizer(test_input, return_tensors="pt", max_length=MAX_LEN, truncation=True)
    if torch.cuda.is_available():
        test_tokens = {k: v.cuda() for k, v in test_tokens.items()}

    with torch.no_grad():
        outputs = model.generate(
            **test_tokens,
            max_length=80,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"üìù –†–µ–∑—É–ª—å—Ç–∞—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {generated_text}")

except Exception as e:
    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")

# –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
del model, trainer, dataset, tokenized_data
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()

print("üéâ –°–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à–µ–Ω!")