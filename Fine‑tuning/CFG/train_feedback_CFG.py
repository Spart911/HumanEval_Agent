import os
import ast
import torch
import networkx as nx
import subprocess
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from peft import LoraConfig, get_peft_model
from torch.utils.data import Dataset
import gc

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ parallelism
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ---------------- 1. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ----------------
LOCAL_MODEL_DIR = "/home/nyuroprint/Jupyter/Qwen2.5-Coder-3B"  # –ü–∞–ø–∫–∞ —Å —É–∂–µ —Å–∫–∞—á–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
MAX_LEN = 512
EPOCHS = 2
BATCH_SIZE = 1
LR = 5e-6
OUTPUT_DIR = "../../cfg_feedback_trained_github"
LIMIT = 10000

REPOS = [
    "https://github.com/psf/requests.git",
    "https://github.com/pallets/flask.git",
    "https://github.com/pandas-dev/pandas.git",
    "https://github.com/numpy/numpy.git",
    "https://github.com/scipy/scipy.git",
    "https://github.com/scikit-learn/scikit-learn.git",
    "https://github.com/matplotlib/matplotlib.git",
    "https://github.com/plotly/plotly.py.git",
    "https://github.com/pytorch/pytorch.git",
    "https://github.com/tensorflow/tensorflow.git"
]

CLONE_DIR = "../../github_repos"
os.makedirs(CLONE_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ
if not os.path.exists(LOCAL_MODEL_DIR):
    print(f"‚ùå –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –ø–∞–ø–∫–µ: {LOCAL_MODEL_DIR}")
    print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞ –≤ —ç—Ç—É –ø–∞–ø–∫—É")
    exit(1)
else:
    print(f"‚úÖ –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞ –≤: {LOCAL_MODEL_DIR}")

# ---------------- 2. –ö–ª–æ–Ω–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ ----------------
print("üìÅ –ö–ª–æ–Ω–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏...")
for repo in REPOS:
    repo_name = repo.split("/")[-1].replace(".git", "")
    dest = os.path.join(CLONE_DIR, repo_name)
    if not os.path.exists(dest):
        print(f"Cloning {repo} ‚Ä¶")
        try:
            result = subprocess.run(
                ["git", "clone", "--depth", "1", repo, dest],
                capture_output=True,
                text=True,
                timeout=300
            )
            if result.returncode != 0:
                print(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ —Å –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º {repo}: {result.stderr}")
            else:
                print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω {repo}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è {repo}: {e}")

# ---------------- 3. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ CFG ----------------

def build_cfg_from_ast(node, graph, parent=None, edge_label=""):
    """
    –°—Ç—Ä–æ–∏—Ç –Ω–∞—Å—Ç–æ—è—â–∏–π Control Flow Graph –∏–∑ AST –¥–µ—Ä–µ–≤–∞
    """
    if not isinstance(node, ast.AST):
        return

    current_id = len(graph.nodes)
    node_name = type(node).__name__

    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–∑–∏—Ü–∏–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    if hasattr(node, 'lineno'):
        node_label = f"{node_name} (line {node.lineno})"
    else:
        node_label = node_name

    graph.add_node(current_id, label=node_label, type=node_name)

    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–±—Ä–æ –æ—Ç —Ä–æ–¥–∏—Ç–µ–ª—è
    if parent is not None:
        graph.add_edge(parent, current_id, label=edge_label)

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã —É–∑–ª–æ–≤
    if isinstance(node, ast.FunctionDef):
        # –î–ª—è —Ñ—É–Ω–∫—Ü–∏–π –¥–æ–±–∞–≤–ª—è–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∏ —Ç–µ–ª–æ
        for arg in node.args.args:
            build_cfg_from_ast(arg, graph, current_id, "arg")
        for item in node.body:
            build_cfg_from_ast(item, graph, current_id, "body")

    elif isinstance(node, ast.ClassDef):
        # –î–ª—è –∫–ª–∞—Å—Å–æ–≤ –¥–æ–±–∞–≤–ª—è–µ–º —Ç–µ–ª–æ
        for item in node.body:
            build_cfg_from_ast(item, graph, current_id, "class_body")

    elif isinstance(node, ast.If):
        # –î–ª—è —É—Å–ª–æ–≤–∏–π –¥–æ–±–∞–≤–ª—è–µ–º test, body –∏ orelse
        build_cfg_from_ast(node.test, graph, current_id, "condition")
        for item in node.body:
            build_cfg_from_ast(item, graph, current_id, "if_body")
        for item in node.orelse:
            build_cfg_from_ast(item, graph, current_id, "else_body")

    elif isinstance(node, ast.For) or isinstance(node, ast.While):
        # –î–ª—è —Ü–∏–∫–ª–æ–≤ –¥–æ–±–∞–≤–ª—è–µ–º target, iter –∏ body
        build_cfg_from_ast(node.target, graph, current_id, "loop_var")
        build_cfg_from_ast(node.iter, graph, current_id, "loop_iter")
        for item in node.body:
            build_cfg_from_ast(item, graph, current_id, "loop_body")
        for item in node.orelse:
            build_cfg_from_ast(item, graph, current_id, "loop_else")

    elif isinstance(node, ast.Try):
        # –î–ª—è try-except –±–ª–æ–∫–æ–≤
        for item in node.body:
            build_cfg_from_ast(item, graph, current_id, "try_body")
        for handler in node.handlers:
            build_cfg_from_ast(handler, graph, current_id, "except")
        for item in node.orelse:
            build_cfg_from_ast(item, graph, current_id, "else_body")
        for item in node.finalbody:
            build_cfg_from_ast(item, graph, current_id, "finally")

    elif isinstance(node, ast.ExceptHandler):
        # –î–ª—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
        if node.type:
            build_cfg_from_ast(node.type, graph, current_id, "exception_type")
        for item in node.body:
            build_cfg_from_ast(item, graph, current_id, "except_body")

    elif isinstance(node, ast.With):
        # –î–ª—è with –±–ª–æ–∫–æ–≤
        for item in node.items:
            build_cfg_from_ast(item, graph, current_id, "with_item")
        for item in node.body:
            build_cfg_from_ast(item, graph, current_id, "with_body")

    elif isinstance(node, ast.Call):
        # –î–ª—è –≤—ã–∑–æ–≤–æ–≤ —Ñ—É–Ω–∫—Ü–∏–π
        build_cfg_from_ast(node.func, graph, current_id, "function")
        for arg in node.args:
            build_cfg_from_ast(arg, graph, current_id, "arg")
        for keyword in node.keywords:
            build_cfg_from_ast(keyword, graph, current_id, "kwarg")

    elif isinstance(node, ast.BinOp):
        # –î–ª—è –±–∏–Ω–∞—Ä–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
        build_cfg_from_ast(node.left, graph, current_id, "left")
        build_cfg_from_ast(node.op, graph, current_id, "operator")
        build_cfg_from_ast(node.right, graph, current_id, "right")

    elif isinstance(node, ast.Compare):
        # –î–ª—è –æ–ø–µ—Ä–∞—Ü–∏–π —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        build_cfg_from_ast(node.left, graph, current_id, "left")
        for op, comparator in zip(node.ops, node.comparators):
            build_cfg_from_ast(op, graph, current_id, "comparator_op")
            build_cfg_from_ast(comparator, graph, current_id, "comparator")

    elif isinstance(node, ast.Assign):
        # –î–ª—è –ø—Ä–∏—Å–≤–∞–∏–≤–∞–Ω–∏–π
        for target in node.targets:
            build_cfg_from_ast(target, graph, current_id, "target")
        build_cfg_from_ast(node.value, graph, current_id, "value")

    elif isinstance(node, ast.AugAssign):
        # –î–ª—è augmented –ø—Ä–∏—Å–≤–∞–∏–≤–∞–Ω–∏–π
        build_cfg_from_ast(node.target, graph, current_id, "target")
        build_cfg_from_ast(node.op, graph, current_id, "operator")
        build_cfg_from_ast(node.value, graph, current_id, "value")

    else:
        # –î–ª—è –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —É–∑–ª–æ–≤ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ—á–µ—Ä–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for field, value in ast.iter_fields(node):
            if isinstance(value, list):
                for item in value:
                    if isinstance(item, ast.AST):
                        build_cfg_from_ast(item, graph, current_id, field)
            elif isinstance(value, ast.AST):
                build_cfg_from_ast(value, graph, current_id, field)


def extract_cfg_sequence(code: str) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —É–∑–ª–æ–≤ CFG –∏–∑ –∫–æ–¥–∞
    """
    try:
        tree = ast.parse(code)
        graph = nx.DiGraph()

        # –°—Ç—Ä–æ–∏–º CFG
        build_cfg_from_ast(tree, graph)

        if len(graph.nodes) == 0:
            return ""

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —É–∑–ª–æ–≤ –≤ –ø–æ—Ä—è–¥–∫–µ –æ–±—Ö–æ–¥–∞
        sequence = []

        def traverse_cfg(node_id, visited=None):
            if visited is None:
                visited = set()

            if node_id in visited:
                return

            visited.add(node_id)
            node_data = graph.nodes[node_id]
            sequence.append(node_data['type'])  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–∏–ø —É–∑–ª–∞

            # –û–±—Ö–æ–¥–∏–º —Å–æ—Å–µ–¥–µ–π –≤ –ø–æ—Ä—è–¥–∫–µ out-edges
            for neighbor in graph.successors(node_id):
                traverse_cfg(neighbor, visited)

        # –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ö–æ–¥ —Å –∫–æ—Ä–Ω–µ–≤–æ–≥–æ —É–∑–ª–∞ (0)
        traverse_cfg(0)

        return " ".join(sequence)

    except SyntaxError as e:
        print(f"–°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –∫–æ–¥–µ: {e}")
        return ""
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è CFG: {e}")
        return ""


def visualize_cfg_debug(code: str, filename: str):
    """
    –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ - –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç CFG –¥–ª—è –Ω–µ–±–æ–ª—å—à–æ–≥–æ –∫–æ–¥–∞
    """
    try:
        tree = ast.parse(code)
        graph = nx.DiGraph()
        build_cfg_from_ast(tree, graph)

        print(f"\nüîç CFG –¥–ª—è {filename}:")
        print(f"–£–∑–ª–æ–≤: {len(graph.nodes)}, –†–µ–±–µ—Ä: {len(graph.edges)}")

        # –ü–æ–∫–∞–∂–µ–º –ø–µ—Ä–≤—ã–µ 10 —É–∑–ª–æ–≤ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        for i, (node_id, node_data) in enumerate(list(graph.nodes(data=True))[:10]):
            print(f"  {node_id}: {node_data['label']}")

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")



# ---------------- 4.1. –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –∫–æ–º–ø–∏–ª—è—Ç–æ—Ä–∞/–∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è ----------------

def get_compiler_feedback(path: str, run_timeout: int = 5) -> str:
    """
    –ü—ã—Ç–∞–µ–º—Å—è —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞—Ç—å –∏ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∫–æ–¥.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫—Ä–∞—Ç–∫—É—é —Ç–µ–∫—Å—Ç–æ–≤—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.
    –§–æ—Ä–º–∞—Ç: –∫–ª—é—á: —Å–æ–æ–±—â–µ–Ω–∏–µ (–±–µ–∑ –¥–ª–∏–Ω–Ω—ã—Ö —Å—Ç–µ–∫-—Ç—Ä–µ–π—Å–æ–≤, —Å—Ç–∞—Ä–∞–µ–º—Å—è —É–∫–æ—Ä–æ—Ç–∏—Ç—å).
    """
    parts = []

    # 1) –ö–æ–º–ø–∏–ª—è—Ü–∏—è –≤ –±–∞–π—Ç–∫–æ–¥ (py_compile)
    try:
        compile_proc = subprocess.run(
            ["python3", "-m", "py_compile", path],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=run_timeout
        )
        stderr = compile_proc.stderr.strip()
        if stderr:
            # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Ç—Ä–∞—Å—Å—ã ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—ã–µ 2 —Å—Ç—Ä–æ–∫–∏
            lines = stderr.splitlines()
            brief = " | ".join(lines[:2])
            parts.append(f"COMPILE_ERROR:{brief}")
    except subprocess.TimeoutExpired:
        parts.append("COMPILE_TIMEOUT")
    except Exception as e:
        parts.append(f"COMPILE_CRASH:{str(e)}")

    # 2) –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–ø—É—Å–∫–∞ (–≤ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ)
    try:
        run_proc = subprocess.run(
            ["python3", path],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=run_timeout
        )
        stdout = run_proc.stdout.strip()
        stderr = run_proc.stderr.strip()
        if stdout:
            s = stdout.replace("\n", " \\n ")
            # —É–∫–æ—Ä–æ—Ç–∏–º –¥–æ 200 —Å–∏–º–≤–æ–ª–æ–≤
            parts.append("RUNTIME_OUT:" + (s[:200] + ("..." if len(s) > 200 else "")))
        if stderr:
            lines = stderr.splitlines()
            brief = " | ".join(lines[:3])
            parts.append("RUNTIME_ERR:" + (brief[:300] + ("..." if len(brief) > 300 else "")))
    except subprocess.TimeoutExpired:
        parts.append("RUNTIME_TIMEOUT")
    except Exception as e:
        parts.append(f"RUNTIME_CRASH:{str(e)}")

    if not parts:
        return "OK"
    return " ".join(parts)



# ---------------- 5. –°–±–æ—Ä Python —Ñ–∞–π–ª–æ–≤ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ CFG + –∫–æ–º–ø–∏–ª. –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ ----------------
texts = []
count = 0

print("üìÅ –°–±–æ—Ä Python —Ñ–∞–π–ª–æ–≤ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ CFG + —Å–±–æ—Ä –∫–æ–º–ø–∏–ª. —Ñ–∏–¥–±–µ–∫–∞...")
for root, dirs, files in os.walk(CLONE_DIR):
    for file in files:
        if file.endswith(".py") and not file.startswith("test_"):  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã
            file_path = os.path.join(root, file)
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    code = f.read()

                cfg_sequence = extract_cfg_sequence(code)

                # —Ñ–∏–ª—å—Ç—Ä: —Ç–æ–ª—å–∫–æ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–±–æ–ª—å—à–µ N —Ç–∏–ø–æ–≤)
                if cfg_sequence and len(cfg_sequence.split()) > 20:
                    # —Å–æ–±–∏—Ä–∞–µ–º –∫–æ–º–ø–∏–ª—è—Ç–æ—Ä–Ω—É—é/—Ä–∞–Ω—Ç–∞–π–º –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å
                    compiler_seq = get_compiler_feedback(file_path, run_timeout=3)

                    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–±—É—á–∞—é—â–∏–π —Ç–µ–∫—Å—Ç
                    # –§–æ—Ä–º–∞—Ç: CFG: ... COMPILER: ...
                    full_text = f"CFG: {cfg_sequence} COMPILER: {compiler_seq}"
                    texts.append({"content": full_text})
                    count += 1

                    # –î–ª—è –ø–µ—Ä–≤—ã—Ö –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                    if count <= 3:
                        visualize_cfg_debug(code, file)
                        print(f"–ü—Ä–∏–º–µ—Ä –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {cfg_sequence[:200]}...")
                        print(f"–ü—Ä–∏–º–µ—Ä compiler_seq: {compiler_seq}")

                if count % 50 == 0 and count > 0:
                    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {count} —Ñ–∞–π–ª–æ–≤")

            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
                continue

        if count >= LIMIT:
            break
    if count >= LIMIT:
        break

print(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(texts)} –ø—Ä–∏–º–µ—Ä–æ–≤ CFG+COMPILER")

if len(texts) == 0:
    print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
    print("üîÑ –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º CFG...")

    test_codes = [
        """
def calculate_sum(a, b):
    result = a + b
    if result > 10:
        print("Large sum")
        return result * 2
    else:
        print("Small sum")
        return result
        """,

        """
class Calculator:
    def __init__(self, initial_value=0):
        self.value = initial_value

    def add(self, x):
        self.value += x
        return self.value

    def multiply(self, x):
        self.value *= x
        return self.value
        """,

        """
def process_data(data_list):
    results = []
    for item in data_list:
        if item is None:
            continue
        try:
            processed = item * 2
            results.append(processed)
        except Exception as e:
            print(f"Error processing {item}: {e}")
    return results
        """
    ]

    for i, code in enumerate(test_codes):
        seq = extract_cfg_sequence(code)
        if seq:
            texts.append({"content": seq})
            print(f"–¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä {i+1}: {seq[:200]}...")

    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(texts)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Å CFG")

# ---------------- 5. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è ----------------
print("üî§ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏...")
try:
    tokenizer = AutoTokenizer.from_pretrained(
        LOCAL_MODEL_DIR,
        trust_remote_code=True,
        local_files_only=True
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {e}")
    exit(1)

print("üî§ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
tokenized_data = []
for i, text in enumerate(texts):
    try:
        tokenized = tokenizer(
            text["content"],
            truncation=True,
            padding="max_length",
            max_length=MAX_LEN,
            return_tensors="pt"
        )
        tokenized["labels"] = tokenized["input_ids"].clone()
        tokenized_data.append(tokenized)

        if (i + 1) % 100 == 0:
            print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {i + 1} –ø—Ä–∏–º–µ—Ä–æ–≤")

    except Exception as e:
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–π –ø—Ä–∏–º–µ—Ä
        continue

print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(tokenized_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")

if len(tokenized_data) == 0:
    print("‚ùå –ù–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")
    exit(1)

# ---------------- 7. –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ ----------------
class CFGDataset(Dataset):
    def __init__(self, tokenized_data):
        self.tokenized_data = tokenized_data

    def __len__(self):
        return len(self.tokenized_data)

    def __getitem__(self, idx):
        item = self.tokenized_data[idx]
        return {
            'input_ids': item['input_ids'].squeeze(0),
            'attention_mask': item['attention_mask'].squeeze(0),
            'labels': item['labels'].squeeze(0)
        }

dataset = CFGDataset(tokenized_data)
print("‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω")

# ---------------- 8. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏ ----------------
print("ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏...")
# –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Qwen2ForCausalLM –∫–∞–∫ —Ä–∞–Ω—å—à–µ; –µ—Å–ª–∏ —É –≤–∞—Å –¥—Ä—É–≥–∞—è –º–æ–¥–µ–ª—å ‚Äî –∑–∞–º–µ–Ω–∏—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –∫–ª–∞—Å—Å–æ–º.
try:
    from transformers import Qwen2ForCausalLM  # –µ—Å–ª–∏ trust_remote_code —Ä–µ–∞–ª–∏–∑—É–µ—Ç —Ç–∞–∫–æ–π –∫–ª–∞—Å—Å
    ModelClass = Qwen2ForCausalLM
except Exception:
    ModelClass = AutoModelForCausalLM  # fallback

try:
    model = ModelClass.from_pretrained(
        LOCAL_MODEL_DIR,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map="auto",  # 'cuda' -> –∏—Å–ø–æ–ª—å–∑—É–µ–º auto, —á—Ç–æ–±—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—Ç—å
        local_files_only=True
    )
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å float16")

except Exception as e:
    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å float16: {e}")
    try:
        model = ModelClass.from_pretrained(
            LOCAL_MODEL_DIR,
            trust_remote_code=True,
            device_map="auto",
            local_files_only=True
        )
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –±–µ–∑ float16")
    except Exception as e2:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e2}")
        exit(1)

# ---------------- 9. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA ----------------
print("üéõÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA...")
try:
    lora_config = LoraConfig(
        r=8,
        lora_alpha=32,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )

    model = get_peft_model(model, lora_config)
    # –ü–µ—á–∞—Ç—å —Å–∫–æ–ª—å–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–∞–µ–º—ã—Ö
    try:
        model.print_trainable_parameters()
    except Exception:
        pass
    print("‚úÖ LoRA –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞")

except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LoRA: {e}")
    exit(1)

# ---------------- 10. –û–±—É—á–µ–Ω–∏–µ ----------------
print("üöÄ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è...")

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=4,
    num_train_epochs=EPOCHS,
    learning_rate=LR,
    warmup_steps=50,
    logging_steps=10,
    save_steps=200,
    save_total_limit=1,
    fp16=torch.cuda.is_available() and getattr(model, "dtype", None) == torch.float16,
    dataloader_pin_memory=False,
    remove_unused_columns=False,
    report_to="none",
    disable_tqdm=False,
)

def simple_collate_fn(batch):
    return {
        'input_ids': torch.stack([item['input_ids'] for item in batch]),
        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),
        'labels': torch.stack([item['labels'] for item in batch])
    }

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=simple_collate_fn
)

print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ ‚Ä¶")
try:
    trainer.train()
    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")

# ---------------- 11. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ----------------
print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
try:
    trainer.save_model()
    tokenizer.save_pretrained(OUTPUT_DIR)
    print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {OUTPUT_DIR}")

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    print("üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å...")
    test_input = "CFG: Function Assign BinOp If Compare Call Return COMPILER: OK"
    test_tokens = tokenizer(test_input, return_tensors="pt", max_length=MAX_LEN, truncation=True)
    if torch.cuda.is_available():
        test_tokens = {k: v.cuda() for k, v in test_tokens.items()}

    with torch.no_grad():
        outputs = model.generate(
            **test_tokens,
            max_length=80,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"üìù –†–µ–∑—É–ª—å—Ç–∞—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {generated_text}")

except Exception as e:
    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")

# –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
del model, trainer, dataset, tokenized_data
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()

print("üéâ –°–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à–µ–Ω!")