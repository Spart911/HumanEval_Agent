"""–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏."""
import os
import torch
import logging
from typing import Union, Dict, Any
from transformers import AutoTokenizer, AutoModelForCausalLM, QuantoConfig
from peft import PeftModel
from src.utils.environment import get_device, login_to_huggingface

logger = logging.getLogger(__name__)


def setup_quantization(config: Dict[str, Any] = None) -> QuantoConfig:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏."""
    if config is None:
        config = {}
    weights_type = config.get("weights", "int8")
    quantization_config = QuantoConfig(weights=weights_type)
    logger.info(f"–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ {weights_type}")
    return quantization_config


def _expand_local_path(path: str) -> str:
    """–†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ—Ç ~ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–µ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø—É—Ç–∏."""
    expanded = os.path.expanduser(path)
    if os.path.exists(expanded):
        return expanded
    return path  # –µ—Å–ª–∏ —ç—Ç–æ repo_id (–Ω–∞–ø—Ä–∏–º–µ—Ä, "Qwen/Qwen2.5-Coder-3B")


def load_model(
    model_path: str,
    base_model_path: str,
    device: Union[str, torch.device] = "auto",
    quantization_config: QuantoConfig = None,
    use_lora: bool = False,
    trust_remote_code: bool = True,
    torch_dtype: torch.dtype = torch.float16
) -> tuple:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π –∏ LoRA.
    """
    os.environ.setdefault("HF_HUB_OFFLINE", "1")
    os.environ.setdefault("TRANSFORMERS_OFFLINE", "1")


    login_to_huggingface()

    model_path = _expand_local_path(model_path)
    if base_model_path:
        base_model_path = _expand_local_path(base_model_path)

    if device == "auto":
        device = get_device()
    elif isinstance(device, str):
        device = torch.device(device)

    if not os.path.exists(model_path) and "/" not in model_path:
        logger.error(f"–ú–æ–¥–µ–ª—å {model_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ –∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ –∏–º—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è.")
        raise ValueError(f"Model path {model_path} does not exist")

    # ---------------------- –í—ã–±–æ—Ä –∫–ª–∞—Å—Å–∞ –º–æ–¥–µ–ª–∏ ----------------------
    try:
        from transformers import Qwen2ForCausalLM
        ModelClass = Qwen2ForCausalLM
        logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª–∞—Å—Å Qwen2ForCausalLM")
    except Exception:
        ModelClass = AutoModelForCausalLM
        logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å AutoModelForCausalLM")

    try:
        # ---------------------- –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä ----------------------
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ {base_model_path}")
        tokenizer = AutoTokenizer.from_pretrained(
            base_model_path,
            trust_remote_code=trust_remote_code,
            local_files_only=True
        )

        if tokenizer.pad_token_id is None:
            tokenizer.pad_token_id = tokenizer.eos_token_id
            logger.info("–£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω pad_token_id —Ä–∞–≤–Ω—ã–º eos_token_id")

        # ---------------------- –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ ----------------------
        load_kwargs = {
            "trust_remote_code": trust_remote_code,
            "torch_dtype": torch_dtype,
            "local_files_only": True,
        }
        if quantization_config:
            load_kwargs["quantization_config"] = quantization_config

        if device.type == "cuda":
            load_kwargs["device_map"] = "cuda"
        else:
            load_kwargs["device_map"] = None

        # ---------------------- –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ ----------------------
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –∏–∑ {AutoModelForCausalLM}")
        base_model = AutoModelForCausalLM.from_pretrained(base_model_path, **load_kwargs)

        logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –∞–¥–∞–ø—Ç–µ—Ä LoRA –∏–∑ {model_path}")
        model = PeftModel.from_pretrained(base_model, model_path, local_files_only=True)
        model = model.to(device)

        model.eval()
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ {device}")

        return tokenizer, model, device

    except Exception as e:
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {e}")
        raise


def load_model_with_lora(
    lora_path: str,
    base_model_path: str,
    device: Union[str, torch.device] = "auto",
    torch_dtype: torch.dtype = torch.float16
) -> tuple:
    """–£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –∞–¥–∞–ø—Ç–µ—Ä–æ–º LoRA."""
    return load_model(
        model_path=lora_path,
        base_model_path=base_model_path,
        device=device,
        use_lora=True,
        torch_dtype=torch_dtype
    )


def load_base_model_only(
    base_model_path: str,
    device: Union[str, torch.device] = "auto",
    torch_dtype: torch.dtype = torch.float16,
    trust_remote_code: bool = True
) -> tuple:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –±–µ–∑ –∫–∞–∫–∏—Ö-–ª–∏–±–æ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ –∏–ª–∏ –¥–æ–æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–µ—Ä—Å–∏–π.

    Args:
        base_model_path: –ü—É—Ç—å –∫ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
        torch_dtype: –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏
        trust_remote_code: –î–æ–≤–µ—Ä—è—Ç—å —É–¥–∞–ª–µ–Ω–Ω–æ–º—É –∫–æ–¥—É

    Returns:
        –ö–æ—Ä—Ç–µ–∂ (tokenizer, model, device)
    """
    os.environ.setdefault("HF_HUB_OFFLINE", "1")
    os.environ.setdefault("TRANSFORMERS_OFFLINE", "1")

    login_to_huggingface()

    base_model_path = _expand_local_path(base_model_path)

    if device == "auto":
        device = get_device()
    elif isinstance(device, str):
        device = torch.device(device)

    if not os.path.exists(base_model_path) and "/" not in base_model_path:
        logger.error(f"–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å {base_model_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ –∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ –∏–º—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è.")
        raise ValueError(f"Base model path {base_model_path} does not exist")

    try:
        # ---------------------- –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä ----------------------
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ {base_model_path}")
        tokenizer = AutoTokenizer.from_pretrained(
            base_model_path,
            trust_remote_code=trust_remote_code,
            local_files_only=True
        )

        if tokenizer.pad_token_id is None:
            tokenizer.pad_token_id = tokenizer.eos_token_id
            logger.info("–£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω pad_token_id —Ä–∞–≤–Ω—ã–º eos_token_id")

        # ---------------------- –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ ----------------------
        load_kwargs = {
            "trust_remote_code": trust_remote_code,
            "torch_dtype": torch_dtype,
            "local_files_only": True,
        }

        if device.type == "cuda":
            load_kwargs["device_map"] = "cuda"
        else:
            load_kwargs["device_map"] = None

        # ---------------------- –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ ----------------------
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –∏–∑ {base_model_path}")
        model = AutoModelForCausalLM.from_pretrained(base_model_path, **load_kwargs)

        model = model.to(device)
        model.eval()
        logger.info(f"‚úÖ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ {device}")

        return tokenizer, model, device

    except Exception as e:
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å: {e}")
        raise


def load_merged_model(
    lora_path: str,
    base_model_path: str,
    save_path: str = None,
    device: Union[str, torch.device] = "auto",
    torch_dtype: torch.dtype = torch.float16
) -> tuple:
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ LoRA + –±–∞–∑–æ–≤–æ–π."""
    tokenizer, lora_model, device = load_model_with_lora(lora_path, base_model_path, device, torch_dtype)

    logger.info("–û–±—ä–µ–¥–∏–Ω—è–µ–º –∞–¥–∞–ø—Ç–µ—Ä LoRA —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é")
    merged_model = lora_model.merge_and_unload()

    if save_path:
        save_path = _expand_local_path(save_path)
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤ {save_path}")
        os.makedirs(save_path, exist_ok=True)
        merged_model.save_pretrained(save_path)
        tokenizer.save_pretrained(save_path)

    return tokenizer, merged_model, device
