"""–£—Ç–∏–ª–∏—Ç—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π LLM."""
import torch
import logging
import textwrap
from typing import Optional, Dict, Any
from transformers import AutoTokenizer


from ..utils.code_utils import check_code_in_subprocess, strip_code_fences

logger = logging.getLogger(__name__)


def generate_code_with_model(
        prompt: str,
        tokenizer: AutoTokenizer,
        model,
        device: torch.device,
        generation_config: Optional[Dict[str, Any]] = None,
        iterations: int = 3
) -> str:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫.

    Args:
        prompt: –ü—Ä–æ–º–ø—Ç –∑–∞–¥–∞—á–∏.
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –º–æ–¥–µ–ª–∏.
        model: –Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å.
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
        generation_config: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
        iterations: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏/–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–¥–∞.

    Returns:
        –õ—É—á—à–∏–π —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥ –ø–æ—Å–ª–µ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è.
    """
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    default_config = {
        "max_new_tokens": 400,
        "do_sample": True,
        "temperature": 0.6,
        "top_k": 40,
        "pad_token_id": tokenizer.eos_token_id,
        "eos_token_id": tokenizer.eos_token_id
    }

    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    if generation_config:
        default_config.update(generation_config)

    base_prompt = prompt
    current_prompt = prompt
    last_successful = ""
    all_steps = []

    for step in range(iterations):
        logger.info(f"üß© –ò—Ç–µ—Ä–∞—Ü–∏—è {step + 1}/{iterations}: —É—Ç–æ—á–Ω–µ–Ω–∏–µ –∫–æ–¥–∞...")

        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞
        inputs = tokenizer(current_prompt, return_tensors="pt", padding=True, truncation=True)
        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º —Ç–µ–Ω–∑–æ—Ä—ã –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        inputs = {k: v.to(device) for k, v in inputs.items()}

        try:
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞
            with torch.no_grad():
                outputs = model.generate(
                    input_ids=inputs["input_ids"],
                    attention_mask=inputs.get("attention_mask", None),
                    **default_config
                )

            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            gen_suffix = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True).strip()

            # –û—á–∏—â–∞–µ–º –æ–≥—Ä–∞–∂–¥–µ–Ω–∏—è markdown –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º
            gen_suffix = strip_code_fences(gen_suffix)
            gen_suffix = textwrap.dedent(gen_suffix).strip()
            all_steps.append(gen_suffix)

            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–¥–∞ –≤ subprocess
            success, error_msg = check_code_in_subprocess(gen_suffix, timeout=6)

            if success:
                logger.info(f"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–¥–∞ –ø—Ä–æ–π–¥–µ–Ω–∞ –Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏ {step + 1}")
                last_successful = gen_suffix
                return last_successful.strip()
            else:
                logger.error(f"‚ùå –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–¥–∞ –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞: {error_msg}")

                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± –æ—à–∏–±–∫–µ
                current_prompt = (
                    f"{base_prompt}\\n\\n"
                    f"Previous solution:\\n{gen_suffix}\\n\\n"
                    f"The last attempt failed during automatic checking with the following error (exact text):\\n"
                    f"```\\n{error_msg}\\n```\\n\\n"
                    "Analyze the code carefully and return a corrected, fully functional, syntactically correct Python function.\\n"
                    "Keep the same function name and parameters. Fix all indentation and syntax errors and any runtime error "
                    "reported above. Do not include any explanations, comments or additional text ‚Äî return only the updated code.\\n"
                )
                last_error = error_msg
                # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            continue

    logger.warning("‚ö†Ô∏è –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –Ω–µ –¥–∞–ª–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∫–æ–¥–∞.")

    if last_successful:
        logger.info("–í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —É—Å–ø–µ—à–Ω—É—é —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é.")
        return last_successful.strip()
    else:
        logger.info("–í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é (–±–µ–∑ —É—Å–ø–µ—à–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏).")
        return all_steps[-1].strip() if all_steps else ""


def generate_single_turn_code(
        prompt: str,
        tokenizer: AutoTokenizer,
        model,
        device: torch.device,
        generation_config: Optional[Dict[str, Any]] = None
) -> str:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ –±–µ–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–∏.

    Args:
        prompt: –ü—Ä–æ–º–ø—Ç –∑–∞–¥–∞—á–∏.
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –º–æ–¥–µ–ª–∏.
        model: –Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å.
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
        generation_config: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

    Returns:
        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥ –±–µ–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–∏.
    """
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    default_config = {
        "max_new_tokens": 400,
        "do_sample": True,
        "temperature": 0.6,
        "top_k": 40,
        "pad_token_id": tokenizer.eos_token_id,
        "eos_token_id": tokenizer.eos_token_id
    }

    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    if generation_config:
        default_config.update(generation_config)

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    try:
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞
        with torch.no_grad():
            outputs = model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=inputs.get("attention_mask", None),
                **default_config
            )

        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—á–∏—Å—Ç–∫–∞
        gen_text = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True).strip()
        gen_text = strip_code_fences(gen_text)
        gen_text = textwrap.dedent(gen_text).strip()

        return gen_text
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞: {e}")
        return ""