"""–£—Ç–∏–ª–∏—Ç—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π LLM."""
import torch
import logging
import textwrap
from typing import Optional, Dict, Any
from transformers import AutoTokenizer


from ..utils.code_utils import check_code_in_subprocess, strip_code_fences

logger = logging.getLogger(__name__)


def _clean_and_prepare_gen_kwargs(tokenizer: AutoTokenizer, user_cfg: Optional[Dict[str, Any]], input_len: int) -> Dict[str, Any]:
    """–°–æ–±–∏—Ä–∞–µ–º kwargs –¥–ª—è model.generate, —Ñ–∏–ª—å—Ç—Ä—É–µ–º None –∏ –≤—ã—Å—Ç–∞–≤–ª—è–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π max_length.

    Notes:
    - –ù–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –≤–µ—Ä—Å–∏—è—Ö transformers –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è max_new_tokens, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω max_length.
      –ü–æ—ç—Ç–æ–º—É –≤—ã—Å—Ç–∞–≤–ª—è–µ–º max_length = input_len + max_new_tokens –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏.
    - –ò—Å–∫–ª—é—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è None, —á—Ç–æ–±—ã –Ω–µ –∑–∞—Ç–∏—Ä–∞—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ pad/eos token id.
    """
    base_cfg: Dict[str, Any] = {
        "max_new_tokens": 400,
        "do_sample": True,
        "temperature": 0.6,
        "top_k": 40,
        "pad_token_id": tokenizer.eos_token_id,
        "eos_token_id": tokenizer.eos_token_id,
    }

    if user_cfg:
        # —É–±–∏—Ä–∞–µ–º None –∑–Ω–∞—á–µ–Ω–∏—è
        cleaned = {k: v for k, v in user_cfg.items() if v is not None}
        base_cfg.update(cleaned)

    # —É–±–µ–¥–∏–º—Å—è, —á—Ç–æ pad/eos –∑–∞–¥–∞–Ω—ã
    if base_cfg.get("pad_token_id") is None:
        base_cfg["pad_token_id"] = tokenizer.eos_token_id
    if base_cfg.get("eos_token_id") is None:
        base_cfg["eos_token_id"] = tokenizer.eos_token_id

    # —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –≤–µ—Ä—Å–∏—è–º–∏, –æ–∂–∏–¥–∞—é—â–∏–º–∏ max_length
    max_new = int(base_cfg.get("max_new_tokens", 256))
    base_cfg["max_length"] = int(input_len + max_new)


    return base_cfg

def generate_code_with_model(
        prompt: str,
        tokenizer: AutoTokenizer,
        model,
        device: torch.device,
        generation_config: Optional[Dict[str, Any]] = None,
        iterations: int = 3
) -> str:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫.

    Args:
        prompt: –ü—Ä–æ–º–ø—Ç –∑–∞–¥–∞—á–∏.
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –º–æ–¥–µ–ª–∏.
        model: –Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å.
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
        generation_config: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
        iterations: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏/–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–¥–∞.

    Returns:
        –õ—É—á—à–∏–π —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥ –ø–æ—Å–ª–µ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è.
    """
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±—É–¥—É—Ç —Å–æ–±—Ä–∞–Ω—ã –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –Ω–∏–∂–µ –≤ gen_kwargs

    base_prompt = prompt
    current_prompt = prompt
    last_successful = ""
    all_steps = []

    for step in range(iterations):
        logger.info(f"üß© –ò—Ç–µ—Ä–∞—Ü–∏—è {step + 1}/{iterations}: —É—Ç–æ—á–Ω–µ–Ω–∏–µ –∫–æ–¥–∞...")

        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞
        inputs = tokenizer(current_prompt, return_tensors="pt", padding=True, truncation=True)
        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º —Ç–µ–Ω–∑–æ—Ä—ã –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        inputs = {k: v.to(device) for k, v in inputs.items()}

        # –°–±–æ—Ä —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö kwargs –¥–ª—è generate
        gen_kwargs = _clean_and_prepare_gen_kwargs(tokenizer, generation_config, inputs["input_ids"].shape[1])

        try:
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞
            with torch.no_grad():
                outputs = model.generate(
                    input_ids=inputs["input_ids"],
                    attention_mask=inputs.get("attention_mask", None),
                    **gen_kwargs
                )

            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            gen_suffix = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True).strip()

            # –û—á–∏—â–∞–µ–º –æ–≥—Ä–∞–∂–¥–µ–Ω–∏—è markdown –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º
            gen_suffix = strip_code_fences(gen_suffix)
            gen_suffix = textwrap.dedent(gen_suffix).strip()

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ª–∏—Ç–µ—Ä–∞–ª—ã "\n" –∏ "\t" –≤ —Ä–µ–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã/—Ç–∞–±—É–ª—è—Ü–∏–∏
            gen_suffix = gen_suffix.replace("\\n", "\n").replace("\\t", "\t")
            all_steps.append(gen_suffix)

            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–¥–∞ –≤ subprocess
            success, error_msg = check_code_in_subprocess(gen_suffix, timeout=6)

            if success:
                logger.info(f"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–¥–∞ –ø—Ä–æ–π–¥–µ–Ω–∞ –Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏ {step + 1}")
                last_successful = gen_suffix
                return last_successful.strip()
            else:
                logger.error(f"‚ùå –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–¥–∞ –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞: {error_msg}")

                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± –æ—à–∏–±–∫–µ
                current_prompt = (
                    f"{base_prompt}\n\n"
                    f"Previous solution:\n{gen_suffix}\n\n"
                    f"The last attempt failed during automatic checking with the following error (exact text):\n"
                    f"```\n{error_msg}\n```\n\n"
                    "Analyze the code carefully and return a corrected, fully functional, syntactically correct Python function.\n"
                    "Keep the same function name and parameters. Fix all indentation and syntax errors and any runtime error "
                    "reported above. Do not include any explanations, comments or additional text ‚Äî return only the updated code.\n"
                )
                last_error = error_msg
                # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            continue

    logger.warning("‚ö†Ô∏è –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –Ω–µ –¥–∞–ª–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∫–æ–¥–∞.")

    if last_successful:
        logger.info("–í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —É—Å–ø–µ—à–Ω—É—é —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é.")
        return last_successful.strip()
    else:
        logger.info("–í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é (–±–µ–∑ —É—Å–ø–µ—à–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏).")
        return all_steps[-1].strip() if all_steps else ""


def generate_single_turn_code(
        prompt: str,
        tokenizer: AutoTokenizer,
        model,
        device: torch.device,
        generation_config: Optional[Dict[str, Any]] = None
) -> str:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ –±–µ–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–∏.

    Args:
        prompt: –ü—Ä–æ–º–ø—Ç –∑–∞–¥–∞—á–∏.
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –º–æ–¥–µ–ª–∏.
        model: –Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å.
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
        generation_config: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

    Returns:
        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥ –±–µ–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–∏.
    """

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # –°–±–æ—Ä —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö kwargs –¥–ª—è generate
    gen_kwargs = _clean_and_prepare_gen_kwargs(tokenizer, generation_config, inputs["input_ids"].shape[1])

    try:
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞
        with torch.no_grad():
            outputs = model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=inputs.get("attention_mask", None),
                **gen_kwargs
            )

        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—á–∏—Å—Ç–∫–∞
        gen_text = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True).strip()
        gen_text = strip_code_fences(gen_text)
        gen_text = textwrap.dedent(gen_text).strip()
        gen_text = gen_text.replace("\\n", "\n").replace("\\t", "\t")

        return gen_text
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞: {e}")
        return ""