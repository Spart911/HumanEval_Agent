{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9997857066323799,
  "eval_steps": 500,
  "global_step": 4666,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004285867352405443,
      "grad_norm": 10.19269847869873,
      "learning_rate": 7.000000000000001e-07,
      "loss": 3.3507,
      "step": 10
    },
    {
      "epoch": 0.008571734704810886,
      "grad_norm": 1.8009535074234009,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 3.4182,
      "step": 20
    },
    {
      "epoch": 0.01285760205721633,
      "grad_norm": 10.788662910461426,
      "learning_rate": 2.7000000000000004e-06,
      "loss": 3.1142,
      "step": 30
    },
    {
      "epoch": 0.017143469409621772,
      "grad_norm": 6.3501715660095215,
      "learning_rate": 3.7e-06,
      "loss": 3.0502,
      "step": 40
    },
    {
      "epoch": 0.021429336762027216,
      "grad_norm": 13.57762622833252,
      "learning_rate": 4.7e-06,
      "loss": 4.1554,
      "step": 50
    },
    {
      "epoch": 0.02571520411443266,
      "grad_norm": 13.090801239013672,
      "learning_rate": 4.992417677642981e-06,
      "loss": 3.6612,
      "step": 60
    },
    {
      "epoch": 0.0300010714668381,
      "grad_norm": 7.2000203132629395,
      "learning_rate": 4.981585788561526e-06,
      "loss": 3.0915,
      "step": 70
    },
    {
      "epoch": 0.034286938819243544,
      "grad_norm": 12.93870735168457,
      "learning_rate": 4.97075389948007e-06,
      "loss": 2.8133,
      "step": 80
    },
    {
      "epoch": 0.03857280617164899,
      "grad_norm": 8.832478523254395,
      "learning_rate": 4.959922010398614e-06,
      "loss": 2.8852,
      "step": 90
    },
    {
      "epoch": 0.04285867352405443,
      "grad_norm": 8.722970962524414,
      "learning_rate": 4.949090121317159e-06,
      "loss": 2.8553,
      "step": 100
    },
    {
      "epoch": 0.047144540876459876,
      "grad_norm": 7.528014659881592,
      "learning_rate": 4.938258232235703e-06,
      "loss": 2.1997,
      "step": 110
    },
    {
      "epoch": 0.05143040822886532,
      "grad_norm": 7.416879653930664,
      "learning_rate": 4.927426343154246e-06,
      "loss": 2.0616,
      "step": 120
    },
    {
      "epoch": 0.055716275581270756,
      "grad_norm": 3.8452117443084717,
      "learning_rate": 4.916594454072791e-06,
      "loss": 2.4826,
      "step": 130
    },
    {
      "epoch": 0.0600021429336762,
      "grad_norm": 2.3000776767730713,
      "learning_rate": 4.905762564991335e-06,
      "loss": 1.7681,
      "step": 140
    },
    {
      "epoch": 0.06428801028608165,
      "grad_norm": 2.962864637374878,
      "learning_rate": 4.894930675909879e-06,
      "loss": 1.9052,
      "step": 150
    },
    {
      "epoch": 0.06857387763848709,
      "grad_norm": 11.436185836791992,
      "learning_rate": 4.884098786828424e-06,
      "loss": 1.6788,
      "step": 160
    },
    {
      "epoch": 0.07285974499089254,
      "grad_norm": 1.1496460437774658,
      "learning_rate": 4.873266897746968e-06,
      "loss": 1.6925,
      "step": 170
    },
    {
      "epoch": 0.07714561234329798,
      "grad_norm": 1.555388331413269,
      "learning_rate": 4.862435008665511e-06,
      "loss": 1.6128,
      "step": 180
    },
    {
      "epoch": 0.08143147969570341,
      "grad_norm": 2.9694817066192627,
      "learning_rate": 4.851603119584056e-06,
      "loss": 1.5499,
      "step": 190
    },
    {
      "epoch": 0.08571734704810886,
      "grad_norm": 1.6913188695907593,
      "learning_rate": 4.8407712305026e-06,
      "loss": 1.4075,
      "step": 200
    },
    {
      "epoch": 0.0900032144005143,
      "grad_norm": 5.023242950439453,
      "learning_rate": 4.829939341421144e-06,
      "loss": 1.4077,
      "step": 210
    },
    {
      "epoch": 0.09428908175291975,
      "grad_norm": 2.4138479232788086,
      "learning_rate": 4.819107452339689e-06,
      "loss": 1.2426,
      "step": 220
    },
    {
      "epoch": 0.09857494910532519,
      "grad_norm": 2.065967559814453,
      "learning_rate": 4.808275563258233e-06,
      "loss": 1.1928,
      "step": 230
    },
    {
      "epoch": 0.10286081645773064,
      "grad_norm": 1.3058521747589111,
      "learning_rate": 4.7974436741767765e-06,
      "loss": 1.1466,
      "step": 240
    },
    {
      "epoch": 0.10714668381013608,
      "grad_norm": 4.474078178405762,
      "learning_rate": 4.786611785095321e-06,
      "loss": 1.273,
      "step": 250
    },
    {
      "epoch": 0.11143255116254151,
      "grad_norm": 4.471909999847412,
      "learning_rate": 4.775779896013865e-06,
      "loss": 1.1073,
      "step": 260
    },
    {
      "epoch": 0.11571841851494696,
      "grad_norm": 1.3171703815460205,
      "learning_rate": 4.764948006932409e-06,
      "loss": 1.231,
      "step": 270
    },
    {
      "epoch": 0.1200042858673524,
      "grad_norm": 1.0186667442321777,
      "learning_rate": 4.754116117850954e-06,
      "loss": 1.1379,
      "step": 280
    },
    {
      "epoch": 0.12429015321975785,
      "grad_norm": 1.2173006534576416,
      "learning_rate": 4.743284228769498e-06,
      "loss": 1.0369,
      "step": 290
    },
    {
      "epoch": 0.1285760205721633,
      "grad_norm": 1.767109751701355,
      "learning_rate": 4.733535528596187e-06,
      "loss": 1.0381,
      "step": 300
    },
    {
      "epoch": 0.13286188792456874,
      "grad_norm": 1.1273707151412964,
      "learning_rate": 4.722703639514732e-06,
      "loss": 0.9922,
      "step": 310
    },
    {
      "epoch": 0.13714775527697418,
      "grad_norm": 1.8130964040756226,
      "learning_rate": 4.711871750433276e-06,
      "loss": 1.0414,
      "step": 320
    },
    {
      "epoch": 0.1414336226293796,
      "grad_norm": 1.193823218345642,
      "learning_rate": 4.70103986135182e-06,
      "loss": 1.0339,
      "step": 330
    },
    {
      "epoch": 0.14571948998178508,
      "grad_norm": 1.1979973316192627,
      "learning_rate": 4.6902079722703646e-06,
      "loss": 1.0541,
      "step": 340
    },
    {
      "epoch": 0.15000535733419051,
      "grad_norm": 2.7614314556121826,
      "learning_rate": 4.679376083188908e-06,
      "loss": 0.9127,
      "step": 350
    },
    {
      "epoch": 0.15429122468659595,
      "grad_norm": 1.2294315099716187,
      "learning_rate": 4.668544194107452e-06,
      "loss": 0.9646,
      "step": 360
    },
    {
      "epoch": 0.1585770920390014,
      "grad_norm": 0.9793633222579956,
      "learning_rate": 4.657712305025997e-06,
      "loss": 0.9624,
      "step": 370
    },
    {
      "epoch": 0.16286295939140683,
      "grad_norm": 0.9787138104438782,
      "learning_rate": 4.646880415944541e-06,
      "loss": 0.9367,
      "step": 380
    },
    {
      "epoch": 0.1671488267438123,
      "grad_norm": 1.0385668277740479,
      "learning_rate": 4.636048526863085e-06,
      "loss": 0.8775,
      "step": 390
    },
    {
      "epoch": 0.17143469409621773,
      "grad_norm": 1.0271546840667725,
      "learning_rate": 4.62521663778163e-06,
      "loss": 0.9603,
      "step": 400
    },
    {
      "epoch": 0.17572056144862316,
      "grad_norm": 0.9762676954269409,
      "learning_rate": 4.614384748700174e-06,
      "loss": 0.7779,
      "step": 410
    },
    {
      "epoch": 0.1800064288010286,
      "grad_norm": 0.9116787314414978,
      "learning_rate": 4.603552859618718e-06,
      "loss": 0.7899,
      "step": 420
    },
    {
      "epoch": 0.18429229615343404,
      "grad_norm": 0.9306312799453735,
      "learning_rate": 4.5927209705372625e-06,
      "loss": 0.9125,
      "step": 430
    },
    {
      "epoch": 0.1885781635058395,
      "grad_norm": 0.7987822890281677,
      "learning_rate": 4.581889081455807e-06,
      "loss": 0.8572,
      "step": 440
    },
    {
      "epoch": 0.19286403085824494,
      "grad_norm": 1.2476435899734497,
      "learning_rate": 4.57105719237435e-06,
      "loss": 0.8914,
      "step": 450
    },
    {
      "epoch": 0.19714989821065038,
      "grad_norm": 0.8836965560913086,
      "learning_rate": 4.560225303292895e-06,
      "loss": 0.7999,
      "step": 460
    },
    {
      "epoch": 0.2014357655630558,
      "grad_norm": 3.9337310791015625,
      "learning_rate": 4.549393414211439e-06,
      "loss": 0.7861,
      "step": 470
    },
    {
      "epoch": 0.20572163291546128,
      "grad_norm": 1.0570436716079712,
      "learning_rate": 4.538561525129983e-06,
      "loss": 0.8546,
      "step": 480
    },
    {
      "epoch": 0.21000750026786671,
      "grad_norm": 0.9088846445083618,
      "learning_rate": 4.527729636048528e-06,
      "loss": 0.8205,
      "step": 490
    },
    {
      "epoch": 0.21429336762027215,
      "grad_norm": 1.153165340423584,
      "learning_rate": 4.516897746967072e-06,
      "loss": 0.8149,
      "step": 500
    },
    {
      "epoch": 0.2185792349726776,
      "grad_norm": 1.0236756801605225,
      "learning_rate": 4.506065857885615e-06,
      "loss": 0.8296,
      "step": 510
    },
    {
      "epoch": 0.22286510232508303,
      "grad_norm": 1.053029179573059,
      "learning_rate": 4.49523396880416e-06,
      "loss": 0.7606,
      "step": 520
    },
    {
      "epoch": 0.2271509696774885,
      "grad_norm": 0.9539627432823181,
      "learning_rate": 4.484402079722704e-06,
      "loss": 0.6826,
      "step": 530
    },
    {
      "epoch": 0.23143683702989393,
      "grad_norm": 1.1068402528762817,
      "learning_rate": 4.473570190641248e-06,
      "loss": 0.7772,
      "step": 540
    },
    {
      "epoch": 0.23572270438229936,
      "grad_norm": 1.150665521621704,
      "learning_rate": 4.462738301559793e-06,
      "loss": 0.7066,
      "step": 550
    },
    {
      "epoch": 0.2400085717347048,
      "grad_norm": 1.0768728256225586,
      "learning_rate": 4.451906412478337e-06,
      "loss": 0.8293,
      "step": 560
    },
    {
      "epoch": 0.24429443908711027,
      "grad_norm": 0.9314787983894348,
      "learning_rate": 4.441074523396881e-06,
      "loss": 0.7798,
      "step": 570
    },
    {
      "epoch": 0.2485803064395157,
      "grad_norm": 1.5265097618103027,
      "learning_rate": 4.430242634315425e-06,
      "loss": 0.7502,
      "step": 580
    },
    {
      "epoch": 0.25286617379192117,
      "grad_norm": 0.9139493703842163,
      "learning_rate": 4.419410745233969e-06,
      "loss": 0.6764,
      "step": 590
    },
    {
      "epoch": 0.2571520411443266,
      "grad_norm": 1.0279673337936401,
      "learning_rate": 4.408578856152513e-06,
      "loss": 0.7848,
      "step": 600
    },
    {
      "epoch": 0.26143790849673204,
      "grad_norm": 1.4454189538955688,
      "learning_rate": 4.397746967071058e-06,
      "loss": 0.7669,
      "step": 610
    },
    {
      "epoch": 0.2657237758491375,
      "grad_norm": 0.9570242762565613,
      "learning_rate": 4.386915077989602e-06,
      "loss": 0.7246,
      "step": 620
    },
    {
      "epoch": 0.2700096432015429,
      "grad_norm": 0.9343456625938416,
      "learning_rate": 4.376083188908146e-06,
      "loss": 0.7162,
      "step": 630
    },
    {
      "epoch": 0.27429551055394835,
      "grad_norm": 1.0602854490280151,
      "learning_rate": 4.36525129982669e-06,
      "loss": 0.7515,
      "step": 640
    },
    {
      "epoch": 0.2785813779063538,
      "grad_norm": 1.1893960237503052,
      "learning_rate": 4.354419410745234e-06,
      "loss": 0.7063,
      "step": 650
    },
    {
      "epoch": 0.2828672452587592,
      "grad_norm": 1.3889098167419434,
      "learning_rate": 4.343587521663778e-06,
      "loss": 0.6808,
      "step": 660
    },
    {
      "epoch": 0.28715311261116466,
      "grad_norm": 1.2061735391616821,
      "learning_rate": 4.332755632582323e-06,
      "loss": 0.7804,
      "step": 670
    },
    {
      "epoch": 0.29143897996357016,
      "grad_norm": 1.2037581205368042,
      "learning_rate": 4.321923743500867e-06,
      "loss": 0.7137,
      "step": 680
    },
    {
      "epoch": 0.2957248473159756,
      "grad_norm": 1.137209177017212,
      "learning_rate": 4.311091854419411e-06,
      "loss": 0.7012,
      "step": 690
    },
    {
      "epoch": 0.30001071466838103,
      "grad_norm": 1.1902202367782593,
      "learning_rate": 4.300259965337955e-06,
      "loss": 0.669,
      "step": 700
    },
    {
      "epoch": 0.30429658202078647,
      "grad_norm": 1.4461733102798462,
      "learning_rate": 4.289428076256499e-06,
      "loss": 0.7137,
      "step": 710
    },
    {
      "epoch": 0.3085824493731919,
      "grad_norm": 1.177813172340393,
      "learning_rate": 4.278596187175043e-06,
      "loss": 0.6825,
      "step": 720
    },
    {
      "epoch": 0.31286831672559734,
      "grad_norm": 1.1183420419692993,
      "learning_rate": 4.267764298093588e-06,
      "loss": 0.5498,
      "step": 730
    },
    {
      "epoch": 0.3171541840780028,
      "grad_norm": 1.1575071811676025,
      "learning_rate": 4.256932409012132e-06,
      "loss": 0.6401,
      "step": 740
    },
    {
      "epoch": 0.3214400514304082,
      "grad_norm": 7.64162540435791,
      "learning_rate": 4.246100519930676e-06,
      "loss": 0.6383,
      "step": 750
    },
    {
      "epoch": 0.32572591878281365,
      "grad_norm": 1.255079984664917,
      "learning_rate": 4.23526863084922e-06,
      "loss": 0.7129,
      "step": 760
    },
    {
      "epoch": 0.3300117861352191,
      "grad_norm": 1.5506333112716675,
      "learning_rate": 4.224436741767764e-06,
      "loss": 0.7394,
      "step": 770
    },
    {
      "epoch": 0.3342976534876246,
      "grad_norm": 1.2134592533111572,
      "learning_rate": 4.2136048526863085e-06,
      "loss": 0.7409,
      "step": 780
    },
    {
      "epoch": 0.33858352084003,
      "grad_norm": 1.4171383380889893,
      "learning_rate": 4.202772963604853e-06,
      "loss": 0.6674,
      "step": 790
    },
    {
      "epoch": 0.34286938819243545,
      "grad_norm": 1.433900237083435,
      "learning_rate": 4.191941074523397e-06,
      "loss": 0.679,
      "step": 800
    },
    {
      "epoch": 0.3471552555448409,
      "grad_norm": 1.2231507301330566,
      "learning_rate": 4.181109185441941e-06,
      "loss": 0.68,
      "step": 810
    },
    {
      "epoch": 0.3514411228972463,
      "grad_norm": 1.1186408996582031,
      "learning_rate": 4.170277296360486e-06,
      "loss": 0.6048,
      "step": 820
    },
    {
      "epoch": 0.35572699024965176,
      "grad_norm": 1.511538028717041,
      "learning_rate": 4.15944540727903e-06,
      "loss": 0.6659,
      "step": 830
    },
    {
      "epoch": 0.3600128576020572,
      "grad_norm": 1.2028781175613403,
      "learning_rate": 4.148613518197574e-06,
      "loss": 0.5799,
      "step": 840
    },
    {
      "epoch": 0.36429872495446264,
      "grad_norm": 1.5704145431518555,
      "learning_rate": 4.137781629116119e-06,
      "loss": 0.6157,
      "step": 850
    },
    {
      "epoch": 0.3685845923068681,
      "grad_norm": 1.310463547706604,
      "learning_rate": 4.126949740034662e-06,
      "loss": 0.668,
      "step": 860
    },
    {
      "epoch": 0.37287045965927357,
      "grad_norm": 1.8006763458251953,
      "learning_rate": 4.1161178509532064e-06,
      "loss": 0.6405,
      "step": 870
    },
    {
      "epoch": 0.377156327011679,
      "grad_norm": 1.4018893241882324,
      "learning_rate": 4.105285961871751e-06,
      "loss": 0.6509,
      "step": 880
    },
    {
      "epoch": 0.38144219436408444,
      "grad_norm": 1.0635138750076294,
      "learning_rate": 4.094454072790295e-06,
      "loss": 0.6676,
      "step": 890
    },
    {
      "epoch": 0.3857280617164899,
      "grad_norm": 1.1015722751617432,
      "learning_rate": 4.083622183708839e-06,
      "loss": 0.6229,
      "step": 900
    },
    {
      "epoch": 0.3900139290688953,
      "grad_norm": 1.2081326246261597,
      "learning_rate": 4.072790294627384e-06,
      "loss": 0.5858,
      "step": 910
    },
    {
      "epoch": 0.39429979642130075,
      "grad_norm": 1.3333593606948853,
      "learning_rate": 4.061958405545928e-06,
      "loss": 0.696,
      "step": 920
    },
    {
      "epoch": 0.3985856637737062,
      "grad_norm": 1.3719980716705322,
      "learning_rate": 4.051126516464472e-06,
      "loss": 0.5739,
      "step": 930
    },
    {
      "epoch": 0.4028715311261116,
      "grad_norm": 1.0606849193572998,
      "learning_rate": 4.040294627383016e-06,
      "loss": 0.5655,
      "step": 940
    },
    {
      "epoch": 0.40715739847851706,
      "grad_norm": 1.3026093244552612,
      "learning_rate": 4.02946273830156e-06,
      "loss": 0.6178,
      "step": 950
    },
    {
      "epoch": 0.41144326583092256,
      "grad_norm": 1.7999908924102783,
      "learning_rate": 4.018630849220104e-06,
      "loss": 0.5313,
      "step": 960
    },
    {
      "epoch": 0.415729133183328,
      "grad_norm": 1.525875210762024,
      "learning_rate": 4.007798960138649e-06,
      "loss": 0.607,
      "step": 970
    },
    {
      "epoch": 0.42001500053573343,
      "grad_norm": 1.605513334274292,
      "learning_rate": 3.996967071057193e-06,
      "loss": 0.5531,
      "step": 980
    },
    {
      "epoch": 0.42430086788813887,
      "grad_norm": 1.5119513273239136,
      "learning_rate": 3.986135181975737e-06,
      "loss": 0.6548,
      "step": 990
    },
    {
      "epoch": 0.4285867352405443,
      "grad_norm": 1.2787320613861084,
      "learning_rate": 3.975303292894281e-06,
      "loss": 0.5768,
      "step": 1000
    },
    {
      "epoch": 0.43287260259294974,
      "grad_norm": 1.4758243560791016,
      "learning_rate": 3.964471403812825e-06,
      "loss": 0.6582,
      "step": 1010
    },
    {
      "epoch": 0.4371584699453552,
      "grad_norm": 1.2317612171173096,
      "learning_rate": 3.9536395147313694e-06,
      "loss": 0.6117,
      "step": 1020
    },
    {
      "epoch": 0.4414443372977606,
      "grad_norm": 1.251670002937317,
      "learning_rate": 3.942807625649914e-06,
      "loss": 0.5588,
      "step": 1030
    },
    {
      "epoch": 0.44573020465016605,
      "grad_norm": 1.4878852367401123,
      "learning_rate": 3.931975736568458e-06,
      "loss": 0.5572,
      "step": 1040
    },
    {
      "epoch": 0.45001607200257154,
      "grad_norm": 1.18247389793396,
      "learning_rate": 3.921143847487002e-06,
      "loss": 0.6109,
      "step": 1050
    },
    {
      "epoch": 0.454301939354977,
      "grad_norm": 1.1960088014602661,
      "learning_rate": 3.910311958405546e-06,
      "loss": 0.5315,
      "step": 1060
    },
    {
      "epoch": 0.4585878067073824,
      "grad_norm": 1.5756752490997314,
      "learning_rate": 3.89948006932409e-06,
      "loss": 0.5874,
      "step": 1070
    },
    {
      "epoch": 0.46287367405978785,
      "grad_norm": 1.5033007860183716,
      "learning_rate": 3.8886481802426345e-06,
      "loss": 0.5977,
      "step": 1080
    },
    {
      "epoch": 0.4671595414121933,
      "grad_norm": 1.4177849292755127,
      "learning_rate": 3.877816291161179e-06,
      "loss": 0.6027,
      "step": 1090
    },
    {
      "epoch": 0.47144540876459873,
      "grad_norm": 1.6970202922821045,
      "learning_rate": 3.866984402079723e-06,
      "loss": 0.5958,
      "step": 1100
    },
    {
      "epoch": 0.47573127611700416,
      "grad_norm": 1.702669382095337,
      "learning_rate": 3.8561525129982674e-06,
      "loss": 0.5484,
      "step": 1110
    },
    {
      "epoch": 0.4800171434694096,
      "grad_norm": 1.193121075630188,
      "learning_rate": 3.845320623916811e-06,
      "loss": 0.4676,
      "step": 1120
    },
    {
      "epoch": 0.48430301082181504,
      "grad_norm": 1.3933732509613037,
      "learning_rate": 3.834488734835355e-06,
      "loss": 0.6053,
      "step": 1130
    },
    {
      "epoch": 0.48858887817422053,
      "grad_norm": 1.1633251905441284,
      "learning_rate": 3.8236568457538995e-06,
      "loss": 0.5772,
      "step": 1140
    },
    {
      "epoch": 0.49287474552662597,
      "grad_norm": 1.294140100479126,
      "learning_rate": 3.812824956672444e-06,
      "loss": 0.5428,
      "step": 1150
    },
    {
      "epoch": 0.4971606128790314,
      "grad_norm": 1.4894963502883911,
      "learning_rate": 3.801993067590988e-06,
      "loss": 0.5439,
      "step": 1160
    },
    {
      "epoch": 0.5014464802314368,
      "grad_norm": 1.3932174444198608,
      "learning_rate": 3.7911611785095325e-06,
      "loss": 0.5635,
      "step": 1170
    },
    {
      "epoch": 0.5057323475838423,
      "grad_norm": 1.9045659303665161,
      "learning_rate": 3.7803292894280764e-06,
      "loss": 0.5648,
      "step": 1180
    },
    {
      "epoch": 0.5100182149362478,
      "grad_norm": 1.4735013246536255,
      "learning_rate": 3.7694974003466207e-06,
      "loss": 0.5122,
      "step": 1190
    },
    {
      "epoch": 0.5143040822886532,
      "grad_norm": 1.3744878768920898,
      "learning_rate": 3.758665511265165e-06,
      "loss": 0.6,
      "step": 1200
    },
    {
      "epoch": 0.5185899496410586,
      "grad_norm": 1.5441983938217163,
      "learning_rate": 3.7478336221837093e-06,
      "loss": 0.6037,
      "step": 1210
    },
    {
      "epoch": 0.5228758169934641,
      "grad_norm": 2.0010976791381836,
      "learning_rate": 3.7370017331022536e-06,
      "loss": 0.4659,
      "step": 1220
    },
    {
      "epoch": 0.5271616843458695,
      "grad_norm": 2.194812536239624,
      "learning_rate": 3.726169844020798e-06,
      "loss": 0.4682,
      "step": 1230
    },
    {
      "epoch": 0.531447551698275,
      "grad_norm": 1.5712989568710327,
      "learning_rate": 3.7153379549393414e-06,
      "loss": 0.5581,
      "step": 1240
    },
    {
      "epoch": 0.5357334190506804,
      "grad_norm": 2.7097811698913574,
      "learning_rate": 3.7045060658578857e-06,
      "loss": 0.541,
      "step": 1250
    },
    {
      "epoch": 0.5400192864030858,
      "grad_norm": 1.591760277748108,
      "learning_rate": 3.69367417677643e-06,
      "loss": 0.5198,
      "step": 1260
    },
    {
      "epoch": 0.5443051537554913,
      "grad_norm": 1.3316611051559448,
      "learning_rate": 3.6828422876949743e-06,
      "loss": 0.5432,
      "step": 1270
    },
    {
      "epoch": 0.5485910211078967,
      "grad_norm": 1.2899127006530762,
      "learning_rate": 3.6720103986135186e-06,
      "loss": 0.5612,
      "step": 1280
    },
    {
      "epoch": 0.5528768884603021,
      "grad_norm": 1.4306495189666748,
      "learning_rate": 3.661178509532063e-06,
      "loss": 0.5812,
      "step": 1290
    },
    {
      "epoch": 0.5571627558127076,
      "grad_norm": 1.7553075551986694,
      "learning_rate": 3.6503466204506064e-06,
      "loss": 0.6184,
      "step": 1300
    },
    {
      "epoch": 0.561448623165113,
      "grad_norm": 1.8069440126419067,
      "learning_rate": 3.6395147313691507e-06,
      "loss": 0.5061,
      "step": 1310
    },
    {
      "epoch": 0.5657344905175185,
      "grad_norm": 1.6159651279449463,
      "learning_rate": 3.628682842287695e-06,
      "loss": 0.5829,
      "step": 1320
    },
    {
      "epoch": 0.5700203578699239,
      "grad_norm": 1.4940646886825562,
      "learning_rate": 3.6178509532062394e-06,
      "loss": 0.5526,
      "step": 1330
    },
    {
      "epoch": 0.5743062252223293,
      "grad_norm": 1.6136897802352905,
      "learning_rate": 3.6070190641247837e-06,
      "loss": 0.5266,
      "step": 1340
    },
    {
      "epoch": 0.5785920925747348,
      "grad_norm": 1.6407560110092163,
      "learning_rate": 3.596187175043328e-06,
      "loss": 0.5526,
      "step": 1350
    },
    {
      "epoch": 0.5828779599271403,
      "grad_norm": 1.5342203378677368,
      "learning_rate": 3.585355285961872e-06,
      "loss": 0.5112,
      "step": 1360
    },
    {
      "epoch": 0.5871638272795457,
      "grad_norm": 1.4929665327072144,
      "learning_rate": 3.574523396880416e-06,
      "loss": 0.4998,
      "step": 1370
    },
    {
      "epoch": 0.5914496946319512,
      "grad_norm": 1.5300381183624268,
      "learning_rate": 3.5636915077989605e-06,
      "loss": 0.5325,
      "step": 1380
    },
    {
      "epoch": 0.5957355619843566,
      "grad_norm": 2.5968310832977295,
      "learning_rate": 3.552859618717505e-06,
      "loss": 0.4945,
      "step": 1390
    },
    {
      "epoch": 0.6000214293367621,
      "grad_norm": 1.5290939807891846,
      "learning_rate": 3.5420277296360487e-06,
      "loss": 0.5114,
      "step": 1400
    },
    {
      "epoch": 0.6043072966891675,
      "grad_norm": 1.7374370098114014,
      "learning_rate": 3.531195840554593e-06,
      "loss": 0.5324,
      "step": 1410
    },
    {
      "epoch": 0.6085931640415729,
      "grad_norm": 1.3114396333694458,
      "learning_rate": 3.5203639514731373e-06,
      "loss": 0.5661,
      "step": 1420
    },
    {
      "epoch": 0.6128790313939784,
      "grad_norm": 1.5101549625396729,
      "learning_rate": 3.5095320623916812e-06,
      "loss": 0.5275,
      "step": 1430
    },
    {
      "epoch": 0.6171648987463838,
      "grad_norm": 1.6339350938796997,
      "learning_rate": 3.4987001733102256e-06,
      "loss": 0.4971,
      "step": 1440
    },
    {
      "epoch": 0.6214507660987892,
      "grad_norm": 2.0672662258148193,
      "learning_rate": 3.48786828422877e-06,
      "loss": 0.4423,
      "step": 1450
    },
    {
      "epoch": 0.6257366334511947,
      "grad_norm": 1.641098141670227,
      "learning_rate": 3.477036395147314e-06,
      "loss": 0.5565,
      "step": 1460
    },
    {
      "epoch": 0.6300225008036001,
      "grad_norm": 1.292019248008728,
      "learning_rate": 3.4662045060658585e-06,
      "loss": 0.4158,
      "step": 1470
    },
    {
      "epoch": 0.6343083681560056,
      "grad_norm": 1.437404990196228,
      "learning_rate": 3.455372616984403e-06,
      "loss": 0.5451,
      "step": 1480
    },
    {
      "epoch": 0.638594235508411,
      "grad_norm": 1.582450270652771,
      "learning_rate": 3.4445407279029463e-06,
      "loss": 0.5715,
      "step": 1490
    },
    {
      "epoch": 0.6428801028608164,
      "grad_norm": 1.4039548635482788,
      "learning_rate": 3.4337088388214906e-06,
      "loss": 0.4815,
      "step": 1500
    },
    {
      "epoch": 0.6471659702132219,
      "grad_norm": 1.77971351146698,
      "learning_rate": 3.422876949740035e-06,
      "loss": 0.5803,
      "step": 1510
    },
    {
      "epoch": 0.6514518375656273,
      "grad_norm": 1.68051016330719,
      "learning_rate": 3.4120450606585792e-06,
      "loss": 0.5121,
      "step": 1520
    },
    {
      "epoch": 0.6557377049180327,
      "grad_norm": 1.860501766204834,
      "learning_rate": 3.4012131715771235e-06,
      "loss": 0.4959,
      "step": 1530
    },
    {
      "epoch": 0.6600235722704382,
      "grad_norm": 1.6175034046173096,
      "learning_rate": 3.390381282495668e-06,
      "loss": 0.4748,
      "step": 1540
    },
    {
      "epoch": 0.6643094396228437,
      "grad_norm": 1.1471309661865234,
      "learning_rate": 3.3795493934142113e-06,
      "loss": 0.4572,
      "step": 1550
    },
    {
      "epoch": 0.6685953069752492,
      "grad_norm": 1.6365562677383423,
      "learning_rate": 3.3687175043327556e-06,
      "loss": 0.5413,
      "step": 1560
    },
    {
      "epoch": 0.6728811743276546,
      "grad_norm": 1.318750023841858,
      "learning_rate": 3.3578856152513e-06,
      "loss": 0.4642,
      "step": 1570
    },
    {
      "epoch": 0.67716704168006,
      "grad_norm": 2.2970385551452637,
      "learning_rate": 3.3470537261698443e-06,
      "loss": 0.6314,
      "step": 1580
    },
    {
      "epoch": 0.6814529090324655,
      "grad_norm": 1.6377310752868652,
      "learning_rate": 3.3362218370883886e-06,
      "loss": 0.5214,
      "step": 1590
    },
    {
      "epoch": 0.6857387763848709,
      "grad_norm": 1.906256914138794,
      "learning_rate": 3.325389948006933e-06,
      "loss": 0.5455,
      "step": 1600
    },
    {
      "epoch": 0.6900246437372763,
      "grad_norm": 2.230104923248291,
      "learning_rate": 3.3145580589254768e-06,
      "loss": 0.4768,
      "step": 1610
    },
    {
      "epoch": 0.6943105110896818,
      "grad_norm": 1.9113914966583252,
      "learning_rate": 3.303726169844021e-06,
      "loss": 0.5129,
      "step": 1620
    },
    {
      "epoch": 0.6985963784420872,
      "grad_norm": 1.2004691362380981,
      "learning_rate": 3.2928942807625654e-06,
      "loss": 0.4278,
      "step": 1630
    },
    {
      "epoch": 0.7028822457944927,
      "grad_norm": 1.6740309000015259,
      "learning_rate": 3.2820623916811097e-06,
      "loss": 0.538,
      "step": 1640
    },
    {
      "epoch": 0.7071681131468981,
      "grad_norm": 1.7652454376220703,
      "learning_rate": 3.2712305025996536e-06,
      "loss": 0.5478,
      "step": 1650
    },
    {
      "epoch": 0.7114539804993035,
      "grad_norm": 1.9702668190002441,
      "learning_rate": 3.260398613518198e-06,
      "loss": 0.5963,
      "step": 1660
    },
    {
      "epoch": 0.715739847851709,
      "grad_norm": 1.5030219554901123,
      "learning_rate": 3.249566724436742e-06,
      "loss": 0.5256,
      "step": 1670
    },
    {
      "epoch": 0.7200257152041144,
      "grad_norm": 1.2321991920471191,
      "learning_rate": 3.238734835355286e-06,
      "loss": 0.4451,
      "step": 1680
    },
    {
      "epoch": 0.7243115825565198,
      "grad_norm": 1.3020533323287964,
      "learning_rate": 3.2279029462738304e-06,
      "loss": 0.5064,
      "step": 1690
    },
    {
      "epoch": 0.7285974499089253,
      "grad_norm": 1.235582709312439,
      "learning_rate": 3.2170710571923748e-06,
      "loss": 0.5125,
      "step": 1700
    },
    {
      "epoch": 0.7328833172613307,
      "grad_norm": 1.5498758554458618,
      "learning_rate": 3.206239168110919e-06,
      "loss": 0.4316,
      "step": 1710
    },
    {
      "epoch": 0.7371691846137362,
      "grad_norm": 1.624643325805664,
      "learning_rate": 3.1954072790294634e-06,
      "loss": 0.4895,
      "step": 1720
    },
    {
      "epoch": 0.7414550519661417,
      "grad_norm": 1.7565275430679321,
      "learning_rate": 3.184575389948007e-06,
      "loss": 0.5411,
      "step": 1730
    },
    {
      "epoch": 0.7457409193185471,
      "grad_norm": 1.37533438205719,
      "learning_rate": 3.173743500866551e-06,
      "loss": 0.4653,
      "step": 1740
    },
    {
      "epoch": 0.7500267866709526,
      "grad_norm": 1.7530004978179932,
      "learning_rate": 3.1629116117850955e-06,
      "loss": 0.4865,
      "step": 1750
    },
    {
      "epoch": 0.754312654023358,
      "grad_norm": 1.3872008323669434,
      "learning_rate": 3.15207972270364e-06,
      "loss": 0.3918,
      "step": 1760
    },
    {
      "epoch": 0.7585985213757634,
      "grad_norm": 1.7091271877288818,
      "learning_rate": 3.141247833622184e-06,
      "loss": 0.5424,
      "step": 1770
    },
    {
      "epoch": 0.7628843887281689,
      "grad_norm": 1.4362632036209106,
      "learning_rate": 3.1304159445407284e-06,
      "loss": 0.4635,
      "step": 1780
    },
    {
      "epoch": 0.7671702560805743,
      "grad_norm": 1.7536413669586182,
      "learning_rate": 3.1195840554592723e-06,
      "loss": 0.5495,
      "step": 1790
    },
    {
      "epoch": 0.7714561234329798,
      "grad_norm": 1.2391815185546875,
      "learning_rate": 3.108752166377816e-06,
      "loss": 0.539,
      "step": 1800
    },
    {
      "epoch": 0.7757419907853852,
      "grad_norm": 1.9293549060821533,
      "learning_rate": 3.0979202772963605e-06,
      "loss": 0.5089,
      "step": 1810
    },
    {
      "epoch": 0.7800278581377906,
      "grad_norm": 1.5473499298095703,
      "learning_rate": 3.087088388214905e-06,
      "loss": 0.4871,
      "step": 1820
    },
    {
      "epoch": 0.7843137254901961,
      "grad_norm": 1.7211592197418213,
      "learning_rate": 3.076256499133449e-06,
      "loss": 0.5293,
      "step": 1830
    },
    {
      "epoch": 0.7885995928426015,
      "grad_norm": 1.6971074342727661,
      "learning_rate": 3.0654246100519935e-06,
      "loss": 0.4445,
      "step": 1840
    },
    {
      "epoch": 0.7928854601950069,
      "grad_norm": 2.0485522747039795,
      "learning_rate": 3.0545927209705373e-06,
      "loss": 0.431,
      "step": 1850
    },
    {
      "epoch": 0.7971713275474124,
      "grad_norm": 1.7121912240982056,
      "learning_rate": 3.0437608318890817e-06,
      "loss": 0.4789,
      "step": 1860
    },
    {
      "epoch": 0.8014571948998178,
      "grad_norm": 1.3142673969268799,
      "learning_rate": 3.032928942807626e-06,
      "loss": 0.5022,
      "step": 1870
    },
    {
      "epoch": 0.8057430622522233,
      "grad_norm": 2.1176135540008545,
      "learning_rate": 3.0220970537261703e-06,
      "loss": 0.5332,
      "step": 1880
    },
    {
      "epoch": 0.8100289296046287,
      "grad_norm": 2.259936809539795,
      "learning_rate": 3.0112651646447146e-06,
      "loss": 0.5401,
      "step": 1890
    },
    {
      "epoch": 0.8143147969570341,
      "grad_norm": 1.6098384857177734,
      "learning_rate": 3.0004332755632585e-06,
      "loss": 0.4931,
      "step": 1900
    },
    {
      "epoch": 0.8186006643094397,
      "grad_norm": 1.9685585498809814,
      "learning_rate": 2.9896013864818024e-06,
      "loss": 0.5793,
      "step": 1910
    },
    {
      "epoch": 0.8228865316618451,
      "grad_norm": 1.6272329092025757,
      "learning_rate": 2.9787694974003467e-06,
      "loss": 0.4903,
      "step": 1920
    },
    {
      "epoch": 0.8271723990142505,
      "grad_norm": 1.8950175046920776,
      "learning_rate": 2.967937608318891e-06,
      "loss": 0.5141,
      "step": 1930
    },
    {
      "epoch": 0.831458266366656,
      "grad_norm": 1.920289158821106,
      "learning_rate": 2.9571057192374353e-06,
      "loss": 0.485,
      "step": 1940
    },
    {
      "epoch": 0.8357441337190614,
      "grad_norm": 1.883015513420105,
      "learning_rate": 2.9462738301559796e-06,
      "loss": 0.4412,
      "step": 1950
    },
    {
      "epoch": 0.8400300010714669,
      "grad_norm": 1.8692830801010132,
      "learning_rate": 2.935441941074524e-06,
      "loss": 0.5494,
      "step": 1960
    },
    {
      "epoch": 0.8443158684238723,
      "grad_norm": 1.9714330434799194,
      "learning_rate": 2.9246100519930674e-06,
      "loss": 0.61,
      "step": 1970
    },
    {
      "epoch": 0.8486017357762777,
      "grad_norm": 1.7630953788757324,
      "learning_rate": 2.9137781629116117e-06,
      "loss": 0.5321,
      "step": 1980
    },
    {
      "epoch": 0.8528876031286832,
      "grad_norm": 1.682580828666687,
      "learning_rate": 2.902946273830156e-06,
      "loss": 0.4963,
      "step": 1990
    },
    {
      "epoch": 0.8571734704810886,
      "grad_norm": 1.7609893083572388,
      "learning_rate": 2.8921143847487004e-06,
      "loss": 0.553,
      "step": 2000
    },
    {
      "epoch": 0.861459337833494,
      "grad_norm": 1.8968031406402588,
      "learning_rate": 2.8812824956672447e-06,
      "loss": 0.5034,
      "step": 2010
    },
    {
      "epoch": 0.8657452051858995,
      "grad_norm": 1.6423627138137817,
      "learning_rate": 2.870450606585789e-06,
      "loss": 0.3871,
      "step": 2020
    },
    {
      "epoch": 0.8700310725383049,
      "grad_norm": 2.1457974910736084,
      "learning_rate": 2.859618717504333e-06,
      "loss": 0.5692,
      "step": 2030
    },
    {
      "epoch": 0.8743169398907104,
      "grad_norm": 1.8774127960205078,
      "learning_rate": 2.848786828422877e-06,
      "loss": 0.4958,
      "step": 2040
    },
    {
      "epoch": 0.8786028072431158,
      "grad_norm": 1.6309565305709839,
      "learning_rate": 2.8379549393414215e-06,
      "loss": 0.5027,
      "step": 2050
    },
    {
      "epoch": 0.8828886745955212,
      "grad_norm": 2.0260705947875977,
      "learning_rate": 2.8271230502599654e-06,
      "loss": 0.5996,
      "step": 2060
    },
    {
      "epoch": 0.8871745419479267,
      "grad_norm": 2.0186514854431152,
      "learning_rate": 2.8162911611785097e-06,
      "loss": 0.4972,
      "step": 2070
    },
    {
      "epoch": 0.8914604093003321,
      "grad_norm": 2.489467144012451,
      "learning_rate": 2.805459272097054e-06,
      "loss": 0.4606,
      "step": 2080
    },
    {
      "epoch": 0.8957462766527376,
      "grad_norm": 1.8808560371398926,
      "learning_rate": 2.794627383015598e-06,
      "loss": 0.4551,
      "step": 2090
    },
    {
      "epoch": 0.9000321440051431,
      "grad_norm": 3.086968183517456,
      "learning_rate": 2.7837954939341422e-06,
      "loss": 0.4411,
      "step": 2100
    },
    {
      "epoch": 0.9043180113575485,
      "grad_norm": 1.862233281135559,
      "learning_rate": 2.7729636048526865e-06,
      "loss": 0.5565,
      "step": 2110
    },
    {
      "epoch": 0.908603878709954,
      "grad_norm": 1.8628761768341064,
      "learning_rate": 2.762131715771231e-06,
      "loss": 0.5277,
      "step": 2120
    },
    {
      "epoch": 0.9128897460623594,
      "grad_norm": 1.8495084047317505,
      "learning_rate": 2.751299826689775e-06,
      "loss": 0.5162,
      "step": 2130
    },
    {
      "epoch": 0.9171756134147648,
      "grad_norm": 2.011612892150879,
      "learning_rate": 2.7404679376083195e-06,
      "loss": 0.5606,
      "step": 2140
    },
    {
      "epoch": 0.9214614807671703,
      "grad_norm": 1.6653430461883545,
      "learning_rate": 2.729636048526863e-06,
      "loss": 0.4942,
      "step": 2150
    },
    {
      "epoch": 0.9257473481195757,
      "grad_norm": 2.1707870960235596,
      "learning_rate": 2.7188041594454073e-06,
      "loss": 0.5053,
      "step": 2160
    },
    {
      "epoch": 0.9300332154719811,
      "grad_norm": 1.6290888786315918,
      "learning_rate": 2.7079722703639516e-06,
      "loss": 0.3545,
      "step": 2170
    },
    {
      "epoch": 0.9343190828243866,
      "grad_norm": 1.9144316911697388,
      "learning_rate": 2.697140381282496e-06,
      "loss": 0.6255,
      "step": 2180
    },
    {
      "epoch": 0.938604950176792,
      "grad_norm": 1.8620363473892212,
      "learning_rate": 2.68630849220104e-06,
      "loss": 0.5028,
      "step": 2190
    },
    {
      "epoch": 0.9428908175291975,
      "grad_norm": 2.655818462371826,
      "learning_rate": 2.6754766031195845e-06,
      "loss": 0.3856,
      "step": 2200
    },
    {
      "epoch": 0.9471766848816029,
      "grad_norm": 1.4566932916641235,
      "learning_rate": 2.664644714038129e-06,
      "loss": 0.4701,
      "step": 2210
    },
    {
      "epoch": 0.9514625522340083,
      "grad_norm": 2.0654711723327637,
      "learning_rate": 2.6538128249566723e-06,
      "loss": 0.4954,
      "step": 2220
    },
    {
      "epoch": 0.9557484195864138,
      "grad_norm": 1.9940658807754517,
      "learning_rate": 2.6429809358752166e-06,
      "loss": 0.5347,
      "step": 2230
    },
    {
      "epoch": 0.9600342869388192,
      "grad_norm": 1.2982088327407837,
      "learning_rate": 2.632149046793761e-06,
      "loss": 0.4886,
      "step": 2240
    },
    {
      "epoch": 0.9643201542912246,
      "grad_norm": 2.3942253589630127,
      "learning_rate": 2.6213171577123052e-06,
      "loss": 0.4444,
      "step": 2250
    },
    {
      "epoch": 0.9686060216436301,
      "grad_norm": 1.896156668663025,
      "learning_rate": 2.6104852686308496e-06,
      "loss": 0.4771,
      "step": 2260
    },
    {
      "epoch": 0.9728918889960356,
      "grad_norm": 2.0013225078582764,
      "learning_rate": 2.599653379549394e-06,
      "loss": 0.4709,
      "step": 2270
    },
    {
      "epoch": 0.9771777563484411,
      "grad_norm": 1.7138142585754395,
      "learning_rate": 2.5888214904679378e-06,
      "loss": 0.4331,
      "step": 2280
    },
    {
      "epoch": 0.9814636237008465,
      "grad_norm": 1.8954384326934814,
      "learning_rate": 2.577989601386482e-06,
      "loss": 0.4734,
      "step": 2290
    },
    {
      "epoch": 0.9857494910532519,
      "grad_norm": 2.0475704669952393,
      "learning_rate": 2.5671577123050264e-06,
      "loss": 0.486,
      "step": 2300
    },
    {
      "epoch": 0.9900353584056574,
      "grad_norm": 2.1417391300201416,
      "learning_rate": 2.5563258232235703e-06,
      "loss": 0.415,
      "step": 2310
    },
    {
      "epoch": 0.9943212257580628,
      "grad_norm": 2.1167640686035156,
      "learning_rate": 2.5454939341421146e-06,
      "loss": 0.5933,
      "step": 2320
    },
    {
      "epoch": 0.9986070931104682,
      "grad_norm": 1.811392068862915,
      "learning_rate": 2.534662045060659e-06,
      "loss": 0.4669,
      "step": 2330
    },
    {
      "epoch": 1.0028929604628736,
      "grad_norm": 2.0038442611694336,
      "learning_rate": 2.523830155979203e-06,
      "loss": 0.5097,
      "step": 2340
    },
    {
      "epoch": 1.0071788278152791,
      "grad_norm": 2.112273931503296,
      "learning_rate": 2.512998266897747e-06,
      "loss": 0.5069,
      "step": 2350
    },
    {
      "epoch": 1.0114646951676847,
      "grad_norm": 2.3791093826293945,
      "learning_rate": 2.5021663778162914e-06,
      "loss": 0.4766,
      "step": 2360
    },
    {
      "epoch": 1.01575056252009,
      "grad_norm": 2.152945041656494,
      "learning_rate": 2.4913344887348357e-06,
      "loss": 0.4514,
      "step": 2370
    },
    {
      "epoch": 1.0200364298724955,
      "grad_norm": 1.6088206768035889,
      "learning_rate": 2.4805025996533796e-06,
      "loss": 0.4698,
      "step": 2380
    },
    {
      "epoch": 1.0243222972249009,
      "grad_norm": 1.9239654541015625,
      "learning_rate": 2.469670710571924e-06,
      "loss": 0.4696,
      "step": 2390
    },
    {
      "epoch": 1.0286081645773064,
      "grad_norm": 1.575242042541504,
      "learning_rate": 2.4588388214904683e-06,
      "loss": 0.4845,
      "step": 2400
    },
    {
      "epoch": 1.0328940319297117,
      "grad_norm": 9.50206184387207,
      "learning_rate": 2.448006932409012e-06,
      "loss": 0.4324,
      "step": 2410
    },
    {
      "epoch": 1.0371798992821173,
      "grad_norm": 1.6026043891906738,
      "learning_rate": 2.4371750433275565e-06,
      "loss": 0.4229,
      "step": 2420
    },
    {
      "epoch": 1.0414657666345226,
      "grad_norm": 1.7004908323287964,
      "learning_rate": 2.4263431542461008e-06,
      "loss": 0.4783,
      "step": 2430
    },
    {
      "epoch": 1.0457516339869282,
      "grad_norm": 2.2126543521881104,
      "learning_rate": 2.4155112651646447e-06,
      "loss": 0.4743,
      "step": 2440
    },
    {
      "epoch": 1.0500375013393335,
      "grad_norm": 1.5620511770248413,
      "learning_rate": 2.404679376083189e-06,
      "loss": 0.501,
      "step": 2450
    },
    {
      "epoch": 1.054323368691739,
      "grad_norm": 2.311396598815918,
      "learning_rate": 2.3938474870017333e-06,
      "loss": 0.4941,
      "step": 2460
    },
    {
      "epoch": 1.0586092360441444,
      "grad_norm": 2.083380699157715,
      "learning_rate": 2.383015597920277e-06,
      "loss": 0.4886,
      "step": 2470
    },
    {
      "epoch": 1.06289510339655,
      "grad_norm": 2.2774600982666016,
      "learning_rate": 2.3721837088388215e-06,
      "loss": 0.5335,
      "step": 2480
    },
    {
      "epoch": 1.0671809707489552,
      "grad_norm": 2.301378011703491,
      "learning_rate": 2.361351819757366e-06,
      "loss": 0.5632,
      "step": 2490
    },
    {
      "epoch": 1.0714668381013608,
      "grad_norm": 1.4932959079742432,
      "learning_rate": 2.35051993067591e-06,
      "loss": 0.4552,
      "step": 2500
    },
    {
      "epoch": 1.0757527054537661,
      "grad_norm": 1.7324289083480835,
      "learning_rate": 2.339688041594454e-06,
      "loss": 0.5101,
      "step": 2510
    },
    {
      "epoch": 1.0800385728061717,
      "grad_norm": 1.8518378734588623,
      "learning_rate": 2.3288561525129983e-06,
      "loss": 0.4961,
      "step": 2520
    },
    {
      "epoch": 1.0843244401585772,
      "grad_norm": 2.3420932292938232,
      "learning_rate": 2.3180242634315426e-06,
      "loss": 0.5773,
      "step": 2530
    },
    {
      "epoch": 1.0886103075109825,
      "grad_norm": 1.8993152379989624,
      "learning_rate": 2.307192374350087e-06,
      "loss": 0.4692,
      "step": 2540
    },
    {
      "epoch": 1.092896174863388,
      "grad_norm": 2.256070137023926,
      "learning_rate": 2.2963604852686313e-06,
      "loss": 0.4808,
      "step": 2550
    },
    {
      "epoch": 1.0971820422157934,
      "grad_norm": 1.7388436794281006,
      "learning_rate": 2.285528596187175e-06,
      "loss": 0.4509,
      "step": 2560
    },
    {
      "epoch": 1.101467909568199,
      "grad_norm": 2.10270619392395,
      "learning_rate": 2.2746967071057195e-06,
      "loss": 0.4748,
      "step": 2570
    },
    {
      "epoch": 1.1057537769206043,
      "grad_norm": 1.6374585628509521,
      "learning_rate": 2.263864818024264e-06,
      "loss": 0.4774,
      "step": 2580
    },
    {
      "epoch": 1.1100396442730098,
      "grad_norm": 1.4617801904678345,
      "learning_rate": 2.2530329289428077e-06,
      "loss": 0.4407,
      "step": 2590
    },
    {
      "epoch": 1.1143255116254152,
      "grad_norm": 1.7799770832061768,
      "learning_rate": 2.242201039861352e-06,
      "loss": 0.4832,
      "step": 2600
    },
    {
      "epoch": 1.1186113789778207,
      "grad_norm": 1.6708523035049438,
      "learning_rate": 2.2313691507798963e-06,
      "loss": 0.4731,
      "step": 2610
    },
    {
      "epoch": 1.122897246330226,
      "grad_norm": 2.061755418777466,
      "learning_rate": 2.2205372616984406e-06,
      "loss": 0.4767,
      "step": 2620
    },
    {
      "epoch": 1.1271831136826316,
      "grad_norm": 1.8198977708816528,
      "learning_rate": 2.2097053726169845e-06,
      "loss": 0.4707,
      "step": 2630
    },
    {
      "epoch": 1.131468981035037,
      "grad_norm": 2.487381935119629,
      "learning_rate": 2.198873483535529e-06,
      "loss": 0.5071,
      "step": 2640
    },
    {
      "epoch": 1.1357548483874425,
      "grad_norm": 1.983696699142456,
      "learning_rate": 2.188041594454073e-06,
      "loss": 0.4057,
      "step": 2650
    },
    {
      "epoch": 1.1400407157398478,
      "grad_norm": 2.362008571624756,
      "learning_rate": 2.177209705372617e-06,
      "loss": 0.4384,
      "step": 2660
    },
    {
      "epoch": 1.1443265830922533,
      "grad_norm": 1.8640344142913818,
      "learning_rate": 2.1663778162911613e-06,
      "loss": 0.3628,
      "step": 2670
    },
    {
      "epoch": 1.1486124504446587,
      "grad_norm": 2.268422842025757,
      "learning_rate": 2.1555459272097057e-06,
      "loss": 0.5201,
      "step": 2680
    },
    {
      "epoch": 1.1528983177970642,
      "grad_norm": 2.3636345863342285,
      "learning_rate": 2.1447140381282496e-06,
      "loss": 0.4062,
      "step": 2690
    },
    {
      "epoch": 1.1571841851494695,
      "grad_norm": 1.6066102981567383,
      "learning_rate": 2.133882149046794e-06,
      "loss": 0.5253,
      "step": 2700
    },
    {
      "epoch": 1.161470052501875,
      "grad_norm": 1.839414358139038,
      "learning_rate": 2.123050259965338e-06,
      "loss": 0.4803,
      "step": 2710
    },
    {
      "epoch": 1.1657559198542806,
      "grad_norm": 1.7112228870391846,
      "learning_rate": 2.112218370883882e-06,
      "loss": 0.4347,
      "step": 2720
    },
    {
      "epoch": 1.170041787206686,
      "grad_norm": 1.8029766082763672,
      "learning_rate": 2.1013864818024264e-06,
      "loss": 0.4738,
      "step": 2730
    },
    {
      "epoch": 1.1743276545590913,
      "grad_norm": 2.1004865169525146,
      "learning_rate": 2.0905545927209707e-06,
      "loss": 0.4461,
      "step": 2740
    },
    {
      "epoch": 1.1786135219114968,
      "grad_norm": 1.8659604787826538,
      "learning_rate": 2.079722703639515e-06,
      "loss": 0.5363,
      "step": 2750
    },
    {
      "epoch": 1.1828993892639024,
      "grad_norm": 2.138235569000244,
      "learning_rate": 2.0688908145580593e-06,
      "loss": 0.5444,
      "step": 2760
    },
    {
      "epoch": 1.1871852566163077,
      "grad_norm": 1.6865326166152954,
      "learning_rate": 2.0580589254766032e-06,
      "loss": 0.5165,
      "step": 2770
    },
    {
      "epoch": 1.1914711239687132,
      "grad_norm": 2.213385581970215,
      "learning_rate": 2.0472270363951475e-06,
      "loss": 0.5258,
      "step": 2780
    },
    {
      "epoch": 1.1957569913211186,
      "grad_norm": 2.067246675491333,
      "learning_rate": 2.036395147313692e-06,
      "loss": 0.5004,
      "step": 2790
    },
    {
      "epoch": 1.2000428586735241,
      "grad_norm": 2.301250457763672,
      "learning_rate": 2.025563258232236e-06,
      "loss": 0.4433,
      "step": 2800
    },
    {
      "epoch": 1.2043287260259294,
      "grad_norm": 1.5549873113632202,
      "learning_rate": 2.01473136915078e-06,
      "loss": 0.4404,
      "step": 2810
    },
    {
      "epoch": 1.208614593378335,
      "grad_norm": 1.7368122339248657,
      "learning_rate": 2.0038994800693244e-06,
      "loss": 0.4383,
      "step": 2820
    },
    {
      "epoch": 1.2129004607307403,
      "grad_norm": 2.1240532398223877,
      "learning_rate": 1.9930675909878687e-06,
      "loss": 0.4695,
      "step": 2830
    },
    {
      "epoch": 1.2171863280831459,
      "grad_norm": 1.8910138607025146,
      "learning_rate": 1.9822357019064126e-06,
      "loss": 0.4881,
      "step": 2840
    },
    {
      "epoch": 1.2214721954355512,
      "grad_norm": 2.2565853595733643,
      "learning_rate": 1.971403812824957e-06,
      "loss": 0.4651,
      "step": 2850
    },
    {
      "epoch": 1.2257580627879567,
      "grad_norm": 1.809495449066162,
      "learning_rate": 1.960571923743501e-06,
      "loss": 0.3735,
      "step": 2860
    },
    {
      "epoch": 1.230043930140362,
      "grad_norm": 1.9616756439208984,
      "learning_rate": 1.949740034662045e-06,
      "loss": 0.3913,
      "step": 2870
    },
    {
      "epoch": 1.2343297974927676,
      "grad_norm": 2.210624933242798,
      "learning_rate": 1.9389081455805894e-06,
      "loss": 0.4677,
      "step": 2880
    },
    {
      "epoch": 1.238615664845173,
      "grad_norm": 1.7182807922363281,
      "learning_rate": 1.9280762564991337e-06,
      "loss": 0.5271,
      "step": 2890
    },
    {
      "epoch": 1.2429015321975785,
      "grad_norm": 1.4106121063232422,
      "learning_rate": 1.9172443674176776e-06,
      "loss": 0.4776,
      "step": 2900
    },
    {
      "epoch": 1.247187399549984,
      "grad_norm": 1.9398447275161743,
      "learning_rate": 1.906412478336222e-06,
      "loss": 0.4838,
      "step": 2910
    },
    {
      "epoch": 1.2514732669023894,
      "grad_norm": 1.7491782903671265,
      "learning_rate": 1.8955805892547662e-06,
      "loss": 0.4561,
      "step": 2920
    },
    {
      "epoch": 1.2557591342547947,
      "grad_norm": 2.4230942726135254,
      "learning_rate": 1.8847487001733103e-06,
      "loss": 0.4245,
      "step": 2930
    },
    {
      "epoch": 1.2600450016072002,
      "grad_norm": 2.1148788928985596,
      "learning_rate": 1.8739168110918546e-06,
      "loss": 0.4951,
      "step": 2940
    },
    {
      "epoch": 1.2643308689596058,
      "grad_norm": 2.2831668853759766,
      "learning_rate": 1.863084922010399e-06,
      "loss": 0.4347,
      "step": 2950
    },
    {
      "epoch": 1.268616736312011,
      "grad_norm": 2.2794671058654785,
      "learning_rate": 1.8522530329289429e-06,
      "loss": 0.5276,
      "step": 2960
    },
    {
      "epoch": 1.2729026036644167,
      "grad_norm": 2.070704460144043,
      "learning_rate": 1.8414211438474872e-06,
      "loss": 0.4654,
      "step": 2970
    },
    {
      "epoch": 1.277188471016822,
      "grad_norm": 1.9240748882293701,
      "learning_rate": 1.8305892547660315e-06,
      "loss": 0.4385,
      "step": 2980
    },
    {
      "epoch": 1.2814743383692275,
      "grad_norm": 1.6081435680389404,
      "learning_rate": 1.8197573656845754e-06,
      "loss": 0.4079,
      "step": 2990
    },
    {
      "epoch": 1.2857602057216329,
      "grad_norm": 1.9966120719909668,
      "learning_rate": 1.8089254766031197e-06,
      "loss": 0.4154,
      "step": 3000
    },
    {
      "epoch": 1.2900460730740384,
      "grad_norm": 1.777551531791687,
      "learning_rate": 1.798093587521664e-06,
      "loss": 0.4663,
      "step": 3010
    },
    {
      "epoch": 1.2943319404264437,
      "grad_norm": 2.288695812225342,
      "learning_rate": 1.787261698440208e-06,
      "loss": 0.4923,
      "step": 3020
    },
    {
      "epoch": 1.2986178077788493,
      "grad_norm": 2.51379132270813,
      "learning_rate": 1.7764298093587524e-06,
      "loss": 0.4518,
      "step": 3030
    },
    {
      "epoch": 1.3029036751312546,
      "grad_norm": 1.9548959732055664,
      "learning_rate": 1.7655979202772965e-06,
      "loss": 0.617,
      "step": 3040
    },
    {
      "epoch": 1.3071895424836601,
      "grad_norm": 1.6143277883529663,
      "learning_rate": 1.7547660311958406e-06,
      "loss": 0.4635,
      "step": 3050
    },
    {
      "epoch": 1.3114754098360657,
      "grad_norm": 1.7471833229064941,
      "learning_rate": 1.743934142114385e-06,
      "loss": 0.4197,
      "step": 3060
    },
    {
      "epoch": 1.315761277188471,
      "grad_norm": 3.088665723800659,
      "learning_rate": 1.7331022530329292e-06,
      "loss": 0.5318,
      "step": 3070
    },
    {
      "epoch": 1.3200471445408763,
      "grad_norm": 2.005258321762085,
      "learning_rate": 1.7222703639514731e-06,
      "loss": 0.4409,
      "step": 3080
    },
    {
      "epoch": 1.324333011893282,
      "grad_norm": 2.1720807552337646,
      "learning_rate": 1.7114384748700175e-06,
      "loss": 0.4713,
      "step": 3090
    },
    {
      "epoch": 1.3286188792456874,
      "grad_norm": 2.1129586696624756,
      "learning_rate": 1.7006065857885618e-06,
      "loss": 0.4622,
      "step": 3100
    },
    {
      "epoch": 1.3329047465980928,
      "grad_norm": 2.2043862342834473,
      "learning_rate": 1.6897746967071057e-06,
      "loss": 0.5393,
      "step": 3110
    },
    {
      "epoch": 1.337190613950498,
      "grad_norm": 2.1500015258789062,
      "learning_rate": 1.67894280762565e-06,
      "loss": 0.4882,
      "step": 3120
    },
    {
      "epoch": 1.3414764813029036,
      "grad_norm": 2.3763580322265625,
      "learning_rate": 1.6681109185441943e-06,
      "loss": 0.4383,
      "step": 3130
    },
    {
      "epoch": 1.3457623486553092,
      "grad_norm": 2.319101333618164,
      "learning_rate": 1.6572790294627384e-06,
      "loss": 0.4364,
      "step": 3140
    },
    {
      "epoch": 1.3500482160077145,
      "grad_norm": 1.945813536643982,
      "learning_rate": 1.6464471403812827e-06,
      "loss": 0.4578,
      "step": 3150
    },
    {
      "epoch": 1.35433408336012,
      "grad_norm": 2.868724822998047,
      "learning_rate": 1.6356152512998268e-06,
      "loss": 0.4902,
      "step": 3160
    },
    {
      "epoch": 1.3586199507125254,
      "grad_norm": 1.7527955770492554,
      "learning_rate": 1.624783362218371e-06,
      "loss": 0.4911,
      "step": 3170
    },
    {
      "epoch": 1.362905818064931,
      "grad_norm": 2.397796630859375,
      "learning_rate": 1.6139514731369152e-06,
      "loss": 0.5002,
      "step": 3180
    },
    {
      "epoch": 1.3671916854173363,
      "grad_norm": 2.1112711429595947,
      "learning_rate": 1.6031195840554595e-06,
      "loss": 0.4423,
      "step": 3190
    },
    {
      "epoch": 1.3714775527697418,
      "grad_norm": 1.470816731452942,
      "learning_rate": 1.5922876949740034e-06,
      "loss": 0.4341,
      "step": 3200
    },
    {
      "epoch": 1.3757634201221471,
      "grad_norm": 1.523217797279358,
      "learning_rate": 1.5814558058925477e-06,
      "loss": 0.4669,
      "step": 3210
    },
    {
      "epoch": 1.3800492874745527,
      "grad_norm": 1.9421583414077759,
      "learning_rate": 1.570623916811092e-06,
      "loss": 0.5322,
      "step": 3220
    },
    {
      "epoch": 1.384335154826958,
      "grad_norm": 2.2994155883789062,
      "learning_rate": 1.5597920277296362e-06,
      "loss": 0.411,
      "step": 3230
    },
    {
      "epoch": 1.3886210221793636,
      "grad_norm": 1.9640508890151978,
      "learning_rate": 1.5489601386481803e-06,
      "loss": 0.5127,
      "step": 3240
    },
    {
      "epoch": 1.392906889531769,
      "grad_norm": 2.2315967082977295,
      "learning_rate": 1.5381282495667246e-06,
      "loss": 0.5045,
      "step": 3250
    },
    {
      "epoch": 1.3971927568841744,
      "grad_norm": 2.358121156692505,
      "learning_rate": 1.5272963604852687e-06,
      "loss": 0.4915,
      "step": 3260
    },
    {
      "epoch": 1.4014786242365798,
      "grad_norm": 1.6022588014602661,
      "learning_rate": 1.516464471403813e-06,
      "loss": 0.4622,
      "step": 3270
    },
    {
      "epoch": 1.4057644915889853,
      "grad_norm": 1.8345803022384644,
      "learning_rate": 1.5056325823223573e-06,
      "loss": 0.476,
      "step": 3280
    },
    {
      "epoch": 1.4100503589413909,
      "grad_norm": 1.9884370565414429,
      "learning_rate": 1.4948006932409012e-06,
      "loss": 0.4323,
      "step": 3290
    },
    {
      "epoch": 1.4143362262937962,
      "grad_norm": 2.974045753479004,
      "learning_rate": 1.4839688041594455e-06,
      "loss": 0.4801,
      "step": 3300
    },
    {
      "epoch": 1.4186220936462017,
      "grad_norm": 2.034954071044922,
      "learning_rate": 1.4731369150779898e-06,
      "loss": 0.4367,
      "step": 3310
    },
    {
      "epoch": 1.422907960998607,
      "grad_norm": 2.103086233139038,
      "learning_rate": 1.4623050259965337e-06,
      "loss": 0.5025,
      "step": 3320
    },
    {
      "epoch": 1.4271938283510126,
      "grad_norm": 2.181706666946411,
      "learning_rate": 1.451473136915078e-06,
      "loss": 0.4195,
      "step": 3330
    },
    {
      "epoch": 1.431479695703418,
      "grad_norm": 1.9057247638702393,
      "learning_rate": 1.4406412478336223e-06,
      "loss": 0.462,
      "step": 3340
    },
    {
      "epoch": 1.4357655630558235,
      "grad_norm": 2.862332582473755,
      "learning_rate": 1.4298093587521664e-06,
      "loss": 0.4574,
      "step": 3350
    },
    {
      "epoch": 1.4400514304082288,
      "grad_norm": 1.8451871871948242,
      "learning_rate": 1.4189774696707108e-06,
      "loss": 0.4011,
      "step": 3360
    },
    {
      "epoch": 1.4443372977606344,
      "grad_norm": 1.6380486488342285,
      "learning_rate": 1.4081455805892549e-06,
      "loss": 0.414,
      "step": 3370
    },
    {
      "epoch": 1.4486231651130397,
      "grad_norm": 1.7149356603622437,
      "learning_rate": 1.397313691507799e-06,
      "loss": 0.4378,
      "step": 3380
    },
    {
      "epoch": 1.4529090324654452,
      "grad_norm": 2.473114490509033,
      "learning_rate": 1.3864818024263433e-06,
      "loss": 0.4968,
      "step": 3390
    },
    {
      "epoch": 1.4571948998178508,
      "grad_norm": 2.124302864074707,
      "learning_rate": 1.3756499133448876e-06,
      "loss": 0.4586,
      "step": 3400
    },
    {
      "epoch": 1.461480767170256,
      "grad_norm": 2.192800760269165,
      "learning_rate": 1.3648180242634315e-06,
      "loss": 0.4657,
      "step": 3410
    },
    {
      "epoch": 1.4657666345226614,
      "grad_norm": 2.1154191493988037,
      "learning_rate": 1.3539861351819758e-06,
      "loss": 0.4352,
      "step": 3420
    },
    {
      "epoch": 1.470052501875067,
      "grad_norm": 2.276768207550049,
      "learning_rate": 1.34315424610052e-06,
      "loss": 0.442,
      "step": 3430
    },
    {
      "epoch": 1.4743383692274725,
      "grad_norm": 1.860365390777588,
      "learning_rate": 1.3323223570190644e-06,
      "loss": 0.4818,
      "step": 3440
    },
    {
      "epoch": 1.4786242365798778,
      "grad_norm": 2.6598966121673584,
      "learning_rate": 1.3214904679376083e-06,
      "loss": 0.4485,
      "step": 3450
    },
    {
      "epoch": 1.4829101039322832,
      "grad_norm": 2.274477481842041,
      "learning_rate": 1.3106585788561526e-06,
      "loss": 0.4852,
      "step": 3460
    },
    {
      "epoch": 1.4871959712846887,
      "grad_norm": 1.6364235877990723,
      "learning_rate": 1.299826689774697e-06,
      "loss": 0.4989,
      "step": 3470
    },
    {
      "epoch": 1.4914818386370943,
      "grad_norm": 1.6809707880020142,
      "learning_rate": 1.288994800693241e-06,
      "loss": 0.3768,
      "step": 3480
    },
    {
      "epoch": 1.4957677059894996,
      "grad_norm": 1.911318063735962,
      "learning_rate": 1.2781629116117851e-06,
      "loss": 0.4471,
      "step": 3490
    },
    {
      "epoch": 1.500053573341905,
      "grad_norm": 1.8940989971160889,
      "learning_rate": 1.2673310225303295e-06,
      "loss": 0.4394,
      "step": 3500
    },
    {
      "epoch": 1.5043394406943105,
      "grad_norm": 2.3327369689941406,
      "learning_rate": 1.2564991334488736e-06,
      "loss": 0.4591,
      "step": 3510
    },
    {
      "epoch": 1.508625308046716,
      "grad_norm": 1.8213741779327393,
      "learning_rate": 1.2456672443674179e-06,
      "loss": 0.4665,
      "step": 3520
    },
    {
      "epoch": 1.5129111753991213,
      "grad_norm": 1.51250159740448,
      "learning_rate": 1.234835355285962e-06,
      "loss": 0.5244,
      "step": 3530
    },
    {
      "epoch": 1.517197042751527,
      "grad_norm": 1.9224803447723389,
      "learning_rate": 1.224003466204506e-06,
      "loss": 0.3824,
      "step": 3540
    },
    {
      "epoch": 1.5214829101039324,
      "grad_norm": 1.9424301385879517,
      "learning_rate": 1.2131715771230504e-06,
      "loss": 0.4866,
      "step": 3550
    },
    {
      "epoch": 1.5257687774563378,
      "grad_norm": 2.1708614826202393,
      "learning_rate": 1.2023396880415945e-06,
      "loss": 0.4914,
      "step": 3560
    },
    {
      "epoch": 1.530054644808743,
      "grad_norm": 1.8920530080795288,
      "learning_rate": 1.1915077989601386e-06,
      "loss": 0.4936,
      "step": 3570
    },
    {
      "epoch": 1.5343405121611486,
      "grad_norm": 1.791519284248352,
      "learning_rate": 1.180675909878683e-06,
      "loss": 0.4677,
      "step": 3580
    },
    {
      "epoch": 1.5386263795135542,
      "grad_norm": 1.7973359823226929,
      "learning_rate": 1.169844020797227e-06,
      "loss": 0.4283,
      "step": 3590
    },
    {
      "epoch": 1.5429122468659595,
      "grad_norm": 2.107656478881836,
      "learning_rate": 1.1590121317157713e-06,
      "loss": 0.4459,
      "step": 3600
    },
    {
      "epoch": 1.5471981142183648,
      "grad_norm": 2.0936214923858643,
      "learning_rate": 1.1481802426343156e-06,
      "loss": 0.491,
      "step": 3610
    },
    {
      "epoch": 1.5514839815707704,
      "grad_norm": 1.6498702764511108,
      "learning_rate": 1.1373483535528597e-06,
      "loss": 0.4775,
      "step": 3620
    },
    {
      "epoch": 1.555769848923176,
      "grad_norm": 1.8895268440246582,
      "learning_rate": 1.1265164644714038e-06,
      "loss": 0.5225,
      "step": 3630
    },
    {
      "epoch": 1.5600557162755813,
      "grad_norm": 2.0267062187194824,
      "learning_rate": 1.1156845753899482e-06,
      "loss": 0.4947,
      "step": 3640
    },
    {
      "epoch": 1.5643415836279866,
      "grad_norm": 2.149815559387207,
      "learning_rate": 1.1048526863084923e-06,
      "loss": 0.4723,
      "step": 3650
    },
    {
      "epoch": 1.5686274509803921,
      "grad_norm": 2.0905063152313232,
      "learning_rate": 1.0940207972270366e-06,
      "loss": 0.4379,
      "step": 3660
    },
    {
      "epoch": 1.5729133183327977,
      "grad_norm": 2.4955735206604004,
      "learning_rate": 1.0831889081455807e-06,
      "loss": 0.4119,
      "step": 3670
    },
    {
      "epoch": 1.577199185685203,
      "grad_norm": 1.8739396333694458,
      "learning_rate": 1.0723570190641248e-06,
      "loss": 0.4666,
      "step": 3680
    },
    {
      "epoch": 1.5814850530376083,
      "grad_norm": 1.6398699283599854,
      "learning_rate": 1.061525129982669e-06,
      "loss": 0.4975,
      "step": 3690
    },
    {
      "epoch": 1.5857709203900139,
      "grad_norm": 2.126710891723633,
      "learning_rate": 1.0506932409012132e-06,
      "loss": 0.4429,
      "step": 3700
    },
    {
      "epoch": 1.5900567877424194,
      "grad_norm": 2.3852851390838623,
      "learning_rate": 1.0398613518197575e-06,
      "loss": 0.4224,
      "step": 3710
    },
    {
      "epoch": 1.5943426550948248,
      "grad_norm": 1.6014659404754639,
      "learning_rate": 1.0290294627383016e-06,
      "loss": 0.5504,
      "step": 3720
    },
    {
      "epoch": 1.5986285224472303,
      "grad_norm": 1.8896913528442383,
      "learning_rate": 1.018197573656846e-06,
      "loss": 0.469,
      "step": 3730
    },
    {
      "epoch": 1.6029143897996359,
      "grad_norm": 2.1451196670532227,
      "learning_rate": 1.00736568457539e-06,
      "loss": 0.4999,
      "step": 3740
    },
    {
      "epoch": 1.6072002571520412,
      "grad_norm": 1.6464682817459106,
      "learning_rate": 9.965337954939343e-07,
      "loss": 0.4598,
      "step": 3750
    },
    {
      "epoch": 1.6114861245044465,
      "grad_norm": 1.9752670526504517,
      "learning_rate": 9.857019064124784e-07,
      "loss": 0.489,
      "step": 3760
    },
    {
      "epoch": 1.615771991856852,
      "grad_norm": 2.0390591621398926,
      "learning_rate": 9.748700173310225e-07,
      "loss": 0.4872,
      "step": 3770
    },
    {
      "epoch": 1.6200578592092576,
      "grad_norm": 1.618204116821289,
      "learning_rate": 9.640381282495669e-07,
      "loss": 0.4191,
      "step": 3780
    },
    {
      "epoch": 1.624343726561663,
      "grad_norm": 1.2832615375518799,
      "learning_rate": 9.53206239168111e-07,
      "loss": 0.4058,
      "step": 3790
    },
    {
      "epoch": 1.6286295939140683,
      "grad_norm": 1.981155514717102,
      "learning_rate": 9.423743500866552e-07,
      "loss": 0.4454,
      "step": 3800
    },
    {
      "epoch": 1.6329154612664738,
      "grad_norm": 2.2480859756469727,
      "learning_rate": 9.315424610051995e-07,
      "loss": 0.4612,
      "step": 3810
    },
    {
      "epoch": 1.6372013286188793,
      "grad_norm": 2.026015281677246,
      "learning_rate": 9.207105719237436e-07,
      "loss": 0.4089,
      "step": 3820
    },
    {
      "epoch": 1.6414871959712847,
      "grad_norm": 2.3459060192108154,
      "learning_rate": 9.098786828422877e-07,
      "loss": 0.4254,
      "step": 3830
    },
    {
      "epoch": 1.64577306332369,
      "grad_norm": 2.1048762798309326,
      "learning_rate": 8.99046793760832e-07,
      "loss": 0.4721,
      "step": 3840
    },
    {
      "epoch": 1.6500589306760955,
      "grad_norm": 1.8060386180877686,
      "learning_rate": 8.882149046793762e-07,
      "loss": 0.4271,
      "step": 3850
    },
    {
      "epoch": 1.654344798028501,
      "grad_norm": 2.0653443336486816,
      "learning_rate": 8.773830155979203e-07,
      "loss": 0.5048,
      "step": 3860
    },
    {
      "epoch": 1.6586306653809064,
      "grad_norm": 2.113750696182251,
      "learning_rate": 8.665511265164646e-07,
      "loss": 0.3979,
      "step": 3870
    },
    {
      "epoch": 1.6629165327333117,
      "grad_norm": 1.9494960308074951,
      "learning_rate": 8.557192374350087e-07,
      "loss": 0.5046,
      "step": 3880
    },
    {
      "epoch": 1.6672024000857173,
      "grad_norm": 2.4527645111083984,
      "learning_rate": 8.448873483535528e-07,
      "loss": 0.586,
      "step": 3890
    },
    {
      "epoch": 1.6714882674381228,
      "grad_norm": 2.0831973552703857,
      "learning_rate": 8.340554592720971e-07,
      "loss": 0.5021,
      "step": 3900
    },
    {
      "epoch": 1.6757741347905282,
      "grad_norm": 1.7955328226089478,
      "learning_rate": 8.232235701906414e-07,
      "loss": 0.4246,
      "step": 3910
    },
    {
      "epoch": 1.6800600021429337,
      "grad_norm": 1.9944499731063843,
      "learning_rate": 8.123916811091855e-07,
      "loss": 0.4996,
      "step": 3920
    },
    {
      "epoch": 1.6843458694953393,
      "grad_norm": 1.5284435749053955,
      "learning_rate": 8.015597920277298e-07,
      "loss": 0.4263,
      "step": 3930
    },
    {
      "epoch": 1.6886317368477446,
      "grad_norm": 1.6624491214752197,
      "learning_rate": 7.907279029462739e-07,
      "loss": 0.4152,
      "step": 3940
    },
    {
      "epoch": 1.69291760420015,
      "grad_norm": 1.9895156621932983,
      "learning_rate": 7.798960138648181e-07,
      "loss": 0.4232,
      "step": 3950
    },
    {
      "epoch": 1.6972034715525555,
      "grad_norm": 3.529010772705078,
      "learning_rate": 7.690641247833623e-07,
      "loss": 0.5382,
      "step": 3960
    },
    {
      "epoch": 1.701489338904961,
      "grad_norm": 2.397034168243408,
      "learning_rate": 7.582322357019065e-07,
      "loss": 0.4289,
      "step": 3970
    },
    {
      "epoch": 1.7057752062573663,
      "grad_norm": 1.6538865566253662,
      "learning_rate": 7.474003466204506e-07,
      "loss": 0.4139,
      "step": 3980
    },
    {
      "epoch": 1.7100610736097717,
      "grad_norm": 1.7860782146453857,
      "learning_rate": 7.365684575389949e-07,
      "loss": 0.5088,
      "step": 3990
    },
    {
      "epoch": 1.7143469409621772,
      "grad_norm": 1.4547218084335327,
      "learning_rate": 7.25736568457539e-07,
      "loss": 0.4115,
      "step": 4000
    },
    {
      "epoch": 1.7186328083145828,
      "grad_norm": 1.9944077730178833,
      "learning_rate": 7.149046793760832e-07,
      "loss": 0.4616,
      "step": 4010
    },
    {
      "epoch": 1.722918675666988,
      "grad_norm": 1.978515625,
      "learning_rate": 7.040727902946274e-07,
      "loss": 0.4875,
      "step": 4020
    },
    {
      "epoch": 1.7272045430193934,
      "grad_norm": 1.4248846769332886,
      "learning_rate": 6.932409012131716e-07,
      "loss": 0.4125,
      "step": 4030
    },
    {
      "epoch": 1.731490410371799,
      "grad_norm": 2.447880744934082,
      "learning_rate": 6.824090121317157e-07,
      "loss": 0.4117,
      "step": 4040
    },
    {
      "epoch": 1.7357762777242045,
      "grad_norm": 2.3380608558654785,
      "learning_rate": 6.7157712305026e-07,
      "loss": 0.4558,
      "step": 4050
    },
    {
      "epoch": 1.7400621450766098,
      "grad_norm": 2.663728952407837,
      "learning_rate": 6.607452339688042e-07,
      "loss": 0.4342,
      "step": 4060
    },
    {
      "epoch": 1.7443480124290152,
      "grad_norm": 1.9271684885025024,
      "learning_rate": 6.499133448873485e-07,
      "loss": 0.4628,
      "step": 4070
    },
    {
      "epoch": 1.748633879781421,
      "grad_norm": 2.110672950744629,
      "learning_rate": 6.390814558058926e-07,
      "loss": 0.4564,
      "step": 4080
    },
    {
      "epoch": 1.7529197471338263,
      "grad_norm": 2.507469654083252,
      "learning_rate": 6.282495667244368e-07,
      "loss": 0.4154,
      "step": 4090
    },
    {
      "epoch": 1.7572056144862316,
      "grad_norm": 1.4894450902938843,
      "learning_rate": 6.17417677642981e-07,
      "loss": 0.4122,
      "step": 4100
    },
    {
      "epoch": 1.7614914818386371,
      "grad_norm": 2.070936679840088,
      "learning_rate": 6.065857885615252e-07,
      "loss": 0.4454,
      "step": 4110
    },
    {
      "epoch": 1.7657773491910427,
      "grad_norm": 2.208948850631714,
      "learning_rate": 5.957538994800693e-07,
      "loss": 0.4717,
      "step": 4120
    },
    {
      "epoch": 1.770063216543448,
      "grad_norm": 1.429029107093811,
      "learning_rate": 5.849220103986135e-07,
      "loss": 0.4745,
      "step": 4130
    },
    {
      "epoch": 1.7743490838958533,
      "grad_norm": 2.1541190147399902,
      "learning_rate": 5.740901213171578e-07,
      "loss": 0.4936,
      "step": 4140
    },
    {
      "epoch": 1.7786349512482589,
      "grad_norm": 1.736661434173584,
      "learning_rate": 5.632582322357019e-07,
      "loss": 0.4554,
      "step": 4150
    },
    {
      "epoch": 1.7829208186006644,
      "grad_norm": 1.6976815462112427,
      "learning_rate": 5.524263431542461e-07,
      "loss": 0.4759,
      "step": 4160
    },
    {
      "epoch": 1.7872066859530698,
      "grad_norm": 1.722944974899292,
      "learning_rate": 5.415944540727903e-07,
      "loss": 0.4736,
      "step": 4170
    },
    {
      "epoch": 1.791492553305475,
      "grad_norm": 1.6508605480194092,
      "learning_rate": 5.307625649913345e-07,
      "loss": 0.5263,
      "step": 4180
    },
    {
      "epoch": 1.7957784206578806,
      "grad_norm": 1.5710704326629639,
      "learning_rate": 5.199306759098788e-07,
      "loss": 0.4985,
      "step": 4190
    },
    {
      "epoch": 1.8000642880102862,
      "grad_norm": 1.7029448747634888,
      "learning_rate": 5.09098786828423e-07,
      "loss": 0.4323,
      "step": 4200
    },
    {
      "epoch": 1.8043501553626915,
      "grad_norm": 1.6985162496566772,
      "learning_rate": 4.982668977469672e-07,
      "loss": 0.4607,
      "step": 4210
    },
    {
      "epoch": 1.8086360227150968,
      "grad_norm": 2.2781119346618652,
      "learning_rate": 4.874350086655113e-07,
      "loss": 0.4526,
      "step": 4220
    },
    {
      "epoch": 1.8129218900675024,
      "grad_norm": 2.12555193901062,
      "learning_rate": 4.766031195840555e-07,
      "loss": 0.4968,
      "step": 4230
    },
    {
      "epoch": 1.817207757419908,
      "grad_norm": 2.2358202934265137,
      "learning_rate": 4.6577123050259974e-07,
      "loss": 0.5353,
      "step": 4240
    },
    {
      "epoch": 1.8214936247723132,
      "grad_norm": 1.8210413455963135,
      "learning_rate": 4.5493934142114384e-07,
      "loss": 0.4883,
      "step": 4250
    },
    {
      "epoch": 1.8257794921247188,
      "grad_norm": 1.648436188697815,
      "learning_rate": 4.441074523396881e-07,
      "loss": 0.431,
      "step": 4260
    },
    {
      "epoch": 1.8300653594771243,
      "grad_norm": 1.7581865787506104,
      "learning_rate": 4.332755632582323e-07,
      "loss": 0.4439,
      "step": 4270
    },
    {
      "epoch": 1.8343512268295297,
      "grad_norm": 1.9767519235610962,
      "learning_rate": 4.224436741767764e-07,
      "loss": 0.4341,
      "step": 4280
    },
    {
      "epoch": 1.838637094181935,
      "grad_norm": 2.0293736457824707,
      "learning_rate": 4.116117850953207e-07,
      "loss": 0.4424,
      "step": 4290
    },
    {
      "epoch": 1.8429229615343405,
      "grad_norm": 1.9054415225982666,
      "learning_rate": 4.007798960138649e-07,
      "loss": 0.3749,
      "step": 4300
    },
    {
      "epoch": 1.847208828886746,
      "grad_norm": 2.7650952339172363,
      "learning_rate": 3.8994800693240904e-07,
      "loss": 0.5331,
      "step": 4310
    },
    {
      "epoch": 1.8514946962391514,
      "grad_norm": 1.8669992685317993,
      "learning_rate": 3.7911611785095325e-07,
      "loss": 0.4766,
      "step": 4320
    },
    {
      "epoch": 1.8557805635915567,
      "grad_norm": 2.3828656673431396,
      "learning_rate": 3.6828422876949745e-07,
      "loss": 0.4196,
      "step": 4330
    },
    {
      "epoch": 1.8600664309439623,
      "grad_norm": 1.5358442068099976,
      "learning_rate": 3.574523396880416e-07,
      "loss": 0.471,
      "step": 4340
    },
    {
      "epoch": 1.8643522982963678,
      "grad_norm": 3.1627278327941895,
      "learning_rate": 3.466204506065858e-07,
      "loss": 0.5169,
      "step": 4350
    },
    {
      "epoch": 1.8686381656487732,
      "grad_norm": 2.445443630218506,
      "learning_rate": 3.3578856152513e-07,
      "loss": 0.3919,
      "step": 4360
    },
    {
      "epoch": 1.8729240330011785,
      "grad_norm": 1.469793677330017,
      "learning_rate": 3.2495667244367423e-07,
      "loss": 0.3742,
      "step": 4370
    },
    {
      "epoch": 1.877209900353584,
      "grad_norm": 2.689624786376953,
      "learning_rate": 3.141247833622184e-07,
      "loss": 0.4393,
      "step": 4380
    },
    {
      "epoch": 1.8814957677059896,
      "grad_norm": 1.665643334388733,
      "learning_rate": 3.032928942807626e-07,
      "loss": 0.43,
      "step": 4390
    },
    {
      "epoch": 1.885781635058395,
      "grad_norm": 2.0362744331359863,
      "learning_rate": 2.9246100519930675e-07,
      "loss": 0.474,
      "step": 4400
    },
    {
      "epoch": 1.8900675024108002,
      "grad_norm": 1.8809700012207031,
      "learning_rate": 2.8162911611785096e-07,
      "loss": 0.4684,
      "step": 4410
    },
    {
      "epoch": 1.8943533697632058,
      "grad_norm": 2.1078643798828125,
      "learning_rate": 2.7079722703639517e-07,
      "loss": 0.3972,
      "step": 4420
    },
    {
      "epoch": 1.8986392371156113,
      "grad_norm": 1.6143110990524292,
      "learning_rate": 2.599653379549394e-07,
      "loss": 0.4703,
      "step": 4430
    },
    {
      "epoch": 1.9029251044680167,
      "grad_norm": 1.6876146793365479,
      "learning_rate": 2.491334488734836e-07,
      "loss": 0.4354,
      "step": 4440
    },
    {
      "epoch": 1.9072109718204222,
      "grad_norm": 1.5375676155090332,
      "learning_rate": 2.3830155979202774e-07,
      "loss": 0.471,
      "step": 4450
    },
    {
      "epoch": 1.9114968391728278,
      "grad_norm": 2.6837964057922363,
      "learning_rate": 2.2746967071057192e-07,
      "loss": 0.399,
      "step": 4460
    },
    {
      "epoch": 1.915782706525233,
      "grad_norm": 2.254037380218506,
      "learning_rate": 2.1663778162911616e-07,
      "loss": 0.4699,
      "step": 4470
    },
    {
      "epoch": 1.9200685738776384,
      "grad_norm": 1.9636503458023071,
      "learning_rate": 2.0580589254766034e-07,
      "loss": 0.441,
      "step": 4480
    },
    {
      "epoch": 1.924354441230044,
      "grad_norm": 2.4772841930389404,
      "learning_rate": 1.9497400346620452e-07,
      "loss": 0.5231,
      "step": 4490
    },
    {
      "epoch": 1.9286403085824495,
      "grad_norm": 2.3721964359283447,
      "learning_rate": 1.8414211438474873e-07,
      "loss": 0.4376,
      "step": 4500
    },
    {
      "epoch": 1.9329261759348548,
      "grad_norm": 2.5540401935577393,
      "learning_rate": 1.733102253032929e-07,
      "loss": 0.4617,
      "step": 4510
    },
    {
      "epoch": 1.9372120432872602,
      "grad_norm": 2.3688130378723145,
      "learning_rate": 1.6247833622183712e-07,
      "loss": 0.4712,
      "step": 4520
    },
    {
      "epoch": 1.9414979106396657,
      "grad_norm": 1.907326340675354,
      "learning_rate": 1.516464471403813e-07,
      "loss": 0.417,
      "step": 4530
    },
    {
      "epoch": 1.9457837779920713,
      "grad_norm": 1.918882966041565,
      "learning_rate": 1.4081455805892548e-07,
      "loss": 0.3953,
      "step": 4540
    },
    {
      "epoch": 1.9500696453444766,
      "grad_norm": 2.525657892227173,
      "learning_rate": 1.299826689774697e-07,
      "loss": 0.4209,
      "step": 4550
    },
    {
      "epoch": 1.954355512696882,
      "grad_norm": 1.7366143465042114,
      "learning_rate": 1.1915077989601387e-07,
      "loss": 0.502,
      "step": 4560
    },
    {
      "epoch": 1.9586413800492875,
      "grad_norm": 1.5816043615341187,
      "learning_rate": 1.0831889081455808e-07,
      "loss": 0.4803,
      "step": 4570
    },
    {
      "epoch": 1.962927247401693,
      "grad_norm": 1.7454344034194946,
      "learning_rate": 9.748700173310226e-08,
      "loss": 0.4377,
      "step": 4580
    },
    {
      "epoch": 1.9672131147540983,
      "grad_norm": 2.6687402725219727,
      "learning_rate": 8.665511265164645e-08,
      "loss": 0.4759,
      "step": 4590
    },
    {
      "epoch": 1.9714989821065037,
      "grad_norm": 2.6484179496765137,
      "learning_rate": 7.582322357019065e-08,
      "loss": 0.51,
      "step": 4600
    },
    {
      "epoch": 1.9757848494589092,
      "grad_norm": 1.869915246963501,
      "learning_rate": 6.499133448873484e-08,
      "loss": 0.4883,
      "step": 4610
    },
    {
      "epoch": 1.9800707168113147,
      "grad_norm": 1.6469899415969849,
      "learning_rate": 5.415944540727904e-08,
      "loss": 0.4628,
      "step": 4620
    },
    {
      "epoch": 1.98435658416372,
      "grad_norm": 2.5764589309692383,
      "learning_rate": 4.332755632582323e-08,
      "loss": 0.4507,
      "step": 4630
    },
    {
      "epoch": 1.9886424515161256,
      "grad_norm": 1.9473695755004883,
      "learning_rate": 3.249566724436742e-08,
      "loss": 0.377,
      "step": 4640
    },
    {
      "epoch": 1.9929283188685312,
      "grad_norm": 1.8388713598251343,
      "learning_rate": 2.1663778162911614e-08,
      "loss": 0.4541,
      "step": 4650
    },
    {
      "epoch": 1.9972141862209365,
      "grad_norm": 1.9214297533035278,
      "learning_rate": 1.0831889081455807e-08,
      "loss": 0.5182,
      "step": 4660
    }
  ],
  "logging_steps": 10,
  "max_steps": 4666,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.5930525826940928e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
