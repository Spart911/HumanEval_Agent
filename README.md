# Проект бенчмарков LLM

Н хорошо структурированный Python проект для бенчмаркинга генерации кода моделями LLM, с фокусом на модульности, правильном разделении задач и расширяемости.

## Структура проекта

```
├── src/                 # Исходные модули
│   ├── utils/          # Утилиты (обработка кода, окружение)
│   ├── models/         # Загрузка моделей и генерация кода
│   ├── data/           # Загрузка и обработка датасетов
│   ├── benchmarks/     # Запуск бенчмарков
│   ├── evaluation/     # Запуск тестов и метрики
│   └── config/         # Управление конфигурацией
├── configs/            # YAML файлы конфигурации
├── tests/              # Тестовые файлы
├── scripts/            # Вспомогательные скрипты
├── docs/               # Документация
├── main.py             # Основной файл входа
├── main_original.py    # Оригинальный main.py (сохранен)
├── .env                # Переменные окружения
└── requirements.txt    # Зависимости Python
```

## Возможности

- **Модульная архитектура**: Хорошо разделенные компоненты для разных функций
- **Поддержка LoRA адаптеров**: Может использовать как обычные модели, так и адаптированные модули LoRA
- **Комплексный бенчмаркинг**: Реализует надежную оценку на стандартных бенчмарках, таких как HumanEval
- **Итеративное самокорректирование (AgentChain)**: Автоматически уточняет сгенерированный код на основе ошибок валидации (можно отключить)
- **Управление конфигурацией**: Конфигурация через CLI и YAML
- **Детальное отслеживание результатов**: Сохранение и сравнение результатов бенчмарков

## Использование

### Базовое использование

```bash
# Запуск бенчмарка с конфигурацией по умолчанию
python main.py

# Запуск с пользовательскими аргументами
python main.py --model-path путь/к/модели --limit 10 --temperature 0.7

# Запуск без AgentChain (один проход генерации)
python main.py --model-path путь/к/модели --no-use-agent-chain

# Использование файла конфигурации
python main.py --config configs/example_config.yaml
```

### Программное использование модулей

```python
from src.benchmarks import BenchmarkManager

# Создание менеджера бенчмарков
manager = BenchmarkManager(
    model_path="pdg_trained_github",
    base_model_path="Qwen/Qwen2.5-Coder-3B",
    use_lora=True
)

# Загрузка модели
manager.load_model()

# Запуск бенчмарка
result = manager.run_humaneval(limit=5, iterations=2)
print(f"Pass@1: {result['pass_rate']:.2f}%")

# Сохранение результатов
manager.save_results("results.json")
```

## Конфигурация

### Переменные окружения (.env)

```env
KEY_HUGGINGFACE=hf_ваш_ключ_huggingface
MAIN_MODEL_PATH=pdg_trained_github
BASE_MODEL_PATH=Qwen/Qwen2.5-Coder-3B
```

### Конфигурация YAML

```yaml
model:
  model_path: "pdg_trained_github"
  base_model_path: "Qwen/Qwen2.5-Coder-3B"
  use_lora: true
  device: "auto"
  torch_dtype: "float16"

generation:
  max_new_tokens: 400
  temperature: 0.6
  top_k: 40

benchmark:
  dataset: "humaneval"
  iterations: 3
  verbose: true
```

## Аргументы командной строки

```
Конфигурация модели:
  --model-path PATH      Путь к модели
  --base-model-path PATH Путь к базовой модели (для LoRA)
  --use-lora             Загружать как LoRA
  --device CHOICE        Устройство для использования (auto, cuda, cpu)

Конфигурация генерации:
  --max-new-tokens INT   Максимальное количество токенов для генерации
  --temperature FLOAT    Температура генерации

Конфигурация бенчмарка:
  --dataset CHOICE       Датасет для бенчмарка (humaneval, mbpp)
  --limit INT            Ограничить количество примеров
  --iterations INT       Количество итераций самокорректирования
  --output-file FILE     Файл вывода результатов
  --config FILE          Путь к файлу конфигурации
  --verbose              Подробный вывод
  --no-use-agent-chain   Отключить AgentChain для итеративного исправления кода
```

## Требования

- Python 3.8+
- PyTorch 2.0+
- transformers
- datasets
- peft
- huggingface_hub
- PyYAML
- tqdm

Установка:
```bash
pip install -r requirements.txt
```

## Возможности бенчмарка

### Бенчмарк HumanEval

Бенчмарк HumanEval оценивает способность модели решать задачи программирования на Python. Он измеряет метрику Pass@k, где k указывает на количество попыток, которые получает модель.

Основные возможности:
- Поддержка итеративного самокорректирования
- Проверка синтаксиса и выполнения
- Детальная обработка ошибок
- Несколько метрик помимо Pass@1

### Возможности генерации кода

- Автоматическая промпт-инженерия
- Обнаружение ошибок и самокорректирование
- Безопасное выполнение в изолированных средах
- Несколько стратегий генерации

## Детали архитектуры

### Модуль utils

- `environment.py`: Конфигурация окружения и управление устройством
- `code_utils.py`: Обработка кода, валидация и манипуляции

### Модуль models

- `model_loader.py`: Загрузка моделей с поддержкой адаптеров LoRA и квантизации
- `code_generator.py`: Генерация кода с итеративным самокорректированием

### Модуль data

- `dataset_loader.py`: Загрузка датасетов, таких как HumanEval и MBPP

### Модуль benchmarks

- `humaneval_benchmark.py`: Запуск бенчмарка HumanEval
- `benchmark_manager.py`: Менеджер для координации бенчмарков

### Модуль evaluation

- `test_runner.py`: Запуск тестов в изолированных средах
- `evaluation_metrics.py`: Метрики для оценки, включая Pass@k

### Модуль config

- `config_manager.py`: Управление конфигурацией с поддержкой CLI и YAML

## Русификация проекта

Все сообщения логирования, комментарии в коде и выводы переведены на русский язык. Это включает:

- Весь вывод в консоль на русском языке
- Docstrings и комментарии в коде
- Сообщения об ошибках
- Справочные сообщения

Для запуска демо с русифицированными сообщениями:
```bash
python run_demo.py
```

## Устранение неполадок

- Убедитесь, что у вас достаточно видеопамяти для вашей модели
- Для больших моделей рассмотрите использование квантизации
- При ошибках памяти попробуйте уменьшить `--limit` или использовать `--batch-size`
- Для отладки используйте флаг `--verbose` для более детального вывода

## Миграция с оригинального кода

Оригинальный файл main.py сохранен как `main_original.py`. Новая модульная структура предоставляет:

1. Лучшую организацию и сопровождаемость
2. Более простое тестирование и отладку
3. Расширяемость для новых бенчмарков
4. Лучшее управление конфигурацией
5. Переиспользуемые компоненты