{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 5000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004,
      "grad_norm": 2.826978921890259,
      "learning_rate": 8.000000000000001e-07,
      "loss": 1.1915,
      "step": 10
    },
    {
      "epoch": 0.008,
      "grad_norm": 1.3782882690429688,
      "learning_rate": 1.8000000000000001e-06,
      "loss": 0.8989,
      "step": 20
    },
    {
      "epoch": 0.012,
      "grad_norm": 5.43340539932251,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 1.2928,
      "step": 30
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.4132675528526306,
      "learning_rate": 3.8000000000000005e-06,
      "loss": 0.9741,
      "step": 40
    },
    {
      "epoch": 0.02,
      "grad_norm": 4.900010108947754,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.9409,
      "step": 50
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.5611685514450073,
      "learning_rate": 4.991919191919192e-06,
      "loss": 0.9489,
      "step": 60
    },
    {
      "epoch": 0.028,
      "grad_norm": 8.622583389282227,
      "learning_rate": 4.981818181818182e-06,
      "loss": 1.251,
      "step": 70
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.43310093879699707,
      "learning_rate": 4.971717171717172e-06,
      "loss": 1.1638,
      "step": 80
    },
    {
      "epoch": 0.036,
      "grad_norm": 3.6678802967071533,
      "learning_rate": 4.962626262626263e-06,
      "loss": 1.2122,
      "step": 90
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.4756256639957428,
      "learning_rate": 4.952525252525253e-06,
      "loss": 0.9767,
      "step": 100
    },
    {
      "epoch": 0.044,
      "grad_norm": 3.850782632827759,
      "learning_rate": 4.942424242424243e-06,
      "loss": 1.3627,
      "step": 110
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.6357998251914978,
      "learning_rate": 4.932323232323233e-06,
      "loss": 0.8464,
      "step": 120
    },
    {
      "epoch": 0.052,
      "grad_norm": 0.48873963952064514,
      "learning_rate": 4.922222222222223e-06,
      "loss": 0.7101,
      "step": 130
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.611420214176178,
      "learning_rate": 4.912121212121212e-06,
      "loss": 0.864,
      "step": 140
    },
    {
      "epoch": 0.06,
      "grad_norm": 2.990586042404175,
      "learning_rate": 4.902020202020203e-06,
      "loss": 0.9936,
      "step": 150
    },
    {
      "epoch": 0.064,
      "grad_norm": 3.3840644359588623,
      "learning_rate": 4.891919191919192e-06,
      "loss": 0.9027,
      "step": 160
    },
    {
      "epoch": 0.068,
      "grad_norm": 6.0270795822143555,
      "learning_rate": 4.881818181818182e-06,
      "loss": 0.8576,
      "step": 170
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.4964207112789154,
      "learning_rate": 4.871717171717172e-06,
      "loss": 0.8618,
      "step": 180
    },
    {
      "epoch": 0.076,
      "grad_norm": 1.2762707471847534,
      "learning_rate": 4.861616161616162e-06,
      "loss": 0.874,
      "step": 190
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.6222186088562012,
      "learning_rate": 4.851515151515152e-06,
      "loss": 0.7859,
      "step": 200
    },
    {
      "epoch": 0.084,
      "grad_norm": 3.364349126815796,
      "learning_rate": 4.841414141414142e-06,
      "loss": 0.7374,
      "step": 210
    },
    {
      "epoch": 0.088,
      "grad_norm": 2.323843002319336,
      "learning_rate": 4.831313131313132e-06,
      "loss": 0.7402,
      "step": 220
    },
    {
      "epoch": 0.092,
      "grad_norm": 1.4826629161834717,
      "learning_rate": 4.8212121212121215e-06,
      "loss": 0.7596,
      "step": 230
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.6109187602996826,
      "learning_rate": 4.811111111111111e-06,
      "loss": 0.7461,
      "step": 240
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.4807072579860687,
      "learning_rate": 4.801010101010101e-06,
      "loss": 0.6923,
      "step": 250
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.7883253693580627,
      "learning_rate": 4.790909090909091e-06,
      "loss": 0.6372,
      "step": 260
    },
    {
      "epoch": 0.108,
      "grad_norm": 0.5074913501739502,
      "learning_rate": 4.780808080808082e-06,
      "loss": 0.6949,
      "step": 270
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.4951828122138977,
      "learning_rate": 4.770707070707071e-06,
      "loss": 0.6642,
      "step": 280
    },
    {
      "epoch": 0.116,
      "grad_norm": 0.48180514574050903,
      "learning_rate": 4.760606060606061e-06,
      "loss": 0.6634,
      "step": 290
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.5483615398406982,
      "learning_rate": 4.750505050505051e-06,
      "loss": 0.625,
      "step": 300
    },
    {
      "epoch": 0.124,
      "grad_norm": 0.5922139286994934,
      "learning_rate": 4.7404040404040404e-06,
      "loss": 0.6384,
      "step": 310
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.4975815713405609,
      "learning_rate": 4.730303030303031e-06,
      "loss": 0.6829,
      "step": 320
    },
    {
      "epoch": 0.132,
      "grad_norm": 0.9364001750946045,
      "learning_rate": 4.72020202020202e-06,
      "loss": 0.6212,
      "step": 330
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.46620863676071167,
      "learning_rate": 4.7101010101010105e-06,
      "loss": 0.5821,
      "step": 340
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.7639203667640686,
      "learning_rate": 4.7e-06,
      "loss": 0.6488,
      "step": 350
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.5290879011154175,
      "learning_rate": 4.68989898989899e-06,
      "loss": 0.6232,
      "step": 360
    },
    {
      "epoch": 0.148,
      "grad_norm": 0.5403430461883545,
      "learning_rate": 4.6797979797979805e-06,
      "loss": 0.5587,
      "step": 370
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.49181103706359863,
      "learning_rate": 4.66969696969697e-06,
      "loss": 0.7483,
      "step": 380
    },
    {
      "epoch": 0.156,
      "grad_norm": 0.5194369554519653,
      "learning_rate": 4.65959595959596e-06,
      "loss": 0.6112,
      "step": 390
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.5938708186149597,
      "learning_rate": 4.64949494949495e-06,
      "loss": 0.6506,
      "step": 400
    },
    {
      "epoch": 0.164,
      "grad_norm": 0.4856957197189331,
      "learning_rate": 4.63939393939394e-06,
      "loss": 0.6391,
      "step": 410
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.6904183626174927,
      "learning_rate": 4.629292929292929e-06,
      "loss": 0.6006,
      "step": 420
    },
    {
      "epoch": 0.172,
      "grad_norm": 0.6904210448265076,
      "learning_rate": 4.61919191919192e-06,
      "loss": 0.5726,
      "step": 430
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.5913689732551575,
      "learning_rate": 4.60909090909091e-06,
      "loss": 0.6568,
      "step": 440
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.5125872492790222,
      "learning_rate": 4.598989898989899e-06,
      "loss": 0.5794,
      "step": 450
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.7667382955551147,
      "learning_rate": 4.58888888888889e-06,
      "loss": 0.6152,
      "step": 460
    },
    {
      "epoch": 0.188,
      "grad_norm": 0.5790695548057556,
      "learning_rate": 4.578787878787879e-06,
      "loss": 0.6523,
      "step": 470
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.7080979943275452,
      "learning_rate": 4.568686868686869e-06,
      "loss": 0.6481,
      "step": 480
    },
    {
      "epoch": 0.196,
      "grad_norm": 0.5957751274108887,
      "learning_rate": 4.558585858585859e-06,
      "loss": 0.663,
      "step": 490
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.5381084680557251,
      "learning_rate": 4.548484848484849e-06,
      "loss": 0.5942,
      "step": 500
    },
    {
      "epoch": 0.204,
      "grad_norm": 0.7641535997390747,
      "learning_rate": 4.5383838383838385e-06,
      "loss": 0.6015,
      "step": 510
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.5830039381980896,
      "learning_rate": 4.528282828282828e-06,
      "loss": 0.6486,
      "step": 520
    },
    {
      "epoch": 0.212,
      "grad_norm": 0.8435471057891846,
      "learning_rate": 4.518181818181819e-06,
      "loss": 0.6346,
      "step": 530
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.572569727897644,
      "learning_rate": 4.5080808080808086e-06,
      "loss": 0.572,
      "step": 540
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.5978428721427917,
      "learning_rate": 4.497979797979799e-06,
      "loss": 0.5859,
      "step": 550
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.5150524973869324,
      "learning_rate": 4.487878787878788e-06,
      "loss": 0.5592,
      "step": 560
    },
    {
      "epoch": 0.228,
      "grad_norm": 0.8341496586799622,
      "learning_rate": 4.477777777777778e-06,
      "loss": 0.595,
      "step": 570
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.5804573893547058,
      "learning_rate": 4.467676767676768e-06,
      "loss": 0.5212,
      "step": 580
    },
    {
      "epoch": 0.236,
      "grad_norm": 0.7437700033187866,
      "learning_rate": 4.4575757575757575e-06,
      "loss": 0.6895,
      "step": 590
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.5851811766624451,
      "learning_rate": 4.447474747474748e-06,
      "loss": 0.5652,
      "step": 600
    },
    {
      "epoch": 0.244,
      "grad_norm": 0.6115344762802124,
      "learning_rate": 4.437373737373738e-06,
      "loss": 0.6087,
      "step": 610
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.8252877593040466,
      "learning_rate": 4.4272727272727275e-06,
      "loss": 0.5907,
      "step": 620
    },
    {
      "epoch": 0.252,
      "grad_norm": 0.5751883387565613,
      "learning_rate": 4.417171717171718e-06,
      "loss": 0.5303,
      "step": 630
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.6259891390800476,
      "learning_rate": 4.407070707070707e-06,
      "loss": 0.5475,
      "step": 640
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.5843452215194702,
      "learning_rate": 4.3969696969696975e-06,
      "loss": 0.5486,
      "step": 650
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.6947112679481506,
      "learning_rate": 4.386868686868687e-06,
      "loss": 0.6124,
      "step": 660
    },
    {
      "epoch": 0.268,
      "grad_norm": 2.134688138961792,
      "learning_rate": 4.376767676767677e-06,
      "loss": 0.6033,
      "step": 670
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.5834196209907532,
      "learning_rate": 4.366666666666667e-06,
      "loss": 0.6177,
      "step": 680
    },
    {
      "epoch": 0.276,
      "grad_norm": 0.599333643913269,
      "learning_rate": 4.356565656565657e-06,
      "loss": 0.5927,
      "step": 690
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.7408270239830017,
      "learning_rate": 4.346464646464647e-06,
      "loss": 0.5672,
      "step": 700
    },
    {
      "epoch": 0.284,
      "grad_norm": 0.6334764957427979,
      "learning_rate": 4.336363636363637e-06,
      "loss": 0.5632,
      "step": 710
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.6533430218696594,
      "learning_rate": 4.326262626262627e-06,
      "loss": 0.5612,
      "step": 720
    },
    {
      "epoch": 0.292,
      "grad_norm": 0.877708375453949,
      "learning_rate": 4.316161616161616e-06,
      "loss": 0.5388,
      "step": 730
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.7953272461891174,
      "learning_rate": 4.306060606060607e-06,
      "loss": 0.628,
      "step": 740
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.6415662169456482,
      "learning_rate": 4.295959595959596e-06,
      "loss": 0.4967,
      "step": 750
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.7979187369346619,
      "learning_rate": 4.285858585858586e-06,
      "loss": 0.6306,
      "step": 760
    },
    {
      "epoch": 0.308,
      "grad_norm": 0.6575490236282349,
      "learning_rate": 4.275757575757576e-06,
      "loss": 0.4925,
      "step": 770
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.6632063984870911,
      "learning_rate": 4.265656565656566e-06,
      "loss": 0.5721,
      "step": 780
    },
    {
      "epoch": 0.316,
      "grad_norm": 0.7173708081245422,
      "learning_rate": 4.255555555555556e-06,
      "loss": 0.5978,
      "step": 790
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.7399827837944031,
      "learning_rate": 4.245454545454546e-06,
      "loss": 0.5371,
      "step": 800
    },
    {
      "epoch": 0.324,
      "grad_norm": 1.252711534500122,
      "learning_rate": 4.235353535353536e-06,
      "loss": 0.6041,
      "step": 810
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.7141431570053101,
      "learning_rate": 4.2252525252525256e-06,
      "loss": 0.5285,
      "step": 820
    },
    {
      "epoch": 0.332,
      "grad_norm": 0.8297675848007202,
      "learning_rate": 4.215151515151515e-06,
      "loss": 0.6422,
      "step": 830
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.6719880104064941,
      "learning_rate": 4.205050505050505e-06,
      "loss": 0.4778,
      "step": 840
    },
    {
      "epoch": 0.34,
      "grad_norm": 1.0419039726257324,
      "learning_rate": 4.194949494949495e-06,
      "loss": 0.6215,
      "step": 850
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.7738214731216431,
      "learning_rate": 4.184848484848485e-06,
      "loss": 0.52,
      "step": 860
    },
    {
      "epoch": 0.348,
      "grad_norm": 0.7227770686149597,
      "learning_rate": 4.174747474747475e-06,
      "loss": 0.4974,
      "step": 870
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.897828996181488,
      "learning_rate": 4.164646464646465e-06,
      "loss": 0.586,
      "step": 880
    },
    {
      "epoch": 0.356,
      "grad_norm": 0.6686868667602539,
      "learning_rate": 4.154545454545455e-06,
      "loss": 0.4787,
      "step": 890
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.8146997690200806,
      "learning_rate": 4.1444444444444445e-06,
      "loss": 0.5214,
      "step": 900
    },
    {
      "epoch": 0.364,
      "grad_norm": 1.0734405517578125,
      "learning_rate": 4.134343434343435e-06,
      "loss": 0.5423,
      "step": 910
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.671844482421875,
      "learning_rate": 4.124242424242424e-06,
      "loss": 0.4797,
      "step": 920
    },
    {
      "epoch": 0.372,
      "grad_norm": 1.4277863502502441,
      "learning_rate": 4.1141414141414145e-06,
      "loss": 0.5728,
      "step": 930
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.7708088755607605,
      "learning_rate": 4.104040404040404e-06,
      "loss": 0.5167,
      "step": 940
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.7366316914558411,
      "learning_rate": 4.093939393939394e-06,
      "loss": 0.5195,
      "step": 950
    },
    {
      "epoch": 0.384,
      "grad_norm": 1.3344953060150146,
      "learning_rate": 4.0838383838383845e-06,
      "loss": 0.4959,
      "step": 960
    },
    {
      "epoch": 0.388,
      "grad_norm": 0.7613770365715027,
      "learning_rate": 4.073737373737374e-06,
      "loss": 0.5259,
      "step": 970
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.6435263752937317,
      "learning_rate": 4.063636363636364e-06,
      "loss": 0.5171,
      "step": 980
    },
    {
      "epoch": 0.396,
      "grad_norm": 0.8521034121513367,
      "learning_rate": 4.053535353535354e-06,
      "loss": 0.5776,
      "step": 990
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.968254804611206,
      "learning_rate": 4.043434343434344e-06,
      "loss": 0.5122,
      "step": 1000
    },
    {
      "epoch": 0.404,
      "grad_norm": 0.8048276901245117,
      "learning_rate": 4.033333333333333e-06,
      "loss": 0.5408,
      "step": 1010
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.8632469177246094,
      "learning_rate": 4.023232323232324e-06,
      "loss": 0.5649,
      "step": 1020
    },
    {
      "epoch": 0.412,
      "grad_norm": 0.6935738325119019,
      "learning_rate": 4.013131313131313e-06,
      "loss": 0.5754,
      "step": 1030
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.8438091278076172,
      "learning_rate": 4.003030303030303e-06,
      "loss": 0.4704,
      "step": 1040
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.9589259624481201,
      "learning_rate": 3.992929292929294e-06,
      "loss": 0.5514,
      "step": 1050
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.6656301617622375,
      "learning_rate": 3.982828282828283e-06,
      "loss": 0.6009,
      "step": 1060
    },
    {
      "epoch": 0.428,
      "grad_norm": 0.7025972604751587,
      "learning_rate": 3.972727272727273e-06,
      "loss": 0.5037,
      "step": 1070
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.8054037094116211,
      "learning_rate": 3.962626262626263e-06,
      "loss": 0.5216,
      "step": 1080
    },
    {
      "epoch": 0.436,
      "grad_norm": 0.7996845245361328,
      "learning_rate": 3.952525252525253e-06,
      "loss": 0.5713,
      "step": 1090
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.7108478546142578,
      "learning_rate": 3.942424242424243e-06,
      "loss": 0.5637,
      "step": 1100
    },
    {
      "epoch": 0.444,
      "grad_norm": 0.7463309168815613,
      "learning_rate": 3.932323232323232e-06,
      "loss": 0.5161,
      "step": 1110
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.7644895911216736,
      "learning_rate": 3.922222222222223e-06,
      "loss": 0.508,
      "step": 1120
    },
    {
      "epoch": 0.452,
      "grad_norm": 1.2583487033843994,
      "learning_rate": 3.912121212121213e-06,
      "loss": 0.4287,
      "step": 1130
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.7815088033676147,
      "learning_rate": 3.902020202020203e-06,
      "loss": 0.5191,
      "step": 1140
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.9429184198379517,
      "learning_rate": 3.891919191919192e-06,
      "loss": 0.5202,
      "step": 1150
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.8643417358398438,
      "learning_rate": 3.881818181818182e-06,
      "loss": 0.475,
      "step": 1160
    },
    {
      "epoch": 0.468,
      "grad_norm": 1.0220268964767456,
      "learning_rate": 3.871717171717172e-06,
      "loss": 0.5399,
      "step": 1170
    },
    {
      "epoch": 0.472,
      "grad_norm": 1.1111621856689453,
      "learning_rate": 3.8616161616161615e-06,
      "loss": 0.4705,
      "step": 1180
    },
    {
      "epoch": 0.476,
      "grad_norm": 0.9302399158477783,
      "learning_rate": 3.851515151515152e-06,
      "loss": 0.5464,
      "step": 1190
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.9164108037948608,
      "learning_rate": 3.841414141414141e-06,
      "loss": 0.4197,
      "step": 1200
    },
    {
      "epoch": 0.484,
      "grad_norm": 0.7432589530944824,
      "learning_rate": 3.8313131313131315e-06,
      "loss": 0.4993,
      "step": 1210
    },
    {
      "epoch": 0.488,
      "grad_norm": 1.0162073373794556,
      "learning_rate": 3.821212121212122e-06,
      "loss": 0.5409,
      "step": 1220
    },
    {
      "epoch": 0.492,
      "grad_norm": 0.8611804842948914,
      "learning_rate": 3.8111111111111117e-06,
      "loss": 0.5276,
      "step": 1230
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.7962414622306824,
      "learning_rate": 3.8010101010101015e-06,
      "loss": 0.4771,
      "step": 1240
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.7397692203521729,
      "learning_rate": 3.7909090909090914e-06,
      "loss": 0.5313,
      "step": 1250
    },
    {
      "epoch": 0.504,
      "grad_norm": 1.058871865272522,
      "learning_rate": 3.7808080808080812e-06,
      "loss": 0.6259,
      "step": 1260
    },
    {
      "epoch": 0.508,
      "grad_norm": 0.7690391540527344,
      "learning_rate": 3.7707070707070707e-06,
      "loss": 0.5458,
      "step": 1270
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.847309947013855,
      "learning_rate": 3.7606060606060605e-06,
      "loss": 0.554,
      "step": 1280
    },
    {
      "epoch": 0.516,
      "grad_norm": 0.7914595603942871,
      "learning_rate": 3.7505050505050504e-06,
      "loss": 0.542,
      "step": 1290
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.0395209789276123,
      "learning_rate": 3.740404040404041e-06,
      "loss": 0.4332,
      "step": 1300
    },
    {
      "epoch": 0.524,
      "grad_norm": 0.8418972492218018,
      "learning_rate": 3.7303030303030306e-06,
      "loss": 0.4766,
      "step": 1310
    },
    {
      "epoch": 0.528,
      "grad_norm": 1.3026238679885864,
      "learning_rate": 3.7202020202020204e-06,
      "loss": 0.5714,
      "step": 1320
    },
    {
      "epoch": 0.532,
      "grad_norm": 0.8102118968963623,
      "learning_rate": 3.7101010101010103e-06,
      "loss": 0.4607,
      "step": 1330
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.7281612157821655,
      "learning_rate": 3.7e-06,
      "loss": 0.4881,
      "step": 1340
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.9850870370864868,
      "learning_rate": 3.68989898989899e-06,
      "loss": 0.4694,
      "step": 1350
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.7918195724487305,
      "learning_rate": 3.67979797979798e-06,
      "loss": 0.5451,
      "step": 1360
    },
    {
      "epoch": 0.548,
      "grad_norm": 0.76360023021698,
      "learning_rate": 3.6696969696969697e-06,
      "loss": 0.5473,
      "step": 1370
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.830286979675293,
      "learning_rate": 3.65959595959596e-06,
      "loss": 0.4517,
      "step": 1380
    },
    {
      "epoch": 0.556,
      "grad_norm": 1.0728240013122559,
      "learning_rate": 3.64949494949495e-06,
      "loss": 0.4992,
      "step": 1390
    },
    {
      "epoch": 0.56,
      "grad_norm": 1.1069320440292358,
      "learning_rate": 3.6393939393939398e-06,
      "loss": 0.5541,
      "step": 1400
    },
    {
      "epoch": 0.564,
      "grad_norm": 0.885516881942749,
      "learning_rate": 3.6292929292929296e-06,
      "loss": 0.5731,
      "step": 1410
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.8820556402206421,
      "learning_rate": 3.6191919191919195e-06,
      "loss": 0.5038,
      "step": 1420
    },
    {
      "epoch": 0.572,
      "grad_norm": 0.8328878283500671,
      "learning_rate": 3.6090909090909093e-06,
      "loss": 0.5196,
      "step": 1430
    },
    {
      "epoch": 0.576,
      "grad_norm": 1.090742826461792,
      "learning_rate": 3.598989898989899e-06,
      "loss": 0.495,
      "step": 1440
    },
    {
      "epoch": 0.58,
      "grad_norm": 1.2229657173156738,
      "learning_rate": 3.588888888888889e-06,
      "loss": 0.5296,
      "step": 1450
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.8599764704704285,
      "learning_rate": 3.578787878787879e-06,
      "loss": 0.546,
      "step": 1460
    },
    {
      "epoch": 0.588,
      "grad_norm": 1.0584572553634644,
      "learning_rate": 3.5686868686868692e-06,
      "loss": 0.5671,
      "step": 1470
    },
    {
      "epoch": 0.592,
      "grad_norm": 1.0420277118682861,
      "learning_rate": 3.558585858585859e-06,
      "loss": 0.5264,
      "step": 1480
    },
    {
      "epoch": 0.596,
      "grad_norm": 0.9882311820983887,
      "learning_rate": 3.548484848484849e-06,
      "loss": 0.5282,
      "step": 1490
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.0957719087600708,
      "learning_rate": 3.538383838383839e-06,
      "loss": 0.4765,
      "step": 1500
    },
    {
      "epoch": 0.604,
      "grad_norm": 0.8567398190498352,
      "learning_rate": 3.5282828282828287e-06,
      "loss": 0.5001,
      "step": 1510
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.7934828996658325,
      "learning_rate": 3.5181818181818185e-06,
      "loss": 0.497,
      "step": 1520
    },
    {
      "epoch": 0.612,
      "grad_norm": 0.8836886882781982,
      "learning_rate": 3.5080808080808084e-06,
      "loss": 0.4412,
      "step": 1530
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.9112298488616943,
      "learning_rate": 3.497979797979798e-06,
      "loss": 0.4896,
      "step": 1540
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.8174581527709961,
      "learning_rate": 3.4878787878787885e-06,
      "loss": 0.5448,
      "step": 1550
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.8659775853157043,
      "learning_rate": 3.4777777777777784e-06,
      "loss": 0.5357,
      "step": 1560
    },
    {
      "epoch": 0.628,
      "grad_norm": 0.785257875919342,
      "learning_rate": 3.4676767676767683e-06,
      "loss": 0.5638,
      "step": 1570
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.8680482506752014,
      "learning_rate": 3.4575757575757577e-06,
      "loss": 0.4713,
      "step": 1580
    },
    {
      "epoch": 0.636,
      "grad_norm": 0.7877413034439087,
      "learning_rate": 3.4474747474747476e-06,
      "loss": 0.5815,
      "step": 1590
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.9598121047019958,
      "learning_rate": 3.4373737373737374e-06,
      "loss": 0.4232,
      "step": 1600
    },
    {
      "epoch": 0.644,
      "grad_norm": 1.1495954990386963,
      "learning_rate": 3.4272727272727273e-06,
      "loss": 0.528,
      "step": 1610
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.7229912281036377,
      "learning_rate": 3.417171717171717e-06,
      "loss": 0.5063,
      "step": 1620
    },
    {
      "epoch": 0.652,
      "grad_norm": 1.1169811487197876,
      "learning_rate": 3.407070707070707e-06,
      "loss": 0.5119,
      "step": 1630
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.6393070816993713,
      "learning_rate": 3.3969696969696973e-06,
      "loss": 0.4348,
      "step": 1640
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.7833253741264343,
      "learning_rate": 3.386868686868687e-06,
      "loss": 0.5195,
      "step": 1650
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.8738071322441101,
      "learning_rate": 3.376767676767677e-06,
      "loss": 0.5073,
      "step": 1660
    },
    {
      "epoch": 0.668,
      "grad_norm": 0.9238635897636414,
      "learning_rate": 3.366666666666667e-06,
      "loss": 0.5182,
      "step": 1670
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.866887629032135,
      "learning_rate": 3.3565656565656568e-06,
      "loss": 0.5696,
      "step": 1680
    },
    {
      "epoch": 0.676,
      "grad_norm": 0.9583457708358765,
      "learning_rate": 3.3464646464646466e-06,
      "loss": 0.4374,
      "step": 1690
    },
    {
      "epoch": 0.68,
      "grad_norm": 1.2019484043121338,
      "learning_rate": 3.3363636363636365e-06,
      "loss": 0.5633,
      "step": 1700
    },
    {
      "epoch": 0.684,
      "grad_norm": 0.9121275544166565,
      "learning_rate": 3.3262626262626264e-06,
      "loss": 0.5566,
      "step": 1710
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.8593720197677612,
      "learning_rate": 3.3161616161616166e-06,
      "loss": 0.4932,
      "step": 1720
    },
    {
      "epoch": 0.692,
      "grad_norm": 1.0109039545059204,
      "learning_rate": 3.3060606060606065e-06,
      "loss": 0.4827,
      "step": 1730
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.8866261839866638,
      "learning_rate": 3.2959595959595964e-06,
      "loss": 0.5513,
      "step": 1740
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.9306524395942688,
      "learning_rate": 3.2858585858585862e-06,
      "loss": 0.5186,
      "step": 1750
    },
    {
      "epoch": 0.704,
      "grad_norm": 1.084672212600708,
      "learning_rate": 3.275757575757576e-06,
      "loss": 0.4429,
      "step": 1760
    },
    {
      "epoch": 0.708,
      "grad_norm": 1.0942487716674805,
      "learning_rate": 3.265656565656566e-06,
      "loss": 0.4749,
      "step": 1770
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.9735338091850281,
      "learning_rate": 3.255555555555556e-06,
      "loss": 0.4936,
      "step": 1780
    },
    {
      "epoch": 0.716,
      "grad_norm": 0.9194412231445312,
      "learning_rate": 3.2454545454545457e-06,
      "loss": 0.5202,
      "step": 1790
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.8531841039657593,
      "learning_rate": 3.2353535353535355e-06,
      "loss": 0.4957,
      "step": 1800
    },
    {
      "epoch": 0.724,
      "grad_norm": 1.1350528001785278,
      "learning_rate": 3.225252525252526e-06,
      "loss": 0.5698,
      "step": 1810
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.9832162857055664,
      "learning_rate": 3.2151515151515157e-06,
      "loss": 0.4865,
      "step": 1820
    },
    {
      "epoch": 0.732,
      "grad_norm": 0.864456295967102,
      "learning_rate": 3.2050505050505056e-06,
      "loss": 0.5069,
      "step": 1830
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.9442605376243591,
      "learning_rate": 3.1949494949494954e-06,
      "loss": 0.4862,
      "step": 1840
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.984427273273468,
      "learning_rate": 3.1848484848484853e-06,
      "loss": 0.5065,
      "step": 1850
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.8757807612419128,
      "learning_rate": 3.1747474747474747e-06,
      "loss": 0.4772,
      "step": 1860
    },
    {
      "epoch": 0.748,
      "grad_norm": 0.833840548992157,
      "learning_rate": 3.1646464646464646e-06,
      "loss": 0.4749,
      "step": 1870
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.8536457419395447,
      "learning_rate": 3.1545454545454545e-06,
      "loss": 0.5356,
      "step": 1880
    },
    {
      "epoch": 0.756,
      "grad_norm": 0.8402109742164612,
      "learning_rate": 3.144444444444445e-06,
      "loss": 0.4464,
      "step": 1890
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.8787884712219238,
      "learning_rate": 3.1343434343434346e-06,
      "loss": 0.5058,
      "step": 1900
    },
    {
      "epoch": 0.764,
      "grad_norm": 1.2654554843902588,
      "learning_rate": 3.1242424242424245e-06,
      "loss": 0.4887,
      "step": 1910
    },
    {
      "epoch": 0.768,
      "grad_norm": 1.3623377084732056,
      "learning_rate": 3.1141414141414143e-06,
      "loss": 0.5053,
      "step": 1920
    },
    {
      "epoch": 0.772,
      "grad_norm": 0.9766515493392944,
      "learning_rate": 3.104040404040404e-06,
      "loss": 0.4797,
      "step": 1930
    },
    {
      "epoch": 0.776,
      "grad_norm": 1.0413219928741455,
      "learning_rate": 3.093939393939394e-06,
      "loss": 0.5045,
      "step": 1940
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.971747875213623,
      "learning_rate": 3.083838383838384e-06,
      "loss": 0.4831,
      "step": 1950
    },
    {
      "epoch": 0.784,
      "grad_norm": 1.0689159631729126,
      "learning_rate": 3.0737373737373738e-06,
      "loss": 0.5003,
      "step": 1960
    },
    {
      "epoch": 0.788,
      "grad_norm": 1.0006476640701294,
      "learning_rate": 3.0636363636363636e-06,
      "loss": 0.5569,
      "step": 1970
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.7904179096221924,
      "learning_rate": 3.053535353535354e-06,
      "loss": 0.5587,
      "step": 1980
    },
    {
      "epoch": 0.796,
      "grad_norm": 0.867363452911377,
      "learning_rate": 3.043434343434344e-06,
      "loss": 0.4943,
      "step": 1990
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.0964306592941284,
      "learning_rate": 3.0333333333333337e-06,
      "loss": 0.5091,
      "step": 2000
    },
    {
      "epoch": 0.804,
      "grad_norm": 0.8963266015052795,
      "learning_rate": 3.0232323232323235e-06,
      "loss": 0.5993,
      "step": 2010
    },
    {
      "epoch": 0.808,
      "grad_norm": 1.108490228652954,
      "learning_rate": 3.0131313131313134e-06,
      "loss": 0.5331,
      "step": 2020
    },
    {
      "epoch": 0.812,
      "grad_norm": 0.802377462387085,
      "learning_rate": 3.0030303030303032e-06,
      "loss": 0.5108,
      "step": 2030
    },
    {
      "epoch": 0.816,
      "grad_norm": 1.1048839092254639,
      "learning_rate": 2.992929292929293e-06,
      "loss": 0.426,
      "step": 2040
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.9200604557991028,
      "learning_rate": 2.982828282828283e-06,
      "loss": 0.5054,
      "step": 2050
    },
    {
      "epoch": 0.824,
      "grad_norm": 1.2062772512435913,
      "learning_rate": 2.9727272727272733e-06,
      "loss": 0.5119,
      "step": 2060
    },
    {
      "epoch": 0.828,
      "grad_norm": 1.2835679054260254,
      "learning_rate": 2.962626262626263e-06,
      "loss": 0.5213,
      "step": 2070
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.8523983955383301,
      "learning_rate": 2.952525252525253e-06,
      "loss": 0.4875,
      "step": 2080
    },
    {
      "epoch": 0.836,
      "grad_norm": 1.4147610664367676,
      "learning_rate": 2.942424242424243e-06,
      "loss": 0.4655,
      "step": 2090
    },
    {
      "epoch": 0.84,
      "grad_norm": 1.2970129251480103,
      "learning_rate": 2.9323232323232327e-06,
      "loss": 0.4281,
      "step": 2100
    },
    {
      "epoch": 0.844,
      "grad_norm": 1.1234676837921143,
      "learning_rate": 2.9222222222222226e-06,
      "loss": 0.4515,
      "step": 2110
    },
    {
      "epoch": 0.848,
      "grad_norm": 1.1097908020019531,
      "learning_rate": 2.9121212121212124e-06,
      "loss": 0.5075,
      "step": 2120
    },
    {
      "epoch": 0.852,
      "grad_norm": 1.2110804319381714,
      "learning_rate": 2.902020202020202e-06,
      "loss": 0.4689,
      "step": 2130
    },
    {
      "epoch": 0.856,
      "grad_norm": 1.1726691722869873,
      "learning_rate": 2.8919191919191917e-06,
      "loss": 0.5174,
      "step": 2140
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.7234215140342712,
      "learning_rate": 2.8818181818181824e-06,
      "loss": 0.5301,
      "step": 2150
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.9100030660629272,
      "learning_rate": 2.8717171717171723e-06,
      "loss": 0.5009,
      "step": 2160
    },
    {
      "epoch": 0.868,
      "grad_norm": 1.09447181224823,
      "learning_rate": 2.8616161616161618e-06,
      "loss": 0.4842,
      "step": 2170
    },
    {
      "epoch": 0.872,
      "grad_norm": 1.1431394815444946,
      "learning_rate": 2.8515151515151516e-06,
      "loss": 0.4263,
      "step": 2180
    },
    {
      "epoch": 0.876,
      "grad_norm": 0.8541756272315979,
      "learning_rate": 2.8414141414141415e-06,
      "loss": 0.5233,
      "step": 2190
    },
    {
      "epoch": 0.88,
      "grad_norm": 1.2228448390960693,
      "learning_rate": 2.8313131313131313e-06,
      "loss": 0.4994,
      "step": 2200
    },
    {
      "epoch": 0.884,
      "grad_norm": 1.0349429845809937,
      "learning_rate": 2.821212121212121e-06,
      "loss": 0.5307,
      "step": 2210
    },
    {
      "epoch": 0.888,
      "grad_norm": 2.193402051925659,
      "learning_rate": 2.811111111111111e-06,
      "loss": 0.4986,
      "step": 2220
    },
    {
      "epoch": 0.892,
      "grad_norm": 0.8069092035293579,
      "learning_rate": 2.8010101010101014e-06,
      "loss": 0.5471,
      "step": 2230
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.8265645503997803,
      "learning_rate": 2.7909090909090912e-06,
      "loss": 0.4967,
      "step": 2240
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.961159884929657,
      "learning_rate": 2.780808080808081e-06,
      "loss": 0.4766,
      "step": 2250
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.8434803485870361,
      "learning_rate": 2.770707070707071e-06,
      "loss": 0.4963,
      "step": 2260
    },
    {
      "epoch": 0.908,
      "grad_norm": 0.8554757237434387,
      "learning_rate": 2.760606060606061e-06,
      "loss": 0.5617,
      "step": 2270
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.966715395450592,
      "learning_rate": 2.7505050505050507e-06,
      "loss": 0.4573,
      "step": 2280
    },
    {
      "epoch": 0.916,
      "grad_norm": 1.2536476850509644,
      "learning_rate": 2.7404040404040405e-06,
      "loss": 0.5022,
      "step": 2290
    },
    {
      "epoch": 0.92,
      "grad_norm": 1.0113214254379272,
      "learning_rate": 2.7303030303030304e-06,
      "loss": 0.5287,
      "step": 2300
    },
    {
      "epoch": 0.924,
      "grad_norm": 1.077778697013855,
      "learning_rate": 2.7202020202020203e-06,
      "loss": 0.4058,
      "step": 2310
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.9689172506332397,
      "learning_rate": 2.7101010101010105e-06,
      "loss": 0.4823,
      "step": 2320
    },
    {
      "epoch": 0.932,
      "grad_norm": 0.8722618818283081,
      "learning_rate": 2.7000000000000004e-06,
      "loss": 0.4959,
      "step": 2330
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.8927161693572998,
      "learning_rate": 2.6898989898989903e-06,
      "loss": 0.5142,
      "step": 2340
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.8663895726203918,
      "learning_rate": 2.67979797979798e-06,
      "loss": 0.4367,
      "step": 2350
    },
    {
      "epoch": 0.944,
      "grad_norm": 1.0516639947891235,
      "learning_rate": 2.66969696969697e-06,
      "loss": 0.4368,
      "step": 2360
    },
    {
      "epoch": 0.948,
      "grad_norm": 1.0879862308502197,
      "learning_rate": 2.65959595959596e-06,
      "loss": 0.6337,
      "step": 2370
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.96149080991745,
      "learning_rate": 2.6494949494949497e-06,
      "loss": 0.5346,
      "step": 2380
    },
    {
      "epoch": 0.956,
      "grad_norm": 1.181453824043274,
      "learning_rate": 2.6393939393939396e-06,
      "loss": 0.4411,
      "step": 2390
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.1245464086532593,
      "learning_rate": 2.62929292929293e-06,
      "loss": 0.5012,
      "step": 2400
    },
    {
      "epoch": 0.964,
      "grad_norm": 1.1368491649627686,
      "learning_rate": 2.6191919191919197e-06,
      "loss": 0.4809,
      "step": 2410
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.9854004979133606,
      "learning_rate": 2.6090909090909096e-06,
      "loss": 0.4996,
      "step": 2420
    },
    {
      "epoch": 0.972,
      "grad_norm": 1.3191152811050415,
      "learning_rate": 2.5989898989898995e-06,
      "loss": 0.6329,
      "step": 2430
    },
    {
      "epoch": 0.976,
      "grad_norm": 1.0013132095336914,
      "learning_rate": 2.5888888888888893e-06,
      "loss": 0.6022,
      "step": 2440
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.9727389812469482,
      "learning_rate": 2.5787878787878788e-06,
      "loss": 0.4763,
      "step": 2450
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.9067602157592773,
      "learning_rate": 2.5686868686868686e-06,
      "loss": 0.494,
      "step": 2460
    },
    {
      "epoch": 0.988,
      "grad_norm": 0.9574521780014038,
      "learning_rate": 2.5585858585858585e-06,
      "loss": 0.432,
      "step": 2470
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.8859444260597229,
      "learning_rate": 2.5484848484848484e-06,
      "loss": 0.4746,
      "step": 2480
    },
    {
      "epoch": 0.996,
      "grad_norm": 1.065557837486267,
      "learning_rate": 2.5383838383838386e-06,
      "loss": 0.4784,
      "step": 2490
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.0855300426483154,
      "learning_rate": 2.5282828282828285e-06,
      "loss": 0.4218,
      "step": 2500
    },
    {
      "epoch": 1.004,
      "grad_norm": 1.1943005323410034,
      "learning_rate": 2.5181818181818184e-06,
      "loss": 0.5307,
      "step": 2510
    },
    {
      "epoch": 1.008,
      "grad_norm": 1.0018240213394165,
      "learning_rate": 2.5080808080808082e-06,
      "loss": 0.57,
      "step": 2520
    },
    {
      "epoch": 1.012,
      "grad_norm": 1.0115052461624146,
      "learning_rate": 2.497979797979798e-06,
      "loss": 0.4992,
      "step": 2530
    },
    {
      "epoch": 1.016,
      "grad_norm": 1.06354820728302,
      "learning_rate": 2.487878787878788e-06,
      "loss": 0.5048,
      "step": 2540
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.892726719379425,
      "learning_rate": 2.4777777777777782e-06,
      "loss": 0.4421,
      "step": 2550
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.8765599131584167,
      "learning_rate": 2.467676767676768e-06,
      "loss": 0.481,
      "step": 2560
    },
    {
      "epoch": 1.028,
      "grad_norm": 0.9654801487922668,
      "learning_rate": 2.457575757575758e-06,
      "loss": 0.5228,
      "step": 2570
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.8147931694984436,
      "learning_rate": 2.4474747474747474e-06,
      "loss": 0.4189,
      "step": 2580
    },
    {
      "epoch": 1.036,
      "grad_norm": 0.9715871214866638,
      "learning_rate": 2.4373737373737373e-06,
      "loss": 0.4055,
      "step": 2590
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.9323488473892212,
      "learning_rate": 2.4272727272727276e-06,
      "loss": 0.4726,
      "step": 2600
    },
    {
      "epoch": 1.044,
      "grad_norm": 1.0319777727127075,
      "learning_rate": 2.4171717171717174e-06,
      "loss": 0.5622,
      "step": 2610
    },
    {
      "epoch": 1.048,
      "grad_norm": 1.1538463830947876,
      "learning_rate": 2.4070707070707073e-06,
      "loss": 0.5567,
      "step": 2620
    },
    {
      "epoch": 1.052,
      "grad_norm": 0.965849757194519,
      "learning_rate": 2.396969696969697e-06,
      "loss": 0.4958,
      "step": 2630
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.8972512483596802,
      "learning_rate": 2.386868686868687e-06,
      "loss": 0.4618,
      "step": 2640
    },
    {
      "epoch": 1.06,
      "grad_norm": 1.3041584491729736,
      "learning_rate": 2.376767676767677e-06,
      "loss": 0.5255,
      "step": 2650
    },
    {
      "epoch": 1.064,
      "grad_norm": 1.64046049118042,
      "learning_rate": 2.3666666666666667e-06,
      "loss": 0.4736,
      "step": 2660
    },
    {
      "epoch": 1.068,
      "grad_norm": 1.0052114725112915,
      "learning_rate": 2.3565656565656566e-06,
      "loss": 0.4717,
      "step": 2670
    },
    {
      "epoch": 1.072,
      "grad_norm": 1.043232798576355,
      "learning_rate": 2.346464646464647e-06,
      "loss": 0.4528,
      "step": 2680
    },
    {
      "epoch": 1.076,
      "grad_norm": 1.0332777500152588,
      "learning_rate": 2.3363636363636367e-06,
      "loss": 0.4633,
      "step": 2690
    },
    {
      "epoch": 1.08,
      "grad_norm": 1.1818653345108032,
      "learning_rate": 2.3262626262626266e-06,
      "loss": 0.5377,
      "step": 2700
    },
    {
      "epoch": 1.084,
      "grad_norm": 1.1547613143920898,
      "learning_rate": 2.317171717171717e-06,
      "loss": 0.5319,
      "step": 2710
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.9284811615943909,
      "learning_rate": 2.307070707070707e-06,
      "loss": 0.4519,
      "step": 2720
    },
    {
      "epoch": 1.092,
      "grad_norm": 0.7991663813591003,
      "learning_rate": 2.2969696969696973e-06,
      "loss": 0.4768,
      "step": 2730
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.9600944519042969,
      "learning_rate": 2.286868686868687e-06,
      "loss": 0.3941,
      "step": 2740
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.8824282288551331,
      "learning_rate": 2.276767676767677e-06,
      "loss": 0.4105,
      "step": 2750
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.9673559069633484,
      "learning_rate": 2.266666666666667e-06,
      "loss": 0.5839,
      "step": 2760
    },
    {
      "epoch": 1.108,
      "grad_norm": 0.9164134860038757,
      "learning_rate": 2.2565656565656567e-06,
      "loss": 0.5038,
      "step": 2770
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.9496135115623474,
      "learning_rate": 2.2474747474747476e-06,
      "loss": 0.4089,
      "step": 2780
    },
    {
      "epoch": 1.116,
      "grad_norm": 1.112850308418274,
      "learning_rate": 2.2373737373737375e-06,
      "loss": 0.5515,
      "step": 2790
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.1796678304672241,
      "learning_rate": 2.2272727272727274e-06,
      "loss": 0.4302,
      "step": 2800
    },
    {
      "epoch": 1.124,
      "grad_norm": 0.9416185021400452,
      "learning_rate": 2.2171717171717176e-06,
      "loss": 0.4424,
      "step": 2810
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 1.083537220954895,
      "learning_rate": 2.2070707070707075e-06,
      "loss": 0.4758,
      "step": 2820
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 0.9753857254981995,
      "learning_rate": 2.196969696969697e-06,
      "loss": 0.5518,
      "step": 2830
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 1.0057868957519531,
      "learning_rate": 2.186868686868687e-06,
      "loss": 0.4174,
      "step": 2840
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 1.0250985622406006,
      "learning_rate": 2.1767676767676767e-06,
      "loss": 0.5542,
      "step": 2850
    },
    {
      "epoch": 1.144,
      "grad_norm": 1.2642229795455933,
      "learning_rate": 2.166666666666667e-06,
      "loss": 0.4014,
      "step": 2860
    },
    {
      "epoch": 1.148,
      "grad_norm": 1.1512702703475952,
      "learning_rate": 2.156565656565657e-06,
      "loss": 0.551,
      "step": 2870
    },
    {
      "epoch": 1.152,
      "grad_norm": 1.1231281757354736,
      "learning_rate": 2.1464646464646467e-06,
      "loss": 0.5214,
      "step": 2880
    },
    {
      "epoch": 1.156,
      "grad_norm": 1.08984375,
      "learning_rate": 2.1363636363636365e-06,
      "loss": 0.4617,
      "step": 2890
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.8798406720161438,
      "learning_rate": 2.1262626262626264e-06,
      "loss": 0.5362,
      "step": 2900
    },
    {
      "epoch": 1.164,
      "grad_norm": 1.0396066904067993,
      "learning_rate": 2.1161616161616163e-06,
      "loss": 0.4524,
      "step": 2910
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.9380708932876587,
      "learning_rate": 2.106060606060606e-06,
      "loss": 0.5374,
      "step": 2920
    },
    {
      "epoch": 1.172,
      "grad_norm": 0.816921591758728,
      "learning_rate": 2.095959595959596e-06,
      "loss": 0.4575,
      "step": 2930
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.8562639951705933,
      "learning_rate": 2.0858585858585863e-06,
      "loss": 0.4622,
      "step": 2940
    },
    {
      "epoch": 1.18,
      "grad_norm": 1.1118078231811523,
      "learning_rate": 2.075757575757576e-06,
      "loss": 0.5221,
      "step": 2950
    },
    {
      "epoch": 1.184,
      "grad_norm": 1.17733895778656,
      "learning_rate": 2.065656565656566e-06,
      "loss": 0.5485,
      "step": 2960
    },
    {
      "epoch": 1.188,
      "grad_norm": 0.950395405292511,
      "learning_rate": 2.0555555555555555e-06,
      "loss": 0.4567,
      "step": 2970
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.8941227793693542,
      "learning_rate": 2.0454545454545457e-06,
      "loss": 0.5104,
      "step": 2980
    },
    {
      "epoch": 1.196,
      "grad_norm": 1.0311932563781738,
      "learning_rate": 2.0353535353535356e-06,
      "loss": 0.3758,
      "step": 2990
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.8017210960388184,
      "learning_rate": 2.0252525252525255e-06,
      "loss": 0.4777,
      "step": 3000
    },
    {
      "epoch": 1.204,
      "grad_norm": 1.0593416690826416,
      "learning_rate": 2.0151515151515153e-06,
      "loss": 0.5773,
      "step": 3010
    },
    {
      "epoch": 1.208,
      "grad_norm": 1.116963505744934,
      "learning_rate": 2.005050505050505e-06,
      "loss": 0.5433,
      "step": 3020
    },
    {
      "epoch": 1.212,
      "grad_norm": 1.1780831813812256,
      "learning_rate": 1.994949494949495e-06,
      "loss": 0.5251,
      "step": 3030
    },
    {
      "epoch": 1.216,
      "grad_norm": 1.116431713104248,
      "learning_rate": 1.984848484848485e-06,
      "loss": 0.4842,
      "step": 3040
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.973610520362854,
      "learning_rate": 1.9747474747474748e-06,
      "loss": 0.4501,
      "step": 3050
    },
    {
      "epoch": 1.224,
      "grad_norm": 1.0458836555480957,
      "learning_rate": 1.9646464646464646e-06,
      "loss": 0.5186,
      "step": 3060
    },
    {
      "epoch": 1.228,
      "grad_norm": 1.3197197914123535,
      "learning_rate": 1.954545454545455e-06,
      "loss": 0.4509,
      "step": 3070
    },
    {
      "epoch": 1.232,
      "grad_norm": 1.1950627565383911,
      "learning_rate": 1.944444444444445e-06,
      "loss": 0.4046,
      "step": 3080
    },
    {
      "epoch": 1.236,
      "grad_norm": 1.3278589248657227,
      "learning_rate": 1.9343434343434347e-06,
      "loss": 0.5488,
      "step": 3090
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.139330267906189,
      "learning_rate": 1.924242424242424e-06,
      "loss": 0.5158,
      "step": 3100
    },
    {
      "epoch": 1.244,
      "grad_norm": 1.159881353378296,
      "learning_rate": 1.9141414141414144e-06,
      "loss": 0.4125,
      "step": 3110
    },
    {
      "epoch": 1.248,
      "grad_norm": 1.110988736152649,
      "learning_rate": 1.9040404040404042e-06,
      "loss": 0.5273,
      "step": 3120
    },
    {
      "epoch": 1.252,
      "grad_norm": 1.1784422397613525,
      "learning_rate": 1.8939393939393941e-06,
      "loss": 0.5387,
      "step": 3130
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.9864897131919861,
      "learning_rate": 1.883838383838384e-06,
      "loss": 0.5239,
      "step": 3140
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.916129469871521,
      "learning_rate": 1.873737373737374e-06,
      "loss": 0.5076,
      "step": 3150
    },
    {
      "epoch": 1.264,
      "grad_norm": 1.152918815612793,
      "learning_rate": 1.863636363636364e-06,
      "loss": 0.5734,
      "step": 3160
    },
    {
      "epoch": 1.268,
      "grad_norm": 1.3127241134643555,
      "learning_rate": 1.8535353535353538e-06,
      "loss": 0.5109,
      "step": 3170
    },
    {
      "epoch": 1.272,
      "grad_norm": 1.4967635869979858,
      "learning_rate": 1.8434343434343434e-06,
      "loss": 0.4819,
      "step": 3180
    },
    {
      "epoch": 1.276,
      "grad_norm": 1.380102276802063,
      "learning_rate": 1.8333333333333333e-06,
      "loss": 0.4346,
      "step": 3190
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.1387826204299927,
      "learning_rate": 1.8232323232323234e-06,
      "loss": 0.434,
      "step": 3200
    },
    {
      "epoch": 1.284,
      "grad_norm": 1.2078417539596558,
      "learning_rate": 1.8131313131313132e-06,
      "loss": 0.5311,
      "step": 3210
    },
    {
      "epoch": 1.288,
      "grad_norm": 1.096184492111206,
      "learning_rate": 1.803030303030303e-06,
      "loss": 0.4169,
      "step": 3220
    },
    {
      "epoch": 1.292,
      "grad_norm": 1.1130058765411377,
      "learning_rate": 1.792929292929293e-06,
      "loss": 0.5486,
      "step": 3230
    },
    {
      "epoch": 1.296,
      "grad_norm": 1.4683310985565186,
      "learning_rate": 1.782828282828283e-06,
      "loss": 0.4378,
      "step": 3240
    },
    {
      "epoch": 1.3,
      "grad_norm": 1.0656042098999023,
      "learning_rate": 1.7727272727272729e-06,
      "loss": 0.5065,
      "step": 3250
    },
    {
      "epoch": 1.304,
      "grad_norm": 1.1266556978225708,
      "learning_rate": 1.7626262626262628e-06,
      "loss": 0.5028,
      "step": 3260
    },
    {
      "epoch": 1.308,
      "grad_norm": 1.1216402053833008,
      "learning_rate": 1.7525252525252526e-06,
      "loss": 0.5176,
      "step": 3270
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.9814698100090027,
      "learning_rate": 1.7424242424242427e-06,
      "loss": 0.4509,
      "step": 3280
    },
    {
      "epoch": 1.316,
      "grad_norm": 1.048966407775879,
      "learning_rate": 1.7323232323232326e-06,
      "loss": 0.4842,
      "step": 3290
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.178816556930542,
      "learning_rate": 1.7222222222222224e-06,
      "loss": 0.521,
      "step": 3300
    },
    {
      "epoch": 1.324,
      "grad_norm": 1.184037446975708,
      "learning_rate": 1.7121212121212123e-06,
      "loss": 0.4325,
      "step": 3310
    },
    {
      "epoch": 1.328,
      "grad_norm": 1.370929479598999,
      "learning_rate": 1.7020202020202024e-06,
      "loss": 0.4179,
      "step": 3320
    },
    {
      "epoch": 1.332,
      "grad_norm": 1.1117994785308838,
      "learning_rate": 1.6919191919191922e-06,
      "loss": 0.4713,
      "step": 3330
    },
    {
      "epoch": 1.336,
      "grad_norm": 1.0120700597763062,
      "learning_rate": 1.6818181818181819e-06,
      "loss": 0.5054,
      "step": 3340
    },
    {
      "epoch": 1.34,
      "grad_norm": 1.3071181774139404,
      "learning_rate": 1.6717171717171717e-06,
      "loss": 0.4566,
      "step": 3350
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 1.262330412864685,
      "learning_rate": 1.6616161616161616e-06,
      "loss": 0.3626,
      "step": 3360
    },
    {
      "epoch": 1.3479999999999999,
      "grad_norm": 1.054407000541687,
      "learning_rate": 1.6515151515151517e-06,
      "loss": 0.495,
      "step": 3370
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 1.0269511938095093,
      "learning_rate": 1.6414141414141415e-06,
      "loss": 0.455,
      "step": 3380
    },
    {
      "epoch": 1.3559999999999999,
      "grad_norm": 1.3539952039718628,
      "learning_rate": 1.6313131313131314e-06,
      "loss": 0.5223,
      "step": 3390
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 1.120828628540039,
      "learning_rate": 1.6212121212121213e-06,
      "loss": 0.4555,
      "step": 3400
    },
    {
      "epoch": 1.3639999999999999,
      "grad_norm": 1.1705601215362549,
      "learning_rate": 1.6111111111111113e-06,
      "loss": 0.3862,
      "step": 3410
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 1.0175180435180664,
      "learning_rate": 1.6010101010101012e-06,
      "loss": 0.4971,
      "step": 3420
    },
    {
      "epoch": 1.3719999999999999,
      "grad_norm": 0.9343514442443848,
      "learning_rate": 1.590909090909091e-06,
      "loss": 0.3766,
      "step": 3430
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.8886498212814331,
      "learning_rate": 1.580808080808081e-06,
      "loss": 0.3753,
      "step": 3440
    },
    {
      "epoch": 1.38,
      "grad_norm": 1.171697735786438,
      "learning_rate": 1.570707070707071e-06,
      "loss": 0.4849,
      "step": 3450
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.8943744897842407,
      "learning_rate": 1.5606060606060609e-06,
      "loss": 0.4378,
      "step": 3460
    },
    {
      "epoch": 1.388,
      "grad_norm": 1.190911054611206,
      "learning_rate": 1.5505050505050507e-06,
      "loss": 0.5537,
      "step": 3470
    },
    {
      "epoch": 1.392,
      "grad_norm": 1.2967779636383057,
      "learning_rate": 1.5404040404040404e-06,
      "loss": 0.3823,
      "step": 3480
    },
    {
      "epoch": 1.396,
      "grad_norm": 1.2015717029571533,
      "learning_rate": 1.5303030303030302e-06,
      "loss": 0.4762,
      "step": 3490
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.9893304109573364,
      "learning_rate": 1.5202020202020203e-06,
      "loss": 0.5518,
      "step": 3500
    },
    {
      "epoch": 1.404,
      "grad_norm": 1.0702142715454102,
      "learning_rate": 1.5101010101010102e-06,
      "loss": 0.4503,
      "step": 3510
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.9629096388816833,
      "learning_rate": 1.5e-06,
      "loss": 0.4303,
      "step": 3520
    },
    {
      "epoch": 1.412,
      "grad_norm": 1.1432262659072876,
      "learning_rate": 1.48989898989899e-06,
      "loss": 0.4503,
      "step": 3530
    },
    {
      "epoch": 1.416,
      "grad_norm": 1.1037009954452515,
      "learning_rate": 1.47979797979798e-06,
      "loss": 0.4108,
      "step": 3540
    },
    {
      "epoch": 1.42,
      "grad_norm": 1.0338877439498901,
      "learning_rate": 1.4696969696969698e-06,
      "loss": 0.4253,
      "step": 3550
    },
    {
      "epoch": 1.424,
      "grad_norm": 1.2399377822875977,
      "learning_rate": 1.4595959595959597e-06,
      "loss": 0.4778,
      "step": 3560
    },
    {
      "epoch": 1.428,
      "grad_norm": 0.8812330961227417,
      "learning_rate": 1.4494949494949496e-06,
      "loss": 0.5299,
      "step": 3570
    },
    {
      "epoch": 1.432,
      "grad_norm": 1.2495416402816772,
      "learning_rate": 1.4393939393939396e-06,
      "loss": 0.475,
      "step": 3580
    },
    {
      "epoch": 1.436,
      "grad_norm": 1.0420446395874023,
      "learning_rate": 1.4292929292929295e-06,
      "loss": 0.4614,
      "step": 3590
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.8783065676689148,
      "learning_rate": 1.4191919191919194e-06,
      "loss": 0.5264,
      "step": 3600
    },
    {
      "epoch": 1.444,
      "grad_norm": 0.9426416158676147,
      "learning_rate": 1.409090909090909e-06,
      "loss": 0.4068,
      "step": 3610
    },
    {
      "epoch": 1.448,
      "grad_norm": 1.42103111743927,
      "learning_rate": 1.3989898989898993e-06,
      "loss": 0.4069,
      "step": 3620
    },
    {
      "epoch": 1.452,
      "grad_norm": 1.5274189710617065,
      "learning_rate": 1.3888888888888892e-06,
      "loss": 0.3745,
      "step": 3630
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.9755722284317017,
      "learning_rate": 1.3787878787878788e-06,
      "loss": 0.4141,
      "step": 3640
    },
    {
      "epoch": 1.46,
      "grad_norm": 1.0976322889328003,
      "learning_rate": 1.3686868686868687e-06,
      "loss": 0.5193,
      "step": 3650
    },
    {
      "epoch": 1.464,
      "grad_norm": 1.5383721590042114,
      "learning_rate": 1.3585858585858585e-06,
      "loss": 0.5028,
      "step": 3660
    },
    {
      "epoch": 1.468,
      "grad_norm": 1.052961826324463,
      "learning_rate": 1.3484848484848486e-06,
      "loss": 0.431,
      "step": 3670
    },
    {
      "epoch": 1.472,
      "grad_norm": 1.4423587322235107,
      "learning_rate": 1.3383838383838385e-06,
      "loss": 0.5011,
      "step": 3680
    },
    {
      "epoch": 1.476,
      "grad_norm": 1.1470524072647095,
      "learning_rate": 1.3282828282828283e-06,
      "loss": 0.5809,
      "step": 3690
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.9762181639671326,
      "learning_rate": 1.3181818181818182e-06,
      "loss": 0.5397,
      "step": 3700
    },
    {
      "epoch": 1.484,
      "grad_norm": 1.1456092596054077,
      "learning_rate": 1.3080808080808083e-06,
      "loss": 0.4062,
      "step": 3710
    },
    {
      "epoch": 1.488,
      "grad_norm": 1.0633985996246338,
      "learning_rate": 1.2979797979797981e-06,
      "loss": 0.4895,
      "step": 3720
    },
    {
      "epoch": 1.492,
      "grad_norm": 1.217038869857788,
      "learning_rate": 1.287878787878788e-06,
      "loss": 0.4137,
      "step": 3730
    },
    {
      "epoch": 1.496,
      "grad_norm": 1.0059674978256226,
      "learning_rate": 1.2777777777777779e-06,
      "loss": 0.5395,
      "step": 3740
    },
    {
      "epoch": 1.5,
      "grad_norm": 1.254223108291626,
      "learning_rate": 1.267676767676768e-06,
      "loss": 0.4825,
      "step": 3750
    },
    {
      "epoch": 1.504,
      "grad_norm": 1.3042943477630615,
      "learning_rate": 1.2575757575757578e-06,
      "loss": 0.4425,
      "step": 3760
    },
    {
      "epoch": 1.508,
      "grad_norm": 1.1276164054870605,
      "learning_rate": 1.2474747474747475e-06,
      "loss": 0.5487,
      "step": 3770
    },
    {
      "epoch": 1.512,
      "grad_norm": 1.4966126680374146,
      "learning_rate": 1.2373737373737375e-06,
      "loss": 0.5113,
      "step": 3780
    },
    {
      "epoch": 1.516,
      "grad_norm": 0.9319536089897156,
      "learning_rate": 1.2272727272727274e-06,
      "loss": 0.4922,
      "step": 3790
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.9715514779090881,
      "learning_rate": 1.2171717171717173e-06,
      "loss": 0.3796,
      "step": 3800
    },
    {
      "epoch": 1.524,
      "grad_norm": 0.9254953861236572,
      "learning_rate": 1.2070707070707071e-06,
      "loss": 0.5216,
      "step": 3810
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.8584651350975037,
      "learning_rate": 1.196969696969697e-06,
      "loss": 0.4725,
      "step": 3820
    },
    {
      "epoch": 1.532,
      "grad_norm": 1.0171608924865723,
      "learning_rate": 1.186868686868687e-06,
      "loss": 0.4356,
      "step": 3830
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.9783214330673218,
      "learning_rate": 1.1767676767676767e-06,
      "loss": 0.4748,
      "step": 3840
    },
    {
      "epoch": 1.54,
      "grad_norm": 1.0356159210205078,
      "learning_rate": 1.1666666666666668e-06,
      "loss": 0.491,
      "step": 3850
    },
    {
      "epoch": 1.544,
      "grad_norm": 1.1842730045318604,
      "learning_rate": 1.1565656565656567e-06,
      "loss": 0.5037,
      "step": 3860
    },
    {
      "epoch": 1.548,
      "grad_norm": 1.0691306591033936,
      "learning_rate": 1.1464646464646465e-06,
      "loss": 0.4848,
      "step": 3870
    },
    {
      "epoch": 1.552,
      "grad_norm": 1.3931212425231934,
      "learning_rate": 1.1363636363636364e-06,
      "loss": 0.5106,
      "step": 3880
    },
    {
      "epoch": 1.556,
      "grad_norm": 0.9757219552993774,
      "learning_rate": 1.1262626262626265e-06,
      "loss": 0.4231,
      "step": 3890
    },
    {
      "epoch": 1.56,
      "grad_norm": 1.0659834146499634,
      "learning_rate": 1.1161616161616163e-06,
      "loss": 0.4785,
      "step": 3900
    },
    {
      "epoch": 1.564,
      "grad_norm": 1.1402032375335693,
      "learning_rate": 1.1060606060606062e-06,
      "loss": 0.4453,
      "step": 3910
    },
    {
      "epoch": 1.568,
      "grad_norm": 1.2191123962402344,
      "learning_rate": 1.095959595959596e-06,
      "loss": 0.4419,
      "step": 3920
    },
    {
      "epoch": 1.572,
      "grad_norm": 1.0324569940567017,
      "learning_rate": 1.085858585858586e-06,
      "loss": 0.494,
      "step": 3930
    },
    {
      "epoch": 1.576,
      "grad_norm": 1.2454514503479004,
      "learning_rate": 1.0757575757575758e-06,
      "loss": 0.4468,
      "step": 3940
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.9249784350395203,
      "learning_rate": 1.0656565656565658e-06,
      "loss": 0.4442,
      "step": 3950
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.9912997484207153,
      "learning_rate": 1.0555555555555557e-06,
      "loss": 0.401,
      "step": 3960
    },
    {
      "epoch": 1.588,
      "grad_norm": 1.100840449333191,
      "learning_rate": 1.0454545454545456e-06,
      "loss": 0.4701,
      "step": 3970
    },
    {
      "epoch": 1.592,
      "grad_norm": 1.1899290084838867,
      "learning_rate": 1.0353535353535354e-06,
      "loss": 0.5843,
      "step": 3980
    },
    {
      "epoch": 1.596,
      "grad_norm": 0.9724143743515015,
      "learning_rate": 1.0252525252525253e-06,
      "loss": 0.4732,
      "step": 3990
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.954615592956543,
      "learning_rate": 1.0151515151515152e-06,
      "loss": 0.4438,
      "step": 4000
    },
    {
      "epoch": 1.604,
      "grad_norm": 0.9108484387397766,
      "learning_rate": 1.005050505050505e-06,
      "loss": 0.6483,
      "step": 4010
    },
    {
      "epoch": 1.608,
      "grad_norm": 1.1650375127792358,
      "learning_rate": 9.94949494949495e-07,
      "loss": 0.4067,
      "step": 4020
    },
    {
      "epoch": 1.612,
      "grad_norm": 1.170212745666504,
      "learning_rate": 9.84848484848485e-07,
      "loss": 0.541,
      "step": 4030
    },
    {
      "epoch": 1.616,
      "grad_norm": 1.020229458808899,
      "learning_rate": 9.747474747474748e-07,
      "loss": 0.4544,
      "step": 4040
    },
    {
      "epoch": 1.62,
      "grad_norm": 1.0929181575775146,
      "learning_rate": 9.646464646464647e-07,
      "loss": 0.5644,
      "step": 4050
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.9859409332275391,
      "learning_rate": 9.545454545454548e-07,
      "loss": 0.5193,
      "step": 4060
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 1.274688720703125,
      "learning_rate": 9.444444444444445e-07,
      "loss": 0.3995,
      "step": 4070
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 2.68464994430542,
      "learning_rate": 9.343434343434345e-07,
      "loss": 0.4541,
      "step": 4080
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 1.2754734754562378,
      "learning_rate": 9.242424242424244e-07,
      "loss": 0.4142,
      "step": 4090
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 1.0589066743850708,
      "learning_rate": 9.141414141414143e-07,
      "loss": 0.4022,
      "step": 4100
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 1.665781855583191,
      "learning_rate": 9.040404040404041e-07,
      "loss": 0.605,
      "step": 4110
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 1.2555557489395142,
      "learning_rate": 8.93939393939394e-07,
      "loss": 0.5745,
      "step": 4120
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 1.0694444179534912,
      "learning_rate": 8.838383838383839e-07,
      "loss": 0.4484,
      "step": 4130
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.9189415574073792,
      "learning_rate": 8.737373737373738e-07,
      "loss": 0.4399,
      "step": 4140
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 1.0289256572723389,
      "learning_rate": 8.636363636363637e-07,
      "loss": 0.5193,
      "step": 4150
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 1.1716855764389038,
      "learning_rate": 8.535353535353535e-07,
      "loss": 0.4558,
      "step": 4160
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 1.3332463502883911,
      "learning_rate": 8.434343434343436e-07,
      "loss": 0.4977,
      "step": 4170
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.9644088745117188,
      "learning_rate": 8.333333333333333e-07,
      "loss": 0.419,
      "step": 4180
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 1.027145504951477,
      "learning_rate": 8.232323232323233e-07,
      "loss": 0.4866,
      "step": 4190
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 1.167870283126831,
      "learning_rate": 8.131313131313132e-07,
      "loss": 0.4731,
      "step": 4200
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 3.0550031661987305,
      "learning_rate": 8.030303030303031e-07,
      "loss": 0.5712,
      "step": 4210
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.9751752614974976,
      "learning_rate": 7.92929292929293e-07,
      "loss": 0.4856,
      "step": 4220
    },
    {
      "epoch": 1.692,
      "grad_norm": 0.9258723855018616,
      "learning_rate": 7.82828282828283e-07,
      "loss": 0.4237,
      "step": 4230
    },
    {
      "epoch": 1.696,
      "grad_norm": 1.1137765645980835,
      "learning_rate": 7.727272727272727e-07,
      "loss": 0.4378,
      "step": 4240
    },
    {
      "epoch": 1.7,
      "grad_norm": 1.5201764106750488,
      "learning_rate": 7.626262626262627e-07,
      "loss": 0.5002,
      "step": 4250
    },
    {
      "epoch": 1.704,
      "grad_norm": 1.3116071224212646,
      "learning_rate": 7.525252525252526e-07,
      "loss": 0.4465,
      "step": 4260
    },
    {
      "epoch": 1.708,
      "grad_norm": 1.0860885381698608,
      "learning_rate": 7.424242424242425e-07,
      "loss": 0.4657,
      "step": 4270
    },
    {
      "epoch": 1.712,
      "grad_norm": 1.2434557676315308,
      "learning_rate": 7.323232323232324e-07,
      "loss": 0.5012,
      "step": 4280
    },
    {
      "epoch": 1.716,
      "grad_norm": 1.10932195186615,
      "learning_rate": 7.222222222222222e-07,
      "loss": 0.4641,
      "step": 4290
    },
    {
      "epoch": 1.72,
      "grad_norm": 1.5149301290512085,
      "learning_rate": 7.121212121212122e-07,
      "loss": 0.4741,
      "step": 4300
    },
    {
      "epoch": 1.724,
      "grad_norm": 1.2682241201400757,
      "learning_rate": 7.02020202020202e-07,
      "loss": 0.5231,
      "step": 4310
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.957802951335907,
      "learning_rate": 6.919191919191919e-07,
      "loss": 0.5065,
      "step": 4320
    },
    {
      "epoch": 1.732,
      "grad_norm": 0.917685329914093,
      "learning_rate": 6.818181818181818e-07,
      "loss": 0.4814,
      "step": 4330
    },
    {
      "epoch": 1.736,
      "grad_norm": 1.0093811750411987,
      "learning_rate": 6.717171717171718e-07,
      "loss": 0.4498,
      "step": 4340
    },
    {
      "epoch": 1.74,
      "grad_norm": 1.0540812015533447,
      "learning_rate": 6.616161616161616e-07,
      "loss": 0.5466,
      "step": 4350
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.8856184482574463,
      "learning_rate": 6.515151515151516e-07,
      "loss": 0.5026,
      "step": 4360
    },
    {
      "epoch": 1.748,
      "grad_norm": 1.4447383880615234,
      "learning_rate": 6.414141414141415e-07,
      "loss": 0.5111,
      "step": 4370
    },
    {
      "epoch": 1.752,
      "grad_norm": 1.1212652921676636,
      "learning_rate": 6.313131313131314e-07,
      "loss": 0.5392,
      "step": 4380
    },
    {
      "epoch": 1.756,
      "grad_norm": 1.6557326316833496,
      "learning_rate": 6.212121212121212e-07,
      "loss": 0.5052,
      "step": 4390
    },
    {
      "epoch": 1.76,
      "grad_norm": 1.1236473321914673,
      "learning_rate": 6.111111111111112e-07,
      "loss": 0.4876,
      "step": 4400
    },
    {
      "epoch": 1.764,
      "grad_norm": 0.959677517414093,
      "learning_rate": 6.01010101010101e-07,
      "loss": 0.5437,
      "step": 4410
    },
    {
      "epoch": 1.768,
      "grad_norm": 1.0632014274597168,
      "learning_rate": 5.90909090909091e-07,
      "loss": 0.5602,
      "step": 4420
    },
    {
      "epoch": 1.772,
      "grad_norm": 1.0628646612167358,
      "learning_rate": 5.808080808080809e-07,
      "loss": 0.4767,
      "step": 4430
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.9473639130592346,
      "learning_rate": 5.707070707070707e-07,
      "loss": 0.3726,
      "step": 4440
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.926861584186554,
      "learning_rate": 5.606060606060607e-07,
      "loss": 0.4815,
      "step": 4450
    },
    {
      "epoch": 1.784,
      "grad_norm": 1.2255462408065796,
      "learning_rate": 5.505050505050506e-07,
      "loss": 0.3746,
      "step": 4460
    },
    {
      "epoch": 1.788,
      "grad_norm": 0.986872673034668,
      "learning_rate": 5.404040404040404e-07,
      "loss": 0.4817,
      "step": 4470
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.9506597518920898,
      "learning_rate": 5.303030303030304e-07,
      "loss": 0.4214,
      "step": 4480
    },
    {
      "epoch": 1.796,
      "grad_norm": 1.1798529624938965,
      "learning_rate": 5.202020202020203e-07,
      "loss": 0.4499,
      "step": 4490
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.242308497428894,
      "learning_rate": 5.101010101010101e-07,
      "loss": 0.357,
      "step": 4500
    },
    {
      "epoch": 1.804,
      "grad_norm": 1.005298376083374,
      "learning_rate": 5.000000000000001e-07,
      "loss": 0.4183,
      "step": 4510
    },
    {
      "epoch": 1.808,
      "grad_norm": 1.4436419010162354,
      "learning_rate": 4.898989898989899e-07,
      "loss": 0.4876,
      "step": 4520
    },
    {
      "epoch": 1.812,
      "grad_norm": 0.989608645439148,
      "learning_rate": 4.797979797979798e-07,
      "loss": 0.4814,
      "step": 4530
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 1.2445369958877563,
      "learning_rate": 4.696969696969697e-07,
      "loss": 0.4777,
      "step": 4540
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 1.1575411558151245,
      "learning_rate": 4.595959595959596e-07,
      "loss": 0.4418,
      "step": 4550
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 1.0956766605377197,
      "learning_rate": 4.494949494949495e-07,
      "loss": 0.4969,
      "step": 4560
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 1.2304232120513916,
      "learning_rate": 4.393939393939394e-07,
      "loss": 0.4998,
      "step": 4570
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 1.4180818796157837,
      "learning_rate": 4.2929292929292934e-07,
      "loss": 0.4576,
      "step": 4580
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 0.928998589515686,
      "learning_rate": 4.191919191919192e-07,
      "loss": 0.4547,
      "step": 4590
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 1.1042160987854004,
      "learning_rate": 4.090909090909091e-07,
      "loss": 0.4358,
      "step": 4600
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 1.0551533699035645,
      "learning_rate": 3.9898989898989903e-07,
      "loss": 0.4478,
      "step": 4610
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.9283189177513123,
      "learning_rate": 3.8888888888888895e-07,
      "loss": 0.4816,
      "step": 4620
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 1.2269121408462524,
      "learning_rate": 3.787878787878788e-07,
      "loss": 0.5489,
      "step": 4630
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 1.0219532251358032,
      "learning_rate": 3.6868686868686873e-07,
      "loss": 0.406,
      "step": 4640
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 1.0101099014282227,
      "learning_rate": 3.5858585858585864e-07,
      "loss": 0.5595,
      "step": 4650
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 1.1638087034225464,
      "learning_rate": 3.4848484848484856e-07,
      "loss": 0.3831,
      "step": 4660
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 1.2268677949905396,
      "learning_rate": 3.383838383838384e-07,
      "loss": 0.6211,
      "step": 4670
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 1.2524902820587158,
      "learning_rate": 3.2828282828282834e-07,
      "loss": 0.4752,
      "step": 4680
    },
    {
      "epoch": 1.876,
      "grad_norm": 1.1543196439743042,
      "learning_rate": 3.181818181818182e-07,
      "loss": 0.4223,
      "step": 4690
    },
    {
      "epoch": 1.88,
      "grad_norm": 1.0966638326644897,
      "learning_rate": 3.080808080808081e-07,
      "loss": 0.472,
      "step": 4700
    },
    {
      "epoch": 1.884,
      "grad_norm": 1.1658079624176025,
      "learning_rate": 2.9797979797979803e-07,
      "loss": 0.4266,
      "step": 4710
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.9538494944572449,
      "learning_rate": 2.878787878787879e-07,
      "loss": 0.5155,
      "step": 4720
    },
    {
      "epoch": 1.892,
      "grad_norm": 1.1443248987197876,
      "learning_rate": 2.7777777777777776e-07,
      "loss": 0.4591,
      "step": 4730
    },
    {
      "epoch": 1.896,
      "grad_norm": 1.1630768775939941,
      "learning_rate": 2.676767676767677e-07,
      "loss": 0.4915,
      "step": 4740
    },
    {
      "epoch": 1.9,
      "grad_norm": 1.0958446264266968,
      "learning_rate": 2.575757575757576e-07,
      "loss": 0.5391,
      "step": 4750
    },
    {
      "epoch": 1.904,
      "grad_norm": 1.2078125476837158,
      "learning_rate": 2.474747474747475e-07,
      "loss": 0.4861,
      "step": 4760
    },
    {
      "epoch": 1.908,
      "grad_norm": 1.1943961381912231,
      "learning_rate": 2.373737373737374e-07,
      "loss": 0.4426,
      "step": 4770
    },
    {
      "epoch": 1.912,
      "grad_norm": 1.2457008361816406,
      "learning_rate": 2.2727272727272729e-07,
      "loss": 0.5454,
      "step": 4780
    },
    {
      "epoch": 1.916,
      "grad_norm": 1.327457308769226,
      "learning_rate": 2.171717171717172e-07,
      "loss": 0.4907,
      "step": 4790
    },
    {
      "epoch": 1.92,
      "grad_norm": 1.0271192789077759,
      "learning_rate": 2.070707070707071e-07,
      "loss": 0.4883,
      "step": 4800
    },
    {
      "epoch": 1.924,
      "grad_norm": 1.2202067375183105,
      "learning_rate": 1.9696969696969698e-07,
      "loss": 0.4565,
      "step": 4810
    },
    {
      "epoch": 1.928,
      "grad_norm": 1.1866023540496826,
      "learning_rate": 1.8686868686868687e-07,
      "loss": 0.6204,
      "step": 4820
    },
    {
      "epoch": 1.932,
      "grad_norm": 0.8552526831626892,
      "learning_rate": 1.767676767676768e-07,
      "loss": 0.48,
      "step": 4830
    },
    {
      "epoch": 1.936,
      "grad_norm": 1.1937693357467651,
      "learning_rate": 1.6666666666666668e-07,
      "loss": 0.4476,
      "step": 4840
    },
    {
      "epoch": 1.94,
      "grad_norm": 1.024554967880249,
      "learning_rate": 1.565656565656566e-07,
      "loss": 0.5532,
      "step": 4850
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.954827606678009,
      "learning_rate": 1.4646464646464648e-07,
      "loss": 0.3755,
      "step": 4860
    },
    {
      "epoch": 1.948,
      "grad_norm": 1.0696877241134644,
      "learning_rate": 1.3636363636363637e-07,
      "loss": 0.4823,
      "step": 4870
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.9855774641036987,
      "learning_rate": 1.2626262626262626e-07,
      "loss": 0.4684,
      "step": 4880
    },
    {
      "epoch": 1.956,
      "grad_norm": 1.4587699174880981,
      "learning_rate": 1.1616161616161618e-07,
      "loss": 0.4582,
      "step": 4890
    },
    {
      "epoch": 1.96,
      "grad_norm": 1.4440795183181763,
      "learning_rate": 1.0606060606060608e-07,
      "loss": 0.4628,
      "step": 4900
    },
    {
      "epoch": 1.964,
      "grad_norm": 1.0438967943191528,
      "learning_rate": 9.595959595959596e-08,
      "loss": 0.4554,
      "step": 4910
    },
    {
      "epoch": 1.968,
      "grad_norm": 1.148707628250122,
      "learning_rate": 8.585858585858586e-08,
      "loss": 0.5195,
      "step": 4920
    },
    {
      "epoch": 1.972,
      "grad_norm": 1.4163304567337036,
      "learning_rate": 7.575757575757576e-08,
      "loss": 0.4487,
      "step": 4930
    },
    {
      "epoch": 1.976,
      "grad_norm": 1.2889244556427002,
      "learning_rate": 6.565656565656566e-08,
      "loss": 0.4546,
      "step": 4940
    },
    {
      "epoch": 1.98,
      "grad_norm": 1.1414072513580322,
      "learning_rate": 5.555555555555556e-08,
      "loss": 0.4387,
      "step": 4950
    },
    {
      "epoch": 1.984,
      "grad_norm": 1.105636477470398,
      "learning_rate": 4.545454545454546e-08,
      "loss": 0.5211,
      "step": 4960
    },
    {
      "epoch": 1.988,
      "grad_norm": 0.9591394066810608,
      "learning_rate": 3.535353535353536e-08,
      "loss": 0.4806,
      "step": 4970
    },
    {
      "epoch": 1.992,
      "grad_norm": 1.2730292081832886,
      "learning_rate": 2.5252525252525256e-08,
      "loss": 0.3995,
      "step": 4980
    },
    {
      "epoch": 1.996,
      "grad_norm": 1.1605520248413086,
      "learning_rate": 1.5151515151515152e-08,
      "loss": 0.5672,
      "step": 4990
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.1056946516036987,
      "learning_rate": 5.050505050505051e-09,
      "loss": 0.4755,
      "step": 5000
    }
  ],
  "logging_steps": 10,
  "max_steps": 5000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.707085922304e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
