{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9993619057747527,
  "eval_steps": 500,
  "global_step": 4700,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00425396150164841,
      "grad_norm": 7.864521026611328,
      "learning_rate": 7.000000000000001e-07,
      "loss": 2.858,
      "step": 10
    },
    {
      "epoch": 0.00850792300329682,
      "grad_norm": 4.6059889793396,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 3.3552,
      "step": 20
    },
    {
      "epoch": 0.01276188450494523,
      "grad_norm": 2.3008477687835693,
      "learning_rate": 2.7000000000000004e-06,
      "loss": 3.0668,
      "step": 30
    },
    {
      "epoch": 0.01701584600659364,
      "grad_norm": 6.187544345855713,
      "learning_rate": 3.7e-06,
      "loss": 2.6234,
      "step": 40
    },
    {
      "epoch": 0.02126980750824205,
      "grad_norm": 8.445459365844727,
      "learning_rate": 4.7e-06,
      "loss": 3.3079,
      "step": 50
    },
    {
      "epoch": 0.02552376900989046,
      "grad_norm": 7.637294769287109,
      "learning_rate": 4.99247311827957e-06,
      "loss": 2.8518,
      "step": 60
    },
    {
      "epoch": 0.02977773051153887,
      "grad_norm": 4.7513346672058105,
      "learning_rate": 4.981720430107527e-06,
      "loss": 2.8125,
      "step": 70
    },
    {
      "epoch": 0.03403169201318728,
      "grad_norm": 3.404093027114868,
      "learning_rate": 4.970967741935484e-06,
      "loss": 2.465,
      "step": 80
    },
    {
      "epoch": 0.03828565351483569,
      "grad_norm": 11.951262474060059,
      "learning_rate": 4.960215053763442e-06,
      "loss": 2.8407,
      "step": 90
    },
    {
      "epoch": 0.0425396150164841,
      "grad_norm": 10.543342590332031,
      "learning_rate": 4.949462365591398e-06,
      "loss": 2.1025,
      "step": 100
    },
    {
      "epoch": 0.04679357651813251,
      "grad_norm": 4.703197956085205,
      "learning_rate": 4.938709677419355e-06,
      "loss": 2.2973,
      "step": 110
    },
    {
      "epoch": 0.05104753801978092,
      "grad_norm": 5.400496482849121,
      "learning_rate": 4.927956989247312e-06,
      "loss": 2.2028,
      "step": 120
    },
    {
      "epoch": 0.055301499521429334,
      "grad_norm": 0.6464048027992249,
      "learning_rate": 4.917204301075269e-06,
      "loss": 2.1556,
      "step": 130
    },
    {
      "epoch": 0.05955546102307774,
      "grad_norm": 6.224825859069824,
      "learning_rate": 4.9064516129032266e-06,
      "loss": 1.9326,
      "step": 140
    },
    {
      "epoch": 0.06380942252472616,
      "grad_norm": 1.7657017707824707,
      "learning_rate": 4.895698924731183e-06,
      "loss": 1.785,
      "step": 150
    },
    {
      "epoch": 0.06806338402637456,
      "grad_norm": 0.6758752465248108,
      "learning_rate": 4.88494623655914e-06,
      "loss": 1.7743,
      "step": 160
    },
    {
      "epoch": 0.07231734552802298,
      "grad_norm": 6.722196102142334,
      "learning_rate": 4.874193548387097e-06,
      "loss": 1.821,
      "step": 170
    },
    {
      "epoch": 0.07657130702967138,
      "grad_norm": 1.2623530626296997,
      "learning_rate": 4.863440860215054e-06,
      "loss": 1.7692,
      "step": 180
    },
    {
      "epoch": 0.08082526853131979,
      "grad_norm": 1.541988492012024,
      "learning_rate": 4.8526881720430115e-06,
      "loss": 1.6912,
      "step": 190
    },
    {
      "epoch": 0.0850792300329682,
      "grad_norm": 4.04231071472168,
      "learning_rate": 4.841935483870968e-06,
      "loss": 1.6119,
      "step": 200
    },
    {
      "epoch": 0.08933319153461661,
      "grad_norm": 0.9894896149635315,
      "learning_rate": 4.831182795698925e-06,
      "loss": 1.5084,
      "step": 210
    },
    {
      "epoch": 0.09358715303626503,
      "grad_norm": 0.6935451626777649,
      "learning_rate": 4.820430107526882e-06,
      "loss": 1.5809,
      "step": 220
    },
    {
      "epoch": 0.09784111453791343,
      "grad_norm": 0.8857315182685852,
      "learning_rate": 4.809677419354839e-06,
      "loss": 1.5142,
      "step": 230
    },
    {
      "epoch": 0.10209507603956185,
      "grad_norm": 0.6266822814941406,
      "learning_rate": 4.7989247311827964e-06,
      "loss": 1.4898,
      "step": 240
    },
    {
      "epoch": 0.10634903754121025,
      "grad_norm": 0.9445804357528687,
      "learning_rate": 4.788172043010753e-06,
      "loss": 1.5595,
      "step": 250
    },
    {
      "epoch": 0.11060299904285867,
      "grad_norm": 0.7937890887260437,
      "learning_rate": 4.77741935483871e-06,
      "loss": 1.4903,
      "step": 260
    },
    {
      "epoch": 0.11485696054450707,
      "grad_norm": 0.6631437540054321,
      "learning_rate": 4.766666666666667e-06,
      "loss": 1.4964,
      "step": 270
    },
    {
      "epoch": 0.11911092204615548,
      "grad_norm": 0.7779302597045898,
      "learning_rate": 4.755913978494624e-06,
      "loss": 1.5767,
      "step": 280
    },
    {
      "epoch": 0.1233648835478039,
      "grad_norm": 0.8279611468315125,
      "learning_rate": 4.7451612903225805e-06,
      "loss": 1.5081,
      "step": 290
    },
    {
      "epoch": 0.1276188450494523,
      "grad_norm": 1.2389293909072876,
      "learning_rate": 4.7344086021505385e-06,
      "loss": 1.3821,
      "step": 300
    },
    {
      "epoch": 0.1318728065511007,
      "grad_norm": 0.9438751935958862,
      "learning_rate": 4.723655913978495e-06,
      "loss": 1.4942,
      "step": 310
    },
    {
      "epoch": 0.13612676805274912,
      "grad_norm": 0.7260102033615112,
      "learning_rate": 4.712903225806452e-06,
      "loss": 1.3539,
      "step": 320
    },
    {
      "epoch": 0.14038072955439754,
      "grad_norm": 0.7408748865127563,
      "learning_rate": 4.702150537634409e-06,
      "loss": 1.5551,
      "step": 330
    },
    {
      "epoch": 0.14463469105604596,
      "grad_norm": 0.8001999258995056,
      "learning_rate": 4.6913978494623654e-06,
      "loss": 1.4962,
      "step": 340
    },
    {
      "epoch": 0.14888865255769435,
      "grad_norm": 0.7838761210441589,
      "learning_rate": 4.6806451612903235e-06,
      "loss": 1.4045,
      "step": 350
    },
    {
      "epoch": 0.15314261405934276,
      "grad_norm": 0.6787046790122986,
      "learning_rate": 4.66989247311828e-06,
      "loss": 1.465,
      "step": 360
    },
    {
      "epoch": 0.15739657556099118,
      "grad_norm": 0.7601308226585388,
      "learning_rate": 4.659139784946237e-06,
      "loss": 1.4724,
      "step": 370
    },
    {
      "epoch": 0.16165053706263957,
      "grad_norm": 0.7421821355819702,
      "learning_rate": 4.648387096774194e-06,
      "loss": 1.3976,
      "step": 380
    },
    {
      "epoch": 0.165904498564288,
      "grad_norm": 2.571857213973999,
      "learning_rate": 4.63763440860215e-06,
      "loss": 1.3071,
      "step": 390
    },
    {
      "epoch": 0.1701584600659364,
      "grad_norm": 0.7317758202552795,
      "learning_rate": 4.626881720430108e-06,
      "loss": 1.4557,
      "step": 400
    },
    {
      "epoch": 0.17441242156758482,
      "grad_norm": 0.7648191452026367,
      "learning_rate": 4.616129032258065e-06,
      "loss": 1.4131,
      "step": 410
    },
    {
      "epoch": 0.17866638306923321,
      "grad_norm": 0.6723394989967346,
      "learning_rate": 4.605376344086022e-06,
      "loss": 1.3648,
      "step": 420
    },
    {
      "epoch": 0.18292034457088163,
      "grad_norm": 1.7330658435821533,
      "learning_rate": 4.594623655913979e-06,
      "loss": 1.3319,
      "step": 430
    },
    {
      "epoch": 0.18717430607253005,
      "grad_norm": 0.7132043242454529,
      "learning_rate": 4.583870967741936e-06,
      "loss": 1.4272,
      "step": 440
    },
    {
      "epoch": 0.19142826757417844,
      "grad_norm": 0.8984608054161072,
      "learning_rate": 4.573118279569893e-06,
      "loss": 1.3348,
      "step": 450
    },
    {
      "epoch": 0.19568222907582686,
      "grad_norm": 0.7621375918388367,
      "learning_rate": 4.56236559139785e-06,
      "loss": 1.4518,
      "step": 460
    },
    {
      "epoch": 0.19993619057747528,
      "grad_norm": 0.7713459134101868,
      "learning_rate": 4.551612903225807e-06,
      "loss": 1.3051,
      "step": 470
    },
    {
      "epoch": 0.2041901520791237,
      "grad_norm": 0.8330646753311157,
      "learning_rate": 4.540860215053764e-06,
      "loss": 1.3563,
      "step": 480
    },
    {
      "epoch": 0.20844411358077208,
      "grad_norm": 1.030447006225586,
      "learning_rate": 4.530107526881721e-06,
      "loss": 1.3368,
      "step": 490
    },
    {
      "epoch": 0.2126980750824205,
      "grad_norm": 0.7823641300201416,
      "learning_rate": 4.519354838709678e-06,
      "loss": 1.3498,
      "step": 500
    },
    {
      "epoch": 0.21695203658406892,
      "grad_norm": 0.6921542286872864,
      "learning_rate": 4.5086021505376346e-06,
      "loss": 1.2911,
      "step": 510
    },
    {
      "epoch": 0.22120599808571734,
      "grad_norm": 0.9635091423988342,
      "learning_rate": 4.497849462365592e-06,
      "loss": 1.387,
      "step": 520
    },
    {
      "epoch": 0.22545995958736573,
      "grad_norm": 0.7931999564170837,
      "learning_rate": 4.487096774193549e-06,
      "loss": 1.3294,
      "step": 530
    },
    {
      "epoch": 0.22971392108901414,
      "grad_norm": 0.8285385966300964,
      "learning_rate": 4.476344086021506e-06,
      "loss": 1.2443,
      "step": 540
    },
    {
      "epoch": 0.23396788259066256,
      "grad_norm": 0.7096374034881592,
      "learning_rate": 4.465591397849462e-06,
      "loss": 1.2922,
      "step": 550
    },
    {
      "epoch": 0.23822184409231095,
      "grad_norm": 0.9004937410354614,
      "learning_rate": 4.4548387096774195e-06,
      "loss": 1.1649,
      "step": 560
    },
    {
      "epoch": 0.24247580559395937,
      "grad_norm": 0.9265022873878479,
      "learning_rate": 4.444086021505377e-06,
      "loss": 1.2675,
      "step": 570
    },
    {
      "epoch": 0.2467297670956078,
      "grad_norm": 1.0301387310028076,
      "learning_rate": 4.433333333333334e-06,
      "loss": 1.2636,
      "step": 580
    },
    {
      "epoch": 0.2509837285972562,
      "grad_norm": 0.7712973952293396,
      "learning_rate": 4.422580645161291e-06,
      "loss": 1.2263,
      "step": 590
    },
    {
      "epoch": 0.2552376900989046,
      "grad_norm": 0.8089145421981812,
      "learning_rate": 4.411827956989247e-06,
      "loss": 1.202,
      "step": 600
    },
    {
      "epoch": 0.25949165160055304,
      "grad_norm": 0.7349789142608643,
      "learning_rate": 4.4010752688172044e-06,
      "loss": 1.1319,
      "step": 610
    },
    {
      "epoch": 0.2637456131022014,
      "grad_norm": 0.8121728301048279,
      "learning_rate": 4.390322580645162e-06,
      "loss": 1.258,
      "step": 620
    },
    {
      "epoch": 0.2679995746038498,
      "grad_norm": 0.8431875705718994,
      "learning_rate": 4.379569892473119e-06,
      "loss": 1.2641,
      "step": 630
    },
    {
      "epoch": 0.27225353610549824,
      "grad_norm": 1.0607984066009521,
      "learning_rate": 4.368817204301076e-06,
      "loss": 1.33,
      "step": 640
    },
    {
      "epoch": 0.27650749760714666,
      "grad_norm": 0.9929100871086121,
      "learning_rate": 4.358064516129032e-06,
      "loss": 1.2781,
      "step": 650
    },
    {
      "epoch": 0.2807614591087951,
      "grad_norm": 0.775037407875061,
      "learning_rate": 4.34731182795699e-06,
      "loss": 1.3527,
      "step": 660
    },
    {
      "epoch": 0.2850154206104435,
      "grad_norm": 0.7842227220535278,
      "learning_rate": 4.3365591397849465e-06,
      "loss": 1.1708,
      "step": 670
    },
    {
      "epoch": 0.2892693821120919,
      "grad_norm": 10.088332176208496,
      "learning_rate": 4.325806451612904e-06,
      "loss": 1.2012,
      "step": 680
    },
    {
      "epoch": 0.2935233436137403,
      "grad_norm": 0.8717785477638245,
      "learning_rate": 4.315053763440861e-06,
      "loss": 1.1488,
      "step": 690
    },
    {
      "epoch": 0.2977773051153887,
      "grad_norm": 0.8963331580162048,
      "learning_rate": 4.304301075268817e-06,
      "loss": 1.1579,
      "step": 700
    },
    {
      "epoch": 0.3020312666170371,
      "grad_norm": 0.958288848400116,
      "learning_rate": 4.293548387096775e-06,
      "loss": 1.2873,
      "step": 710
    },
    {
      "epoch": 0.3062852281186855,
      "grad_norm": 1.4913673400878906,
      "learning_rate": 4.2827956989247314e-06,
      "loss": 1.2447,
      "step": 720
    },
    {
      "epoch": 0.31053918962033394,
      "grad_norm": 0.8404967188835144,
      "learning_rate": 4.272043010752689e-06,
      "loss": 1.2731,
      "step": 730
    },
    {
      "epoch": 0.31479315112198236,
      "grad_norm": 0.773956298828125,
      "learning_rate": 4.261290322580646e-06,
      "loss": 1.213,
      "step": 740
    },
    {
      "epoch": 0.3190471126236308,
      "grad_norm": 0.9659913778305054,
      "learning_rate": 4.250537634408602e-06,
      "loss": 1.2281,
      "step": 750
    },
    {
      "epoch": 0.32330107412527914,
      "grad_norm": 1.0225740671157837,
      "learning_rate": 4.239784946236559e-06,
      "loss": 1.1677,
      "step": 760
    },
    {
      "epoch": 0.32755503562692756,
      "grad_norm": 1.0215213298797607,
      "learning_rate": 4.229032258064516e-06,
      "loss": 1.2412,
      "step": 770
    },
    {
      "epoch": 0.331808997128576,
      "grad_norm": 1.189651608467102,
      "learning_rate": 4.2182795698924735e-06,
      "loss": 1.2509,
      "step": 780
    },
    {
      "epoch": 0.3360629586302244,
      "grad_norm": 0.8779934644699097,
      "learning_rate": 4.207526881720431e-06,
      "loss": 1.2484,
      "step": 790
    },
    {
      "epoch": 0.3403169201318728,
      "grad_norm": 0.8199947476387024,
      "learning_rate": 4.196774193548388e-06,
      "loss": 1.0278,
      "step": 800
    },
    {
      "epoch": 0.34457088163352123,
      "grad_norm": 0.9598624110221863,
      "learning_rate": 4.186021505376344e-06,
      "loss": 1.1618,
      "step": 810
    },
    {
      "epoch": 0.34882484313516965,
      "grad_norm": 4.860691070556641,
      "learning_rate": 4.175268817204301e-06,
      "loss": 1.1003,
      "step": 820
    },
    {
      "epoch": 0.353078804636818,
      "grad_norm": 1.239388346672058,
      "learning_rate": 4.1645161290322585e-06,
      "loss": 1.122,
      "step": 830
    },
    {
      "epoch": 0.35733276613846643,
      "grad_norm": 1.0309749841690063,
      "learning_rate": 4.153763440860216e-06,
      "loss": 1.0285,
      "step": 840
    },
    {
      "epoch": 0.36158672764011485,
      "grad_norm": 1.0897265672683716,
      "learning_rate": 4.143010752688173e-06,
      "loss": 1.1293,
      "step": 850
    },
    {
      "epoch": 0.36584068914176326,
      "grad_norm": 1.3884679079055786,
      "learning_rate": 4.132258064516129e-06,
      "loss": 1.0967,
      "step": 860
    },
    {
      "epoch": 0.3700946506434117,
      "grad_norm": 1.1510387659072876,
      "learning_rate": 4.121505376344086e-06,
      "loss": 1.1861,
      "step": 870
    },
    {
      "epoch": 0.3743486121450601,
      "grad_norm": 2.3367116451263428,
      "learning_rate": 4.110752688172043e-06,
      "loss": 1.0409,
      "step": 880
    },
    {
      "epoch": 0.3786025736467085,
      "grad_norm": 0.943311333656311,
      "learning_rate": 4.1e-06,
      "loss": 1.173,
      "step": 890
    },
    {
      "epoch": 0.3828565351483569,
      "grad_norm": 0.9504607915878296,
      "learning_rate": 4.089247311827958e-06,
      "loss": 1.1165,
      "step": 900
    },
    {
      "epoch": 0.3871104966500053,
      "grad_norm": 1.0685200691223145,
      "learning_rate": 4.078494623655914e-06,
      "loss": 1.0539,
      "step": 910
    },
    {
      "epoch": 0.3913644581516537,
      "grad_norm": 1.7657865285873413,
      "learning_rate": 4.067741935483871e-06,
      "loss": 1.1908,
      "step": 920
    },
    {
      "epoch": 0.39561841965330213,
      "grad_norm": 1.0309491157531738,
      "learning_rate": 4.056989247311828e-06,
      "loss": 1.1653,
      "step": 930
    },
    {
      "epoch": 0.39987238115495055,
      "grad_norm": 0.9199163317680359,
      "learning_rate": 4.0462365591397855e-06,
      "loss": 1.0194,
      "step": 940
    },
    {
      "epoch": 0.40412634265659897,
      "grad_norm": 1.059890866279602,
      "learning_rate": 4.035483870967743e-06,
      "loss": 1.0916,
      "step": 950
    },
    {
      "epoch": 0.4083803041582474,
      "grad_norm": 1.0083298683166504,
      "learning_rate": 4.024731182795699e-06,
      "loss": 1.1349,
      "step": 960
    },
    {
      "epoch": 0.4126342656598958,
      "grad_norm": 1.1484085321426392,
      "learning_rate": 4.013978494623656e-06,
      "loss": 1.0601,
      "step": 970
    },
    {
      "epoch": 0.41688822716154417,
      "grad_norm": 1.0383737087249756,
      "learning_rate": 4.003225806451613e-06,
      "loss": 1.0798,
      "step": 980
    },
    {
      "epoch": 0.4211421886631926,
      "grad_norm": 1.004211187362671,
      "learning_rate": 3.99247311827957e-06,
      "loss": 1.066,
      "step": 990
    },
    {
      "epoch": 0.425396150164841,
      "grad_norm": 1.2514740228652954,
      "learning_rate": 3.981720430107528e-06,
      "loss": 1.043,
      "step": 1000
    },
    {
      "epoch": 0.4296501116664894,
      "grad_norm": 1.0586445331573486,
      "learning_rate": 3.970967741935484e-06,
      "loss": 1.1361,
      "step": 1010
    },
    {
      "epoch": 0.43390407316813784,
      "grad_norm": 1.120330810546875,
      "learning_rate": 3.960215053763441e-06,
      "loss": 1.1799,
      "step": 1020
    },
    {
      "epoch": 0.43815803466978626,
      "grad_norm": 1.1935129165649414,
      "learning_rate": 3.949462365591398e-06,
      "loss": 1.2407,
      "step": 1030
    },
    {
      "epoch": 0.4424119961714347,
      "grad_norm": 1.4454847574234009,
      "learning_rate": 3.938709677419355e-06,
      "loss": 1.2285,
      "step": 1040
    },
    {
      "epoch": 0.44666595767308304,
      "grad_norm": 1.327786922454834,
      "learning_rate": 3.9279569892473125e-06,
      "loss": 1.0106,
      "step": 1050
    },
    {
      "epoch": 0.45091991917473145,
      "grad_norm": 1.6359237432479858,
      "learning_rate": 3.917204301075269e-06,
      "loss": 1.0827,
      "step": 1060
    },
    {
      "epoch": 0.45517388067637987,
      "grad_norm": 1.384461522102356,
      "learning_rate": 3.906451612903226e-06,
      "loss": 1.1979,
      "step": 1070
    },
    {
      "epoch": 0.4594278421780283,
      "grad_norm": 1.2219287157058716,
      "learning_rate": 3.895698924731183e-06,
      "loss": 1.0068,
      "step": 1080
    },
    {
      "epoch": 0.4636818036796767,
      "grad_norm": 1.0434811115264893,
      "learning_rate": 3.88494623655914e-06,
      "loss": 1.1198,
      "step": 1090
    },
    {
      "epoch": 0.4679357651813251,
      "grad_norm": 1.3926187753677368,
      "learning_rate": 3.874193548387097e-06,
      "loss": 1.0176,
      "step": 1100
    },
    {
      "epoch": 0.47218972668297354,
      "grad_norm": 1.26758873462677,
      "learning_rate": 3.863440860215054e-06,
      "loss": 1.0888,
      "step": 1110
    },
    {
      "epoch": 0.4764436881846219,
      "grad_norm": 1.137831687927246,
      "learning_rate": 3.852688172043011e-06,
      "loss": 1.0597,
      "step": 1120
    },
    {
      "epoch": 0.4806976496862703,
      "grad_norm": 0.9696798920631409,
      "learning_rate": 3.841935483870968e-06,
      "loss": 1.0921,
      "step": 1130
    },
    {
      "epoch": 0.48495161118791874,
      "grad_norm": 1.3133236169815063,
      "learning_rate": 3.831182795698925e-06,
      "loss": 1.1825,
      "step": 1140
    },
    {
      "epoch": 0.48920557268956716,
      "grad_norm": 1.3432070016860962,
      "learning_rate": 3.8204301075268815e-06,
      "loss": 1.0651,
      "step": 1150
    },
    {
      "epoch": 0.4934595341912156,
      "grad_norm": 1.1757785081863403,
      "learning_rate": 3.809677419354839e-06,
      "loss": 1.0823,
      "step": 1160
    },
    {
      "epoch": 0.497713495692864,
      "grad_norm": 1.0073318481445312,
      "learning_rate": 3.798924731182796e-06,
      "loss": 1.106,
      "step": 1170
    },
    {
      "epoch": 0.5019674571945124,
      "grad_norm": 1.195915699005127,
      "learning_rate": 3.7881720430107534e-06,
      "loss": 1.0214,
      "step": 1180
    },
    {
      "epoch": 0.5062214186961608,
      "grad_norm": 1.0845832824707031,
      "learning_rate": 3.77741935483871e-06,
      "loss": 1.2178,
      "step": 1190
    },
    {
      "epoch": 0.5104753801978092,
      "grad_norm": 1.4093832969665527,
      "learning_rate": 3.766666666666667e-06,
      "loss": 0.9704,
      "step": 1200
    },
    {
      "epoch": 0.5147293416994576,
      "grad_norm": 1.19739830493927,
      "learning_rate": 3.755913978494624e-06,
      "loss": 1.0728,
      "step": 1210
    },
    {
      "epoch": 0.5189833032011061,
      "grad_norm": 1.3326925039291382,
      "learning_rate": 3.7451612903225808e-06,
      "loss": 1.0563,
      "step": 1220
    },
    {
      "epoch": 0.5232372647027544,
      "grad_norm": 1.3333690166473389,
      "learning_rate": 3.7344086021505375e-06,
      "loss": 1.0569,
      "step": 1230
    },
    {
      "epoch": 0.5274912262044028,
      "grad_norm": 1.22807776927948,
      "learning_rate": 3.723655913978495e-06,
      "loss": 0.9681,
      "step": 1240
    },
    {
      "epoch": 0.5317451877060513,
      "grad_norm": 1.8376121520996094,
      "learning_rate": 3.712903225806452e-06,
      "loss": 0.995,
      "step": 1250
    },
    {
      "epoch": 0.5359991492076996,
      "grad_norm": 1.1095569133758545,
      "learning_rate": 3.702150537634409e-06,
      "loss": 1.0524,
      "step": 1260
    },
    {
      "epoch": 0.5402531107093481,
      "grad_norm": 1.4553648233413696,
      "learning_rate": 3.6913978494623657e-06,
      "loss": 1.0939,
      "step": 1270
    },
    {
      "epoch": 0.5445070722109965,
      "grad_norm": 1.213693618774414,
      "learning_rate": 3.680645161290323e-06,
      "loss": 1.0043,
      "step": 1280
    },
    {
      "epoch": 0.548761033712645,
      "grad_norm": 1.5261226892471313,
      "learning_rate": 3.66989247311828e-06,
      "loss": 1.0527,
      "step": 1290
    },
    {
      "epoch": 0.5530149952142933,
      "grad_norm": 1.2122291326522827,
      "learning_rate": 3.6591397849462367e-06,
      "loss": 1.0107,
      "step": 1300
    },
    {
      "epoch": 0.5572689567159417,
      "grad_norm": 1.0921539068222046,
      "learning_rate": 3.648387096774194e-06,
      "loss": 1.0024,
      "step": 1310
    },
    {
      "epoch": 0.5615229182175902,
      "grad_norm": 1.2163686752319336,
      "learning_rate": 3.637634408602151e-06,
      "loss": 1.0897,
      "step": 1320
    },
    {
      "epoch": 0.5657768797192385,
      "grad_norm": 1.4876395463943481,
      "learning_rate": 3.626881720430108e-06,
      "loss": 1.0343,
      "step": 1330
    },
    {
      "epoch": 0.570030841220887,
      "grad_norm": 1.4227172136306763,
      "learning_rate": 3.616129032258065e-06,
      "loss": 1.2363,
      "step": 1340
    },
    {
      "epoch": 0.5742848027225353,
      "grad_norm": 1.3462902307510376,
      "learning_rate": 3.6053763440860217e-06,
      "loss": 1.031,
      "step": 1350
    },
    {
      "epoch": 0.5785387642241838,
      "grad_norm": 1.4771519899368286,
      "learning_rate": 3.5946236559139784e-06,
      "loss": 0.907,
      "step": 1360
    },
    {
      "epoch": 0.5827927257258322,
      "grad_norm": 1.6217429637908936,
      "learning_rate": 3.583870967741936e-06,
      "loss": 1.0819,
      "step": 1370
    },
    {
      "epoch": 0.5870466872274805,
      "grad_norm": 1.1423362493515015,
      "learning_rate": 3.5731182795698927e-06,
      "loss": 1.1144,
      "step": 1380
    },
    {
      "epoch": 0.591300648729129,
      "grad_norm": 1.1942871809005737,
      "learning_rate": 3.56236559139785e-06,
      "loss": 1.1351,
      "step": 1390
    },
    {
      "epoch": 0.5955546102307774,
      "grad_norm": 1.1417404413223267,
      "learning_rate": 3.5516129032258066e-06,
      "loss": 1.1608,
      "step": 1400
    },
    {
      "epoch": 0.5998085717324259,
      "grad_norm": 1.1022942066192627,
      "learning_rate": 3.5408602150537633e-06,
      "loss": 0.9943,
      "step": 1410
    },
    {
      "epoch": 0.6040625332340742,
      "grad_norm": 1.270949125289917,
      "learning_rate": 3.530107526881721e-06,
      "loss": 1.0347,
      "step": 1420
    },
    {
      "epoch": 0.6083164947357227,
      "grad_norm": 1.3536255359649658,
      "learning_rate": 3.5193548387096777e-06,
      "loss": 1.1028,
      "step": 1430
    },
    {
      "epoch": 0.612570456237371,
      "grad_norm": 2.6622424125671387,
      "learning_rate": 3.5086021505376344e-06,
      "loss": 1.001,
      "step": 1440
    },
    {
      "epoch": 0.6168244177390194,
      "grad_norm": 1.4162445068359375,
      "learning_rate": 3.4978494623655915e-06,
      "loss": 1.0206,
      "step": 1450
    },
    {
      "epoch": 0.6210783792406679,
      "grad_norm": 1.3189228773117065,
      "learning_rate": 3.4870967741935487e-06,
      "loss": 1.0239,
      "step": 1460
    },
    {
      "epoch": 0.6253323407423163,
      "grad_norm": 1.1190041303634644,
      "learning_rate": 3.476344086021506e-06,
      "loss": 0.9534,
      "step": 1470
    },
    {
      "epoch": 0.6295863022439647,
      "grad_norm": 0.9735876321792603,
      "learning_rate": 3.4655913978494626e-06,
      "loss": 1.06,
      "step": 1480
    },
    {
      "epoch": 0.6338402637456131,
      "grad_norm": 1.0388658046722412,
      "learning_rate": 3.4548387096774193e-06,
      "loss": 0.923,
      "step": 1490
    },
    {
      "epoch": 0.6380942252472616,
      "grad_norm": 1.6564885377883911,
      "learning_rate": 3.444086021505377e-06,
      "loss": 1.1852,
      "step": 1500
    },
    {
      "epoch": 0.6423481867489099,
      "grad_norm": 1.4112520217895508,
      "learning_rate": 3.4333333333333336e-06,
      "loss": 0.8763,
      "step": 1510
    },
    {
      "epoch": 0.6466021482505583,
      "grad_norm": 1.2865664958953857,
      "learning_rate": 3.422580645161291e-06,
      "loss": 1.0067,
      "step": 1520
    },
    {
      "epoch": 0.6508561097522068,
      "grad_norm": 1.265836477279663,
      "learning_rate": 3.4118279569892475e-06,
      "loss": 1.0214,
      "step": 1530
    },
    {
      "epoch": 0.6551100712538551,
      "grad_norm": 1.901168942451477,
      "learning_rate": 3.4010752688172043e-06,
      "loss": 0.9023,
      "step": 1540
    },
    {
      "epoch": 0.6593640327555036,
      "grad_norm": 1.269441843032837,
      "learning_rate": 3.390322580645162e-06,
      "loss": 0.9577,
      "step": 1550
    },
    {
      "epoch": 0.663617994257152,
      "grad_norm": 1.6055995225906372,
      "learning_rate": 3.3795698924731186e-06,
      "loss": 1.0988,
      "step": 1560
    },
    {
      "epoch": 0.6678719557588004,
      "grad_norm": 1.162814974784851,
      "learning_rate": 3.3688172043010753e-06,
      "loss": 0.9758,
      "step": 1570
    },
    {
      "epoch": 0.6721259172604488,
      "grad_norm": 1.239410161972046,
      "learning_rate": 3.3580645161290325e-06,
      "loss": 0.8189,
      "step": 1580
    },
    {
      "epoch": 0.6763798787620972,
      "grad_norm": 1.650429368019104,
      "learning_rate": 3.347311827956989e-06,
      "loss": 0.984,
      "step": 1590
    },
    {
      "epoch": 0.6806338402637456,
      "grad_norm": 1.3572649955749512,
      "learning_rate": 3.3365591397849468e-06,
      "loss": 1.0669,
      "step": 1600
    },
    {
      "epoch": 0.684887801765394,
      "grad_norm": 1.7449524402618408,
      "learning_rate": 3.3258064516129035e-06,
      "loss": 1.1531,
      "step": 1610
    },
    {
      "epoch": 0.6891417632670425,
      "grad_norm": 2.7968380451202393,
      "learning_rate": 3.3150537634408602e-06,
      "loss": 0.9007,
      "step": 1620
    },
    {
      "epoch": 0.6933957247686908,
      "grad_norm": 1.5864797830581665,
      "learning_rate": 3.304301075268818e-06,
      "loss": 0.9586,
      "step": 1630
    },
    {
      "epoch": 0.6976496862703393,
      "grad_norm": 1.443496584892273,
      "learning_rate": 3.2935483870967745e-06,
      "loss": 1.0729,
      "step": 1640
    },
    {
      "epoch": 0.7019036477719877,
      "grad_norm": 1.130816102027893,
      "learning_rate": 3.2827956989247317e-06,
      "loss": 1.1525,
      "step": 1650
    },
    {
      "epoch": 0.706157609273636,
      "grad_norm": 1.7563989162445068,
      "learning_rate": 3.2720430107526884e-06,
      "loss": 0.9633,
      "step": 1660
    },
    {
      "epoch": 0.7104115707752845,
      "grad_norm": 1.3664358854293823,
      "learning_rate": 3.261290322580645e-06,
      "loss": 1.0985,
      "step": 1670
    },
    {
      "epoch": 0.7146655322769329,
      "grad_norm": 2.044419527053833,
      "learning_rate": 3.2505376344086027e-06,
      "loss": 1.0423,
      "step": 1680
    },
    {
      "epoch": 0.7189194937785813,
      "grad_norm": 1.7798104286193848,
      "learning_rate": 3.2397849462365595e-06,
      "loss": 0.9524,
      "step": 1690
    },
    {
      "epoch": 0.7231734552802297,
      "grad_norm": 1.3978170156478882,
      "learning_rate": 3.229032258064516e-06,
      "loss": 0.8909,
      "step": 1700
    },
    {
      "epoch": 0.7274274167818782,
      "grad_norm": 1.3816660642623901,
      "learning_rate": 3.2182795698924734e-06,
      "loss": 1.1334,
      "step": 1710
    },
    {
      "epoch": 0.7316813782835265,
      "grad_norm": 1.4475852251052856,
      "learning_rate": 3.20752688172043e-06,
      "loss": 0.8786,
      "step": 1720
    },
    {
      "epoch": 0.7359353397851749,
      "grad_norm": 1.2577545642852783,
      "learning_rate": 3.1967741935483877e-06,
      "loss": 1.0561,
      "step": 1730
    },
    {
      "epoch": 0.7401893012868234,
      "grad_norm": 1.7342536449432373,
      "learning_rate": 3.1860215053763444e-06,
      "loss": 0.9636,
      "step": 1740
    },
    {
      "epoch": 0.7444432627884717,
      "grad_norm": 1.5507627725601196,
      "learning_rate": 3.175268817204301e-06,
      "loss": 1.1003,
      "step": 1750
    },
    {
      "epoch": 0.7486972242901202,
      "grad_norm": 1.4025006294250488,
      "learning_rate": 3.1645161290322583e-06,
      "loss": 0.9929,
      "step": 1760
    },
    {
      "epoch": 0.7529511857917686,
      "grad_norm": 1.267829179763794,
      "learning_rate": 3.153763440860215e-06,
      "loss": 1.1432,
      "step": 1770
    },
    {
      "epoch": 0.757205147293417,
      "grad_norm": 1.3947747945785522,
      "learning_rate": 3.1430107526881726e-06,
      "loss": 1.1807,
      "step": 1780
    },
    {
      "epoch": 0.7614591087950654,
      "grad_norm": 1.5242648124694824,
      "learning_rate": 3.1322580645161293e-06,
      "loss": 1.0654,
      "step": 1790
    },
    {
      "epoch": 0.7657130702967138,
      "grad_norm": 1.1717305183410645,
      "learning_rate": 3.121505376344086e-06,
      "loss": 1.0096,
      "step": 1800
    },
    {
      "epoch": 0.7699670317983622,
      "grad_norm": 4.994970798492432,
      "learning_rate": 3.1107526881720437e-06,
      "loss": 1.0044,
      "step": 1810
    },
    {
      "epoch": 0.7742209933000106,
      "grad_norm": 1.0586683750152588,
      "learning_rate": 3.1000000000000004e-06,
      "loss": 1.1387,
      "step": 1820
    },
    {
      "epoch": 0.7784749548016591,
      "grad_norm": 1.841587781906128,
      "learning_rate": 3.089247311827957e-06,
      "loss": 1.049,
      "step": 1830
    },
    {
      "epoch": 0.7827289163033074,
      "grad_norm": 1.3344999551773071,
      "learning_rate": 3.0784946236559143e-06,
      "loss": 0.9277,
      "step": 1840
    },
    {
      "epoch": 0.7869828778049559,
      "grad_norm": 1.7170788049697876,
      "learning_rate": 3.067741935483871e-06,
      "loss": 0.9565,
      "step": 1850
    },
    {
      "epoch": 0.7912368393066043,
      "grad_norm": 1.4890025854110718,
      "learning_rate": 3.0569892473118286e-06,
      "loss": 1.1044,
      "step": 1860
    },
    {
      "epoch": 0.7954908008082527,
      "grad_norm": 1.4311902523040771,
      "learning_rate": 3.0462365591397853e-06,
      "loss": 1.007,
      "step": 1870
    },
    {
      "epoch": 0.7997447623099011,
      "grad_norm": 1.3496352434158325,
      "learning_rate": 3.035483870967742e-06,
      "loss": 1.0667,
      "step": 1880
    },
    {
      "epoch": 0.8039987238115495,
      "grad_norm": 1.077616810798645,
      "learning_rate": 3.024731182795699e-06,
      "loss": 0.9643,
      "step": 1890
    },
    {
      "epoch": 0.8082526853131979,
      "grad_norm": 1.5973199605941772,
      "learning_rate": 3.013978494623656e-06,
      "loss": 1.0506,
      "step": 1900
    },
    {
      "epoch": 0.8125066468148463,
      "grad_norm": 1.2724114656448364,
      "learning_rate": 3.0032258064516127e-06,
      "loss": 1.0037,
      "step": 1910
    },
    {
      "epoch": 0.8167606083164948,
      "grad_norm": 2.2590296268463135,
      "learning_rate": 2.9924731182795703e-06,
      "loss": 1.1065,
      "step": 1920
    },
    {
      "epoch": 0.8210145698181431,
      "grad_norm": 1.3409154415130615,
      "learning_rate": 2.981720430107527e-06,
      "loss": 0.961,
      "step": 1930
    },
    {
      "epoch": 0.8252685313197916,
      "grad_norm": 1.7145460844039917,
      "learning_rate": 2.970967741935484e-06,
      "loss": 0.9595,
      "step": 1940
    },
    {
      "epoch": 0.82952249282144,
      "grad_norm": 1.6322005987167358,
      "learning_rate": 2.9602150537634413e-06,
      "loss": 1.027,
      "step": 1950
    },
    {
      "epoch": 0.8337764543230883,
      "grad_norm": 1.3984640836715698,
      "learning_rate": 2.949462365591398e-06,
      "loss": 0.9773,
      "step": 1960
    },
    {
      "epoch": 0.8380304158247368,
      "grad_norm": 1.3548494577407837,
      "learning_rate": 2.938709677419355e-06,
      "loss": 1.0049,
      "step": 1970
    },
    {
      "epoch": 0.8422843773263852,
      "grad_norm": 1.7006070613861084,
      "learning_rate": 2.927956989247312e-06,
      "loss": 1.0936,
      "step": 1980
    },
    {
      "epoch": 0.8465383388280336,
      "grad_norm": 1.4441378116607666,
      "learning_rate": 2.9172043010752695e-06,
      "loss": 1.2348,
      "step": 1990
    },
    {
      "epoch": 0.850792300329682,
      "grad_norm": 1.4238107204437256,
      "learning_rate": 2.9064516129032262e-06,
      "loss": 0.9653,
      "step": 2000
    },
    {
      "epoch": 0.8550462618313305,
      "grad_norm": 1.7451802492141724,
      "learning_rate": 2.895698924731183e-06,
      "loss": 0.9605,
      "step": 2010
    },
    {
      "epoch": 0.8593002233329788,
      "grad_norm": 1.3955897092819214,
      "learning_rate": 2.88494623655914e-06,
      "loss": 0.8715,
      "step": 2020
    },
    {
      "epoch": 0.8635541848346272,
      "grad_norm": 1.3098186254501343,
      "learning_rate": 2.874193548387097e-06,
      "loss": 1.1211,
      "step": 2030
    },
    {
      "epoch": 0.8678081463362757,
      "grad_norm": 1.3091546297073364,
      "learning_rate": 2.8634408602150536e-06,
      "loss": 1.1125,
      "step": 2040
    },
    {
      "epoch": 0.872062107837924,
      "grad_norm": 1.8629618883132935,
      "learning_rate": 2.852688172043011e-06,
      "loss": 1.0152,
      "step": 2050
    },
    {
      "epoch": 0.8763160693395725,
      "grad_norm": 1.5990424156188965,
      "learning_rate": 2.841935483870968e-06,
      "loss": 0.9808,
      "step": 2060
    },
    {
      "epoch": 0.8805700308412209,
      "grad_norm": 1.4218305349349976,
      "learning_rate": 2.831182795698925e-06,
      "loss": 0.7576,
      "step": 2070
    },
    {
      "epoch": 0.8848239923428693,
      "grad_norm": 2.4650542736053467,
      "learning_rate": 2.8204301075268818e-06,
      "loss": 1.1165,
      "step": 2080
    },
    {
      "epoch": 0.8890779538445177,
      "grad_norm": 1.2598813772201538,
      "learning_rate": 2.809677419354839e-06,
      "loss": 0.8673,
      "step": 2090
    },
    {
      "epoch": 0.8933319153461661,
      "grad_norm": 1.4977080821990967,
      "learning_rate": 2.798924731182796e-06,
      "loss": 0.9467,
      "step": 2100
    },
    {
      "epoch": 0.8975858768478145,
      "grad_norm": 1.3610531091690063,
      "learning_rate": 2.788172043010753e-06,
      "loss": 0.9729,
      "step": 2110
    },
    {
      "epoch": 0.9018398383494629,
      "grad_norm": 1.475723385810852,
      "learning_rate": 2.77741935483871e-06,
      "loss": 1.0759,
      "step": 2120
    },
    {
      "epoch": 0.9060937998511114,
      "grad_norm": 1.4684621095657349,
      "learning_rate": 2.766666666666667e-06,
      "loss": 0.9125,
      "step": 2130
    },
    {
      "epoch": 0.9103477613527597,
      "grad_norm": 1.3061954975128174,
      "learning_rate": 2.755913978494624e-06,
      "loss": 1.0619,
      "step": 2140
    },
    {
      "epoch": 0.9146017228544082,
      "grad_norm": 1.3840562105178833,
      "learning_rate": 2.745161290322581e-06,
      "loss": 1.1807,
      "step": 2150
    },
    {
      "epoch": 0.9188556843560566,
      "grad_norm": 1.3315352201461792,
      "learning_rate": 2.7344086021505378e-06,
      "loss": 1.1382,
      "step": 2160
    },
    {
      "epoch": 0.9231096458577049,
      "grad_norm": 1.9495880603790283,
      "learning_rate": 2.7236559139784945e-06,
      "loss": 1.0378,
      "step": 2170
    },
    {
      "epoch": 0.9273636073593534,
      "grad_norm": 1.5567991733551025,
      "learning_rate": 2.712903225806452e-06,
      "loss": 0.897,
      "step": 2180
    },
    {
      "epoch": 0.9316175688610018,
      "grad_norm": 1.5261642932891846,
      "learning_rate": 2.702150537634409e-06,
      "loss": 0.8401,
      "step": 2190
    },
    {
      "epoch": 0.9358715303626503,
      "grad_norm": 1.7194374799728394,
      "learning_rate": 2.691397849462366e-06,
      "loss": 0.9431,
      "step": 2200
    },
    {
      "epoch": 0.9401254918642986,
      "grad_norm": 1.2632076740264893,
      "learning_rate": 2.6806451612903227e-06,
      "loss": 0.939,
      "step": 2210
    },
    {
      "epoch": 0.9443794533659471,
      "grad_norm": 1.689735770225525,
      "learning_rate": 2.6698924731182794e-06,
      "loss": 1.0419,
      "step": 2220
    },
    {
      "epoch": 0.9486334148675954,
      "grad_norm": 2.0507185459136963,
      "learning_rate": 2.659139784946237e-06,
      "loss": 1.0158,
      "step": 2230
    },
    {
      "epoch": 0.9528873763692438,
      "grad_norm": 1.3411438465118408,
      "learning_rate": 2.6483870967741937e-06,
      "loss": 1.056,
      "step": 2240
    },
    {
      "epoch": 0.9571413378708923,
      "grad_norm": 2.230699300765991,
      "learning_rate": 2.637634408602151e-06,
      "loss": 1.0545,
      "step": 2250
    },
    {
      "epoch": 0.9613952993725406,
      "grad_norm": 1.6356067657470703,
      "learning_rate": 2.6268817204301076e-06,
      "loss": 1.0232,
      "step": 2260
    },
    {
      "epoch": 0.9656492608741891,
      "grad_norm": 1.4711682796478271,
      "learning_rate": 2.6161290322580648e-06,
      "loss": 0.8481,
      "step": 2270
    },
    {
      "epoch": 0.9699032223758375,
      "grad_norm": 1.9793827533721924,
      "learning_rate": 2.605376344086022e-06,
      "loss": 1.0317,
      "step": 2280
    },
    {
      "epoch": 0.974157183877486,
      "grad_norm": 1.4829621315002441,
      "learning_rate": 2.5946236559139787e-06,
      "loss": 1.0587,
      "step": 2290
    },
    {
      "epoch": 0.9784111453791343,
      "grad_norm": 1.3354415893554688,
      "learning_rate": 2.5838709677419354e-06,
      "loss": 0.9249,
      "step": 2300
    },
    {
      "epoch": 0.9826651068807827,
      "grad_norm": 1.393700122833252,
      "learning_rate": 2.573118279569893e-06,
      "loss": 0.8838,
      "step": 2310
    },
    {
      "epoch": 0.9869190683824312,
      "grad_norm": 1.3979153633117676,
      "learning_rate": 2.5623655913978497e-06,
      "loss": 0.9833,
      "step": 2320
    },
    {
      "epoch": 0.9911730298840795,
      "grad_norm": 1.812325119972229,
      "learning_rate": 2.551612903225807e-06,
      "loss": 1.0529,
      "step": 2330
    },
    {
      "epoch": 0.995426991385728,
      "grad_norm": 1.4827791452407837,
      "learning_rate": 2.5408602150537636e-06,
      "loss": 0.9161,
      "step": 2340
    },
    {
      "epoch": 0.9996809528873764,
      "grad_norm": 1.929315447807312,
      "learning_rate": 2.5301075268817203e-06,
      "loss": 0.8634,
      "step": 2350
    },
    {
      "epoch": 1.0039349143890248,
      "grad_norm": 1.7639093399047852,
      "learning_rate": 2.519354838709678e-06,
      "loss": 0.867,
      "step": 2360
    },
    {
      "epoch": 1.0081888758906732,
      "grad_norm": 1.7387815713882446,
      "learning_rate": 2.5086021505376346e-06,
      "loss": 0.9787,
      "step": 2370
    },
    {
      "epoch": 1.0124428373923215,
      "grad_norm": 1.2963173389434814,
      "learning_rate": 2.497849462365592e-06,
      "loss": 0.9679,
      "step": 2380
    },
    {
      "epoch": 1.01669679889397,
      "grad_norm": 1.768119215965271,
      "learning_rate": 2.4870967741935485e-06,
      "loss": 1.0605,
      "step": 2390
    },
    {
      "epoch": 1.0209507603956185,
      "grad_norm": 1.4787356853485107,
      "learning_rate": 2.4763440860215053e-06,
      "loss": 0.9535,
      "step": 2400
    },
    {
      "epoch": 1.0252047218972669,
      "grad_norm": 1.5873894691467285,
      "learning_rate": 2.4655913978494624e-06,
      "loss": 1.0264,
      "step": 2410
    },
    {
      "epoch": 1.0294586833989152,
      "grad_norm": 1.51543390750885,
      "learning_rate": 2.4548387096774196e-06,
      "loss": 1.0157,
      "step": 2420
    },
    {
      "epoch": 1.0337126449005636,
      "grad_norm": 1.2373638153076172,
      "learning_rate": 2.4440860215053767e-06,
      "loss": 1.2083,
      "step": 2430
    },
    {
      "epoch": 1.0379666064022122,
      "grad_norm": 1.2432774305343628,
      "learning_rate": 2.4333333333333335e-06,
      "loss": 0.9262,
      "step": 2440
    },
    {
      "epoch": 1.0422205679038605,
      "grad_norm": 1.7284162044525146,
      "learning_rate": 2.4225806451612906e-06,
      "loss": 0.9518,
      "step": 2450
    },
    {
      "epoch": 1.046474529405509,
      "grad_norm": 1.5221713781356812,
      "learning_rate": 2.4118279569892474e-06,
      "loss": 0.9868,
      "step": 2460
    },
    {
      "epoch": 1.0507284909071573,
      "grad_norm": 1.6017084121704102,
      "learning_rate": 2.4010752688172045e-06,
      "loss": 1.0333,
      "step": 2470
    },
    {
      "epoch": 1.0549824524088056,
      "grad_norm": 2.009554862976074,
      "learning_rate": 2.3903225806451617e-06,
      "loss": 0.8745,
      "step": 2480
    },
    {
      "epoch": 1.0592364139104542,
      "grad_norm": 1.9531253576278687,
      "learning_rate": 2.3795698924731184e-06,
      "loss": 1.0732,
      "step": 2490
    },
    {
      "epoch": 1.0634903754121026,
      "grad_norm": 1.5727653503417969,
      "learning_rate": 2.3688172043010756e-06,
      "loss": 1.0194,
      "step": 2500
    },
    {
      "epoch": 1.067744336913751,
      "grad_norm": 1.8272086381912231,
      "learning_rate": 2.3580645161290323e-06,
      "loss": 0.9831,
      "step": 2510
    },
    {
      "epoch": 1.0719982984153993,
      "grad_norm": 1.1127636432647705,
      "learning_rate": 2.3473118279569894e-06,
      "loss": 0.9315,
      "step": 2520
    },
    {
      "epoch": 1.0762522599170476,
      "grad_norm": 1.3895549774169922,
      "learning_rate": 2.336559139784946e-06,
      "loss": 0.9346,
      "step": 2530
    },
    {
      "epoch": 1.0805062214186962,
      "grad_norm": 1.3525038957595825,
      "learning_rate": 2.3258064516129033e-06,
      "loss": 0.9445,
      "step": 2540
    },
    {
      "epoch": 1.0847601829203446,
      "grad_norm": 1.5993692874908447,
      "learning_rate": 2.3150537634408605e-06,
      "loss": 1.0228,
      "step": 2550
    },
    {
      "epoch": 1.089014144421993,
      "grad_norm": 1.8816953897476196,
      "learning_rate": 2.3043010752688176e-06,
      "loss": 1.0301,
      "step": 2560
    },
    {
      "epoch": 1.0932681059236413,
      "grad_norm": 1.077518343925476,
      "learning_rate": 2.2935483870967744e-06,
      "loss": 0.9025,
      "step": 2570
    },
    {
      "epoch": 1.09752206742529,
      "grad_norm": 1.1753584146499634,
      "learning_rate": 2.282795698924731e-06,
      "loss": 0.9518,
      "step": 2580
    },
    {
      "epoch": 1.1017760289269383,
      "grad_norm": 1.4676547050476074,
      "learning_rate": 2.2720430107526883e-06,
      "loss": 1.0147,
      "step": 2590
    },
    {
      "epoch": 1.1060299904285866,
      "grad_norm": 1.6378138065338135,
      "learning_rate": 2.2612903225806454e-06,
      "loss": 1.13,
      "step": 2600
    },
    {
      "epoch": 1.110283951930235,
      "grad_norm": 2.8353166580200195,
      "learning_rate": 2.2505376344086026e-06,
      "loss": 1.0131,
      "step": 2610
    },
    {
      "epoch": 1.1145379134318834,
      "grad_norm": 1.5205817222595215,
      "learning_rate": 2.2397849462365593e-06,
      "loss": 0.8483,
      "step": 2620
    },
    {
      "epoch": 1.118791874933532,
      "grad_norm": 1.3297830820083618,
      "learning_rate": 2.2290322580645165e-06,
      "loss": 1.055,
      "step": 2630
    },
    {
      "epoch": 1.1230458364351803,
      "grad_norm": 1.1819339990615845,
      "learning_rate": 2.218279569892473e-06,
      "loss": 0.8708,
      "step": 2640
    },
    {
      "epoch": 1.1272997979368287,
      "grad_norm": 1.6349058151245117,
      "learning_rate": 2.2075268817204304e-06,
      "loss": 1.1141,
      "step": 2650
    },
    {
      "epoch": 1.131553759438477,
      "grad_norm": 2.1028411388397217,
      "learning_rate": 2.196774193548387e-06,
      "loss": 0.9941,
      "step": 2660
    },
    {
      "epoch": 1.1358077209401256,
      "grad_norm": 1.3095271587371826,
      "learning_rate": 2.1860215053763442e-06,
      "loss": 0.9363,
      "step": 2670
    },
    {
      "epoch": 1.140061682441774,
      "grad_norm": 1.2454434633255005,
      "learning_rate": 2.1752688172043014e-06,
      "loss": 1.0707,
      "step": 2680
    },
    {
      "epoch": 1.1443156439434223,
      "grad_norm": 1.3827418088912964,
      "learning_rate": 2.164516129032258e-06,
      "loss": 1.0194,
      "step": 2690
    },
    {
      "epoch": 1.1485696054450707,
      "grad_norm": 1.572860836982727,
      "learning_rate": 2.1537634408602153e-06,
      "loss": 1.0555,
      "step": 2700
    },
    {
      "epoch": 1.152823566946719,
      "grad_norm": 1.7651406526565552,
      "learning_rate": 2.143010752688172e-06,
      "loss": 1.0107,
      "step": 2710
    },
    {
      "epoch": 1.1570775284483674,
      "grad_norm": 1.3755277395248413,
      "learning_rate": 2.132258064516129e-06,
      "loss": 1.0512,
      "step": 2720
    },
    {
      "epoch": 1.161331489950016,
      "grad_norm": 1.4633262157440186,
      "learning_rate": 2.1215053763440863e-06,
      "loss": 0.9379,
      "step": 2730
    },
    {
      "epoch": 1.1655854514516644,
      "grad_norm": 1.742668867111206,
      "learning_rate": 2.1107526881720435e-06,
      "loss": 0.9128,
      "step": 2740
    },
    {
      "epoch": 1.1698394129533127,
      "grad_norm": 1.454908013343811,
      "learning_rate": 2.1000000000000002e-06,
      "loss": 1.0298,
      "step": 2750
    },
    {
      "epoch": 1.174093374454961,
      "grad_norm": 1.5276968479156494,
      "learning_rate": 2.089247311827957e-06,
      "loss": 0.886,
      "step": 2760
    },
    {
      "epoch": 1.1783473359566097,
      "grad_norm": 1.2421458959579468,
      "learning_rate": 2.078494623655914e-06,
      "loss": 1.0912,
      "step": 2770
    },
    {
      "epoch": 1.182601297458258,
      "grad_norm": 1.4805397987365723,
      "learning_rate": 2.0677419354838713e-06,
      "loss": 0.9636,
      "step": 2780
    },
    {
      "epoch": 1.1868552589599064,
      "grad_norm": 1.2460007667541504,
      "learning_rate": 2.056989247311828e-06,
      "loss": 1.0479,
      "step": 2790
    },
    {
      "epoch": 1.1911092204615548,
      "grad_norm": 1.3724348545074463,
      "learning_rate": 2.046236559139785e-06,
      "loss": 1.0998,
      "step": 2800
    },
    {
      "epoch": 1.1953631819632031,
      "grad_norm": 1.350792407989502,
      "learning_rate": 2.0354838709677423e-06,
      "loss": 1.0494,
      "step": 2810
    },
    {
      "epoch": 1.1996171434648517,
      "grad_norm": 1.6217832565307617,
      "learning_rate": 2.024731182795699e-06,
      "loss": 0.9456,
      "step": 2820
    },
    {
      "epoch": 1.2038711049665,
      "grad_norm": 1.4696797132492065,
      "learning_rate": 2.0139784946236558e-06,
      "loss": 0.8747,
      "step": 2830
    },
    {
      "epoch": 1.2081250664681484,
      "grad_norm": 1.3171793222427368,
      "learning_rate": 2.003225806451613e-06,
      "loss": 0.8627,
      "step": 2840
    },
    {
      "epoch": 1.2123790279697968,
      "grad_norm": 1.352076530456543,
      "learning_rate": 1.99247311827957e-06,
      "loss": 0.96,
      "step": 2850
    },
    {
      "epoch": 1.2166329894714454,
      "grad_norm": 1.5133265256881714,
      "learning_rate": 1.9817204301075272e-06,
      "loss": 0.8647,
      "step": 2860
    },
    {
      "epoch": 1.2208869509730937,
      "grad_norm": 1.528065800666809,
      "learning_rate": 1.970967741935484e-06,
      "loss": 0.9199,
      "step": 2870
    },
    {
      "epoch": 1.225140912474742,
      "grad_norm": 2.075606107711792,
      "learning_rate": 1.960215053763441e-06,
      "loss": 0.9922,
      "step": 2880
    },
    {
      "epoch": 1.2293948739763905,
      "grad_norm": 1.2221920490264893,
      "learning_rate": 1.949462365591398e-06,
      "loss": 0.8132,
      "step": 2890
    },
    {
      "epoch": 1.2336488354780388,
      "grad_norm": 1.229968786239624,
      "learning_rate": 1.938709677419355e-06,
      "loss": 1.1538,
      "step": 2900
    },
    {
      "epoch": 1.2379027969796874,
      "grad_norm": 1.7169855833053589,
      "learning_rate": 1.927956989247312e-06,
      "loss": 0.9102,
      "step": 2910
    },
    {
      "epoch": 1.2421567584813358,
      "grad_norm": 1.9384640455245972,
      "learning_rate": 1.917204301075269e-06,
      "loss": 0.853,
      "step": 2920
    },
    {
      "epoch": 1.2464107199829841,
      "grad_norm": 1.670237421989441,
      "learning_rate": 1.9064516129032259e-06,
      "loss": 1.1183,
      "step": 2930
    },
    {
      "epoch": 1.2506646814846325,
      "grad_norm": 1.2814937829971313,
      "learning_rate": 1.895698924731183e-06,
      "loss": 1.0521,
      "step": 2940
    },
    {
      "epoch": 1.254918642986281,
      "grad_norm": 1.5265529155731201,
      "learning_rate": 1.88494623655914e-06,
      "loss": 0.9521,
      "step": 2950
    },
    {
      "epoch": 1.2591726044879294,
      "grad_norm": 1.2071293592453003,
      "learning_rate": 1.874193548387097e-06,
      "loss": 0.9007,
      "step": 2960
    },
    {
      "epoch": 1.2634265659895778,
      "grad_norm": 1.5132914781570435,
      "learning_rate": 1.8634408602150538e-06,
      "loss": 0.773,
      "step": 2970
    },
    {
      "epoch": 1.2676805274912262,
      "grad_norm": 1.5825296640396118,
      "learning_rate": 1.852688172043011e-06,
      "loss": 1.0355,
      "step": 2980
    },
    {
      "epoch": 1.2719344889928745,
      "grad_norm": 1.3510682582855225,
      "learning_rate": 1.841935483870968e-06,
      "loss": 0.912,
      "step": 2990
    },
    {
      "epoch": 1.276188450494523,
      "grad_norm": 1.5747524499893188,
      "learning_rate": 1.8311827956989247e-06,
      "loss": 0.9419,
      "step": 3000
    },
    {
      "epoch": 1.2804424119961715,
      "grad_norm": 2.485898733139038,
      "learning_rate": 1.8204301075268818e-06,
      "loss": 0.9703,
      "step": 3010
    },
    {
      "epoch": 1.2846963734978198,
      "grad_norm": 1.4767776727676392,
      "learning_rate": 1.8096774193548388e-06,
      "loss": 0.9917,
      "step": 3020
    },
    {
      "epoch": 1.2889503349994682,
      "grad_norm": 1.480047583580017,
      "learning_rate": 1.798924731182796e-06,
      "loss": 1.0826,
      "step": 3030
    },
    {
      "epoch": 1.2932042965011168,
      "grad_norm": 1.440128207206726,
      "learning_rate": 1.7881720430107527e-06,
      "loss": 0.8922,
      "step": 3040
    },
    {
      "epoch": 1.2974582580027652,
      "grad_norm": 1.8290034532546997,
      "learning_rate": 1.7774193548387098e-06,
      "loss": 1.0348,
      "step": 3050
    },
    {
      "epoch": 1.3017122195044135,
      "grad_norm": 1.5536504983901978,
      "learning_rate": 1.7666666666666668e-06,
      "loss": 0.8755,
      "step": 3060
    },
    {
      "epoch": 1.3059661810060619,
      "grad_norm": 1.4526976346969604,
      "learning_rate": 1.755913978494624e-06,
      "loss": 0.9714,
      "step": 3070
    },
    {
      "epoch": 1.3102201425077102,
      "grad_norm": 2.104363203048706,
      "learning_rate": 1.7451612903225809e-06,
      "loss": 1.0132,
      "step": 3080
    },
    {
      "epoch": 1.3144741040093586,
      "grad_norm": 1.969487190246582,
      "learning_rate": 1.7344086021505376e-06,
      "loss": 1.113,
      "step": 3090
    },
    {
      "epoch": 1.3187280655110072,
      "grad_norm": 2.128305673599243,
      "learning_rate": 1.7236559139784948e-06,
      "loss": 0.901,
      "step": 3100
    },
    {
      "epoch": 1.3229820270126555,
      "grad_norm": 1.5411396026611328,
      "learning_rate": 1.7129032258064517e-06,
      "loss": 1.0525,
      "step": 3110
    },
    {
      "epoch": 1.327235988514304,
      "grad_norm": 1.3895472288131714,
      "learning_rate": 1.7021505376344089e-06,
      "loss": 0.9574,
      "step": 3120
    },
    {
      "epoch": 1.3314899500159523,
      "grad_norm": 1.5398753881454468,
      "learning_rate": 1.6913978494623656e-06,
      "loss": 0.9974,
      "step": 3130
    },
    {
      "epoch": 1.3357439115176009,
      "grad_norm": 1.3336013555526733,
      "learning_rate": 1.6806451612903227e-06,
      "loss": 0.9426,
      "step": 3140
    },
    {
      "epoch": 1.3399978730192492,
      "grad_norm": 1.7378133535385132,
      "learning_rate": 1.6698924731182797e-06,
      "loss": 1.0426,
      "step": 3150
    },
    {
      "epoch": 1.3442518345208976,
      "grad_norm": 2.6329381465911865,
      "learning_rate": 1.6591397849462368e-06,
      "loss": 0.9019,
      "step": 3160
    },
    {
      "epoch": 1.348505796022546,
      "grad_norm": 1.245944857597351,
      "learning_rate": 1.6483870967741936e-06,
      "loss": 1.0647,
      "step": 3170
    },
    {
      "epoch": 1.3527597575241943,
      "grad_norm": 1.9155173301696777,
      "learning_rate": 1.6376344086021505e-06,
      "loss": 0.9803,
      "step": 3180
    },
    {
      "epoch": 1.357013719025843,
      "grad_norm": 1.8562854528427124,
      "learning_rate": 1.6268817204301077e-06,
      "loss": 1.0662,
      "step": 3190
    },
    {
      "epoch": 1.3612676805274913,
      "grad_norm": 1.517432689666748,
      "learning_rate": 1.6161290322580646e-06,
      "loss": 1.0539,
      "step": 3200
    },
    {
      "epoch": 1.3655216420291396,
      "grad_norm": 1.3770544528961182,
      "learning_rate": 1.6053763440860216e-06,
      "loss": 0.8467,
      "step": 3210
    },
    {
      "epoch": 1.369775603530788,
      "grad_norm": 1.5125069618225098,
      "learning_rate": 1.5946236559139785e-06,
      "loss": 1.0663,
      "step": 3220
    },
    {
      "epoch": 1.3740295650324366,
      "grad_norm": 1.890843391418457,
      "learning_rate": 1.5838709677419357e-06,
      "loss": 0.9892,
      "step": 3230
    },
    {
      "epoch": 1.378283526534085,
      "grad_norm": 1.6112499237060547,
      "learning_rate": 1.5731182795698926e-06,
      "loss": 1.0175,
      "step": 3240
    },
    {
      "epoch": 1.3825374880357333,
      "grad_norm": 1.2875782251358032,
      "learning_rate": 1.5623655913978498e-06,
      "loss": 0.9994,
      "step": 3250
    },
    {
      "epoch": 1.3867914495373816,
      "grad_norm": 1.5449961423873901,
      "learning_rate": 1.5516129032258065e-06,
      "loss": 1.0211,
      "step": 3260
    },
    {
      "epoch": 1.39104541103903,
      "grad_norm": 1.5124903917312622,
      "learning_rate": 1.5408602150537634e-06,
      "loss": 0.9162,
      "step": 3270
    },
    {
      "epoch": 1.3952993725406784,
      "grad_norm": 1.3305599689483643,
      "learning_rate": 1.5301075268817206e-06,
      "loss": 0.9204,
      "step": 3280
    },
    {
      "epoch": 1.399553334042327,
      "grad_norm": 1.3443940877914429,
      "learning_rate": 1.5193548387096778e-06,
      "loss": 1.0339,
      "step": 3290
    },
    {
      "epoch": 1.4038072955439753,
      "grad_norm": 2.2430436611175537,
      "learning_rate": 1.5086021505376345e-06,
      "loss": 0.948,
      "step": 3300
    },
    {
      "epoch": 1.4080612570456237,
      "grad_norm": 1.0092350244522095,
      "learning_rate": 1.4978494623655914e-06,
      "loss": 0.9147,
      "step": 3310
    },
    {
      "epoch": 1.4123152185472723,
      "grad_norm": 1.9424515962600708,
      "learning_rate": 1.4870967741935486e-06,
      "loss": 0.8289,
      "step": 3320
    },
    {
      "epoch": 1.4165691800489206,
      "grad_norm": 2.174778938293457,
      "learning_rate": 1.4763440860215055e-06,
      "loss": 1.0807,
      "step": 3330
    },
    {
      "epoch": 1.420823141550569,
      "grad_norm": 1.3213530778884888,
      "learning_rate": 1.4655913978494623e-06,
      "loss": 1.0303,
      "step": 3340
    },
    {
      "epoch": 1.4250771030522174,
      "grad_norm": 1.2284337282180786,
      "learning_rate": 1.4548387096774194e-06,
      "loss": 0.926,
      "step": 3350
    },
    {
      "epoch": 1.4293310645538657,
      "grad_norm": 1.6089284420013428,
      "learning_rate": 1.4440860215053766e-06,
      "loss": 0.9445,
      "step": 3360
    },
    {
      "epoch": 1.433585026055514,
      "grad_norm": 2.0471560955047607,
      "learning_rate": 1.4333333333333335e-06,
      "loss": 1.0319,
      "step": 3370
    },
    {
      "epoch": 1.4378389875571627,
      "grad_norm": 1.3808518648147583,
      "learning_rate": 1.4225806451612907e-06,
      "loss": 1.0712,
      "step": 3380
    },
    {
      "epoch": 1.442092949058811,
      "grad_norm": 1.5297666788101196,
      "learning_rate": 1.4118279569892474e-06,
      "loss": 0.9371,
      "step": 3390
    },
    {
      "epoch": 1.4463469105604594,
      "grad_norm": 1.317947268486023,
      "learning_rate": 1.4010752688172043e-06,
      "loss": 1.0031,
      "step": 3400
    },
    {
      "epoch": 1.450600872062108,
      "grad_norm": 1.818049430847168,
      "learning_rate": 1.3903225806451615e-06,
      "loss": 0.9951,
      "step": 3410
    },
    {
      "epoch": 1.4548548335637563,
      "grad_norm": 1.637516975402832,
      "learning_rate": 1.3795698924731184e-06,
      "loss": 1.08,
      "step": 3420
    },
    {
      "epoch": 1.4591087950654047,
      "grad_norm": 1.4879692792892456,
      "learning_rate": 1.3688172043010752e-06,
      "loss": 0.9192,
      "step": 3430
    },
    {
      "epoch": 1.463362756567053,
      "grad_norm": 1.222428560256958,
      "learning_rate": 1.3580645161290323e-06,
      "loss": 0.9086,
      "step": 3440
    },
    {
      "epoch": 1.4676167180687014,
      "grad_norm": 1.858450174331665,
      "learning_rate": 1.3473118279569895e-06,
      "loss": 1.0103,
      "step": 3450
    },
    {
      "epoch": 1.4718706795703498,
      "grad_norm": 1.2871472835540771,
      "learning_rate": 1.3365591397849464e-06,
      "loss": 0.8315,
      "step": 3460
    },
    {
      "epoch": 1.4761246410719984,
      "grad_norm": 1.3369009494781494,
      "learning_rate": 1.3258064516129032e-06,
      "loss": 1.0848,
      "step": 3470
    },
    {
      "epoch": 1.4803786025736467,
      "grad_norm": 1.2422246932983398,
      "learning_rate": 1.3150537634408603e-06,
      "loss": 1.03,
      "step": 3480
    },
    {
      "epoch": 1.484632564075295,
      "grad_norm": 1.3697466850280762,
      "learning_rate": 1.3043010752688173e-06,
      "loss": 0.9609,
      "step": 3490
    },
    {
      "epoch": 1.4888865255769435,
      "grad_norm": 1.5837078094482422,
      "learning_rate": 1.2935483870967744e-06,
      "loss": 1.098,
      "step": 3500
    },
    {
      "epoch": 1.493140487078592,
      "grad_norm": 1.4388562440872192,
      "learning_rate": 1.2827956989247312e-06,
      "loss": 0.9032,
      "step": 3510
    },
    {
      "epoch": 1.4973944485802404,
      "grad_norm": 2.3480913639068604,
      "learning_rate": 1.2720430107526883e-06,
      "loss": 0.9356,
      "step": 3520
    },
    {
      "epoch": 1.5016484100818888,
      "grad_norm": 1.6978442668914795,
      "learning_rate": 1.2612903225806453e-06,
      "loss": 0.9174,
      "step": 3530
    },
    {
      "epoch": 1.5059023715835371,
      "grad_norm": 1.6847513914108276,
      "learning_rate": 1.2505376344086024e-06,
      "loss": 1.0287,
      "step": 3540
    },
    {
      "epoch": 1.5101563330851855,
      "grad_norm": 3.9599790573120117,
      "learning_rate": 1.2397849462365591e-06,
      "loss": 0.9847,
      "step": 3550
    },
    {
      "epoch": 1.5144102945868338,
      "grad_norm": 1.3385571241378784,
      "learning_rate": 1.2290322580645163e-06,
      "loss": 0.8955,
      "step": 3560
    },
    {
      "epoch": 1.5186642560884824,
      "grad_norm": 1.5268878936767578,
      "learning_rate": 1.2182795698924732e-06,
      "loss": 0.9897,
      "step": 3570
    },
    {
      "epoch": 1.5229182175901308,
      "grad_norm": 1.5116899013519287,
      "learning_rate": 1.2075268817204302e-06,
      "loss": 0.8427,
      "step": 3580
    },
    {
      "epoch": 1.5271721790917794,
      "grad_norm": 1.653231143951416,
      "learning_rate": 1.1967741935483871e-06,
      "loss": 0.9386,
      "step": 3590
    },
    {
      "epoch": 1.5314261405934277,
      "grad_norm": 1.385844349861145,
      "learning_rate": 1.1860215053763443e-06,
      "loss": 1.0768,
      "step": 3600
    },
    {
      "epoch": 1.535680102095076,
      "grad_norm": 1.4572725296020508,
      "learning_rate": 1.1752688172043012e-06,
      "loss": 1.0489,
      "step": 3610
    },
    {
      "epoch": 1.5399340635967245,
      "grad_norm": 1.8257429599761963,
      "learning_rate": 1.1645161290322582e-06,
      "loss": 0.9833,
      "step": 3620
    },
    {
      "epoch": 1.5441880250983728,
      "grad_norm": 1.446702003479004,
      "learning_rate": 1.1537634408602151e-06,
      "loss": 0.8875,
      "step": 3630
    },
    {
      "epoch": 1.5484419866000212,
      "grad_norm": 1.6095061302185059,
      "learning_rate": 1.143010752688172e-06,
      "loss": 1.0811,
      "step": 3640
    },
    {
      "epoch": 1.5526959481016696,
      "grad_norm": 1.4269354343414307,
      "learning_rate": 1.132258064516129e-06,
      "loss": 0.9646,
      "step": 3650
    },
    {
      "epoch": 1.5569499096033181,
      "grad_norm": 1.4039592742919922,
      "learning_rate": 1.1215053763440862e-06,
      "loss": 0.7904,
      "step": 3660
    },
    {
      "epoch": 1.5612038711049665,
      "grad_norm": 1.9542293548583984,
      "learning_rate": 1.1107526881720431e-06,
      "loss": 1.0124,
      "step": 3670
    },
    {
      "epoch": 1.5654578326066149,
      "grad_norm": 1.5330798625946045,
      "learning_rate": 1.1e-06,
      "loss": 1.0604,
      "step": 3680
    },
    {
      "epoch": 1.5697117941082634,
      "grad_norm": 1.8396755456924438,
      "learning_rate": 1.089247311827957e-06,
      "loss": 0.9615,
      "step": 3690
    },
    {
      "epoch": 1.5739657556099118,
      "grad_norm": 1.546714186668396,
      "learning_rate": 1.0784946236559142e-06,
      "loss": 1.0001,
      "step": 3700
    },
    {
      "epoch": 1.5782197171115602,
      "grad_norm": 1.3940509557724,
      "learning_rate": 1.067741935483871e-06,
      "loss": 0.815,
      "step": 3710
    },
    {
      "epoch": 1.5824736786132085,
      "grad_norm": 1.4649646282196045,
      "learning_rate": 1.056989247311828e-06,
      "loss": 0.7939,
      "step": 3720
    },
    {
      "epoch": 1.586727640114857,
      "grad_norm": 1.7261801958084106,
      "learning_rate": 1.046236559139785e-06,
      "loss": 0.8683,
      "step": 3730
    },
    {
      "epoch": 1.5909816016165053,
      "grad_norm": 1.4450191259384155,
      "learning_rate": 1.035483870967742e-06,
      "loss": 1.0337,
      "step": 3740
    },
    {
      "epoch": 1.5952355631181536,
      "grad_norm": 1.44527268409729,
      "learning_rate": 1.024731182795699e-06,
      "loss": 0.9169,
      "step": 3750
    },
    {
      "epoch": 1.5994895246198022,
      "grad_norm": 1.9749517440795898,
      "learning_rate": 1.013978494623656e-06,
      "loss": 1.1549,
      "step": 3760
    },
    {
      "epoch": 1.6037434861214506,
      "grad_norm": 2.0143351554870605,
      "learning_rate": 1.003225806451613e-06,
      "loss": 0.9002,
      "step": 3770
    },
    {
      "epoch": 1.6079974476230992,
      "grad_norm": 1.6688346862792969,
      "learning_rate": 9.9247311827957e-07,
      "loss": 0.9231,
      "step": 3780
    },
    {
      "epoch": 1.6122514091247475,
      "grad_norm": 1.3484151363372803,
      "learning_rate": 9.81720430107527e-07,
      "loss": 0.8379,
      "step": 3790
    },
    {
      "epoch": 1.6165053706263959,
      "grad_norm": 1.279651403427124,
      "learning_rate": 9.709677419354838e-07,
      "loss": 0.8546,
      "step": 3800
    },
    {
      "epoch": 1.6207593321280442,
      "grad_norm": 2.9617676734924316,
      "learning_rate": 9.60215053763441e-07,
      "loss": 0.9634,
      "step": 3810
    },
    {
      "epoch": 1.6250132936296926,
      "grad_norm": 1.9735349416732788,
      "learning_rate": 9.494623655913979e-07,
      "loss": 0.6829,
      "step": 3820
    },
    {
      "epoch": 1.629267255131341,
      "grad_norm": 1.6321265697479248,
      "learning_rate": 9.38709677419355e-07,
      "loss": 1.0754,
      "step": 3830
    },
    {
      "epoch": 1.6335212166329893,
      "grad_norm": 1.490009069442749,
      "learning_rate": 9.279569892473118e-07,
      "loss": 0.9335,
      "step": 3840
    },
    {
      "epoch": 1.637775178134638,
      "grad_norm": 1.4394181966781616,
      "learning_rate": 9.172043010752688e-07,
      "loss": 0.9226,
      "step": 3850
    },
    {
      "epoch": 1.6420291396362863,
      "grad_norm": 1.3812556266784668,
      "learning_rate": 9.064516129032259e-07,
      "loss": 0.8796,
      "step": 3860
    },
    {
      "epoch": 1.6462831011379349,
      "grad_norm": 1.6444172859191895,
      "learning_rate": 8.956989247311828e-07,
      "loss": 1.0282,
      "step": 3870
    },
    {
      "epoch": 1.6505370626395832,
      "grad_norm": 1.8583351373672485,
      "learning_rate": 8.849462365591399e-07,
      "loss": 0.8843,
      "step": 3880
    },
    {
      "epoch": 1.6547910241412316,
      "grad_norm": 1.4209905862808228,
      "learning_rate": 8.741935483870968e-07,
      "loss": 1.1334,
      "step": 3890
    },
    {
      "epoch": 1.65904498564288,
      "grad_norm": 1.7064474821090698,
      "learning_rate": 8.634408602150539e-07,
      "loss": 1.0684,
      "step": 3900
    },
    {
      "epoch": 1.6632989471445283,
      "grad_norm": 2.316007375717163,
      "learning_rate": 8.526881720430108e-07,
      "loss": 0.8706,
      "step": 3910
    },
    {
      "epoch": 1.6675529086461767,
      "grad_norm": 1.4403750896453857,
      "learning_rate": 8.419354838709679e-07,
      "loss": 0.9831,
      "step": 3920
    },
    {
      "epoch": 1.671806870147825,
      "grad_norm": 1.6377184391021729,
      "learning_rate": 8.311827956989247e-07,
      "loss": 0.9541,
      "step": 3930
    },
    {
      "epoch": 1.6760608316494736,
      "grad_norm": 1.5522449016571045,
      "learning_rate": 8.204301075268818e-07,
      "loss": 0.8431,
      "step": 3940
    },
    {
      "epoch": 1.680314793151122,
      "grad_norm": 1.239194393157959,
      "learning_rate": 8.096774193548387e-07,
      "loss": 0.9878,
      "step": 3950
    },
    {
      "epoch": 1.6845687546527703,
      "grad_norm": 1.4331462383270264,
      "learning_rate": 7.989247311827958e-07,
      "loss": 0.9583,
      "step": 3960
    },
    {
      "epoch": 1.688822716154419,
      "grad_norm": 1.743820309638977,
      "learning_rate": 7.881720430107527e-07,
      "loss": 1.0112,
      "step": 3970
    },
    {
      "epoch": 1.6930766776560673,
      "grad_norm": 1.5040870904922485,
      "learning_rate": 7.774193548387098e-07,
      "loss": 0.999,
      "step": 3980
    },
    {
      "epoch": 1.6973306391577156,
      "grad_norm": 1.503712773323059,
      "learning_rate": 7.666666666666667e-07,
      "loss": 0.8912,
      "step": 3990
    },
    {
      "epoch": 1.701584600659364,
      "grad_norm": 1.5874507427215576,
      "learning_rate": 7.559139784946238e-07,
      "loss": 0.9273,
      "step": 4000
    },
    {
      "epoch": 1.7058385621610124,
      "grad_norm": 1.4203625917434692,
      "learning_rate": 7.451612903225806e-07,
      "loss": 1.0657,
      "step": 4010
    },
    {
      "epoch": 1.7100925236626607,
      "grad_norm": 1.284535527229309,
      "learning_rate": 7.344086021505376e-07,
      "loss": 0.9402,
      "step": 4020
    },
    {
      "epoch": 1.714346485164309,
      "grad_norm": 1.5698320865631104,
      "learning_rate": 7.236559139784948e-07,
      "loss": 1.0872,
      "step": 4030
    },
    {
      "epoch": 1.7186004466659577,
      "grad_norm": 1.9154529571533203,
      "learning_rate": 7.129032258064516e-07,
      "loss": 0.927,
      "step": 4040
    },
    {
      "epoch": 1.722854408167606,
      "grad_norm": 1.4619802236557007,
      "learning_rate": 7.021505376344087e-07,
      "loss": 1.0413,
      "step": 4050
    },
    {
      "epoch": 1.7271083696692546,
      "grad_norm": 4.514988422393799,
      "learning_rate": 6.913978494623656e-07,
      "loss": 1.1329,
      "step": 4060
    },
    {
      "epoch": 1.731362331170903,
      "grad_norm": 1.5957149267196655,
      "learning_rate": 6.806451612903227e-07,
      "loss": 0.8808,
      "step": 4070
    },
    {
      "epoch": 1.7356162926725514,
      "grad_norm": 1.2279645204544067,
      "learning_rate": 6.698924731182796e-07,
      "loss": 0.9473,
      "step": 4080
    },
    {
      "epoch": 1.7398702541741997,
      "grad_norm": 1.4051250219345093,
      "learning_rate": 6.591397849462367e-07,
      "loss": 0.9936,
      "step": 4090
    },
    {
      "epoch": 1.744124215675848,
      "grad_norm": 1.6511130332946777,
      "learning_rate": 6.483870967741935e-07,
      "loss": 0.7866,
      "step": 4100
    },
    {
      "epoch": 1.7483781771774964,
      "grad_norm": 1.5754872560501099,
      "learning_rate": 6.376344086021507e-07,
      "loss": 0.8977,
      "step": 4110
    },
    {
      "epoch": 1.7526321386791448,
      "grad_norm": 1.848538875579834,
      "learning_rate": 6.268817204301075e-07,
      "loss": 1.0872,
      "step": 4120
    },
    {
      "epoch": 1.7568861001807934,
      "grad_norm": 1.397221565246582,
      "learning_rate": 6.161290322580646e-07,
      "loss": 0.9632,
      "step": 4130
    },
    {
      "epoch": 1.7611400616824417,
      "grad_norm": 1.4807592630386353,
      "learning_rate": 6.053763440860216e-07,
      "loss": 1.0318,
      "step": 4140
    },
    {
      "epoch": 1.7653940231840903,
      "grad_norm": 2.391531229019165,
      "learning_rate": 5.946236559139786e-07,
      "loss": 0.8878,
      "step": 4150
    },
    {
      "epoch": 1.7696479846857387,
      "grad_norm": 1.5614980459213257,
      "learning_rate": 5.838709677419355e-07,
      "loss": 0.903,
      "step": 4160
    },
    {
      "epoch": 1.773901946187387,
      "grad_norm": 1.4617737531661987,
      "learning_rate": 5.731182795698925e-07,
      "loss": 0.8694,
      "step": 4170
    },
    {
      "epoch": 1.7781559076890354,
      "grad_norm": 2.0481927394866943,
      "learning_rate": 5.623655913978495e-07,
      "loss": 1.078,
      "step": 4180
    },
    {
      "epoch": 1.7824098691906838,
      "grad_norm": 1.5582003593444824,
      "learning_rate": 5.516129032258065e-07,
      "loss": 1.1669,
      "step": 4190
    },
    {
      "epoch": 1.7866638306923321,
      "grad_norm": 1.3596107959747314,
      "learning_rate": 5.408602150537635e-07,
      "loss": 0.9818,
      "step": 4200
    },
    {
      "epoch": 1.7909177921939805,
      "grad_norm": 1.3108245134353638,
      "learning_rate": 5.301075268817204e-07,
      "loss": 0.9572,
      "step": 4210
    },
    {
      "epoch": 1.795171753695629,
      "grad_norm": 1.613224983215332,
      "learning_rate": 5.193548387096775e-07,
      "loss": 0.9638,
      "step": 4220
    },
    {
      "epoch": 1.7994257151972775,
      "grad_norm": 1.460452914237976,
      "learning_rate": 5.086021505376344e-07,
      "loss": 1.0612,
      "step": 4230
    },
    {
      "epoch": 1.803679676698926,
      "grad_norm": 1.5019841194152832,
      "learning_rate": 4.978494623655914e-07,
      "loss": 0.9558,
      "step": 4240
    },
    {
      "epoch": 1.8079336382005744,
      "grad_norm": 1.6622565984725952,
      "learning_rate": 4.870967741935484e-07,
      "loss": 0.8638,
      "step": 4250
    },
    {
      "epoch": 1.8121875997022228,
      "grad_norm": 1.648526906967163,
      "learning_rate": 4.7634408602150547e-07,
      "loss": 0.9276,
      "step": 4260
    },
    {
      "epoch": 1.8164415612038711,
      "grad_norm": 1.2453187704086304,
      "learning_rate": 4.655913978494624e-07,
      "loss": 0.8962,
      "step": 4270
    },
    {
      "epoch": 1.8206955227055195,
      "grad_norm": 1.8173435926437378,
      "learning_rate": 4.548387096774194e-07,
      "loss": 0.8498,
      "step": 4280
    },
    {
      "epoch": 1.8249494842071678,
      "grad_norm": 1.7463124990463257,
      "learning_rate": 4.440860215053764e-07,
      "loss": 0.9967,
      "step": 4290
    },
    {
      "epoch": 1.8292034457088162,
      "grad_norm": 1.5920956134796143,
      "learning_rate": 4.333333333333334e-07,
      "loss": 0.9195,
      "step": 4300
    },
    {
      "epoch": 1.8334574072104648,
      "grad_norm": 1.2254509925842285,
      "learning_rate": 4.2258064516129035e-07,
      "loss": 0.9698,
      "step": 4310
    },
    {
      "epoch": 1.8377113687121132,
      "grad_norm": 1.4899667501449585,
      "learning_rate": 4.1182795698924734e-07,
      "loss": 1.0174,
      "step": 4320
    },
    {
      "epoch": 1.8419653302137615,
      "grad_norm": 1.6167855262756348,
      "learning_rate": 4.0107526881720434e-07,
      "loss": 0.8363,
      "step": 4330
    },
    {
      "epoch": 1.84621929171541,
      "grad_norm": 1.3053642511367798,
      "learning_rate": 3.9032258064516134e-07,
      "loss": 0.8604,
      "step": 4340
    },
    {
      "epoch": 1.8504732532170585,
      "grad_norm": 1.5936143398284912,
      "learning_rate": 3.795698924731183e-07,
      "loss": 0.9108,
      "step": 4350
    },
    {
      "epoch": 1.8547272147187068,
      "grad_norm": 1.6749024391174316,
      "learning_rate": 3.688172043010753e-07,
      "loss": 0.9454,
      "step": 4360
    },
    {
      "epoch": 1.8589811762203552,
      "grad_norm": 1.879622220993042,
      "learning_rate": 3.580645161290323e-07,
      "loss": 1.2031,
      "step": 4370
    },
    {
      "epoch": 1.8632351377220036,
      "grad_norm": 1.2624289989471436,
      "learning_rate": 3.4731182795698927e-07,
      "loss": 1.0686,
      "step": 4380
    },
    {
      "epoch": 1.867489099223652,
      "grad_norm": 1.676245093345642,
      "learning_rate": 3.365591397849462e-07,
      "loss": 0.903,
      "step": 4390
    },
    {
      "epoch": 1.8717430607253003,
      "grad_norm": 1.746654987335205,
      "learning_rate": 3.258064516129032e-07,
      "loss": 0.9342,
      "step": 4400
    },
    {
      "epoch": 1.8759970222269489,
      "grad_norm": 1.7467912435531616,
      "learning_rate": 3.150537634408602e-07,
      "loss": 0.9467,
      "step": 4410
    },
    {
      "epoch": 1.8802509837285972,
      "grad_norm": 1.6267248392105103,
      "learning_rate": 3.043010752688172e-07,
      "loss": 0.9175,
      "step": 4420
    },
    {
      "epoch": 1.8845049452302458,
      "grad_norm": 1.3786072731018066,
      "learning_rate": 2.935483870967742e-07,
      "loss": 0.9901,
      "step": 4430
    },
    {
      "epoch": 1.8887589067318942,
      "grad_norm": 1.4629558324813843,
      "learning_rate": 2.827956989247312e-07,
      "loss": 1.0074,
      "step": 4440
    },
    {
      "epoch": 1.8930128682335425,
      "grad_norm": 1.5857959985733032,
      "learning_rate": 2.720430107526882e-07,
      "loss": 0.9307,
      "step": 4450
    },
    {
      "epoch": 1.897266829735191,
      "grad_norm": 1.4420273303985596,
      "learning_rate": 2.612903225806452e-07,
      "loss": 1.037,
      "step": 4460
    },
    {
      "epoch": 1.9015207912368393,
      "grad_norm": 2.371570348739624,
      "learning_rate": 2.505376344086022e-07,
      "loss": 0.8575,
      "step": 4470
    },
    {
      "epoch": 1.9057747527384876,
      "grad_norm": 1.5911372900009155,
      "learning_rate": 2.397849462365592e-07,
      "loss": 0.8753,
      "step": 4480
    },
    {
      "epoch": 1.910028714240136,
      "grad_norm": 1.3353266716003418,
      "learning_rate": 2.2903225806451616e-07,
      "loss": 1.0157,
      "step": 4490
    },
    {
      "epoch": 1.9142826757417846,
      "grad_norm": 1.3777573108673096,
      "learning_rate": 2.1827956989247313e-07,
      "loss": 0.9214,
      "step": 4500
    },
    {
      "epoch": 1.918536637243433,
      "grad_norm": 1.6701884269714355,
      "learning_rate": 2.0752688172043013e-07,
      "loss": 0.9235,
      "step": 4510
    },
    {
      "epoch": 1.9227905987450815,
      "grad_norm": 1.009879231452942,
      "learning_rate": 1.967741935483871e-07,
      "loss": 0.9677,
      "step": 4520
    },
    {
      "epoch": 1.9270445602467299,
      "grad_norm": 1.2898012399673462,
      "learning_rate": 1.860215053763441e-07,
      "loss": 0.9838,
      "step": 4530
    },
    {
      "epoch": 1.9312985217483782,
      "grad_norm": 1.4380953311920166,
      "learning_rate": 1.752688172043011e-07,
      "loss": 1.1477,
      "step": 4540
    },
    {
      "epoch": 1.9355524832500266,
      "grad_norm": 2.1016170978546143,
      "learning_rate": 1.645161290322581e-07,
      "loss": 0.9186,
      "step": 4550
    },
    {
      "epoch": 1.939806444751675,
      "grad_norm": 1.316989541053772,
      "learning_rate": 1.5376344086021506e-07,
      "loss": 0.947,
      "step": 4560
    },
    {
      "epoch": 1.9440604062533233,
      "grad_norm": 1.4844828844070435,
      "learning_rate": 1.4301075268817206e-07,
      "loss": 0.9479,
      "step": 4570
    },
    {
      "epoch": 1.9483143677549717,
      "grad_norm": 1.3425465822219849,
      "learning_rate": 1.3225806451612903e-07,
      "loss": 0.8742,
      "step": 4580
    },
    {
      "epoch": 1.9525683292566203,
      "grad_norm": 2.859834671020508,
      "learning_rate": 1.2150537634408603e-07,
      "loss": 0.9403,
      "step": 4590
    },
    {
      "epoch": 1.9568222907582686,
      "grad_norm": 1.885156512260437,
      "learning_rate": 1.1075268817204302e-07,
      "loss": 0.9543,
      "step": 4600
    },
    {
      "epoch": 1.961076252259917,
      "grad_norm": 1.297104835510254,
      "learning_rate": 1.0000000000000001e-07,
      "loss": 0.8747,
      "step": 4610
    },
    {
      "epoch": 1.9653302137615656,
      "grad_norm": 1.7872618436813354,
      "learning_rate": 8.924731182795699e-08,
      "loss": 1.0733,
      "step": 4620
    },
    {
      "epoch": 1.969584175263214,
      "grad_norm": 1.3409584760665894,
      "learning_rate": 7.849462365591398e-08,
      "loss": 0.8366,
      "step": 4630
    },
    {
      "epoch": 1.9738381367648623,
      "grad_norm": 1.6619309186935425,
      "learning_rate": 6.774193548387097e-08,
      "loss": 0.8685,
      "step": 4640
    },
    {
      "epoch": 1.9780920982665107,
      "grad_norm": 1.7741172313690186,
      "learning_rate": 5.6989247311827964e-08,
      "loss": 0.925,
      "step": 4650
    },
    {
      "epoch": 1.982346059768159,
      "grad_norm": 1.6325889825820923,
      "learning_rate": 4.623655913978495e-08,
      "loss": 1.0751,
      "step": 4660
    },
    {
      "epoch": 1.9866000212698074,
      "grad_norm": 1.3796652555465698,
      "learning_rate": 3.548387096774194e-08,
      "loss": 0.8262,
      "step": 4670
    },
    {
      "epoch": 1.9908539827714558,
      "grad_norm": 1.0893055200576782,
      "learning_rate": 2.473118279569893e-08,
      "loss": 0.9983,
      "step": 4680
    },
    {
      "epoch": 1.9951079442731043,
      "grad_norm": 1.3851855993270874,
      "learning_rate": 1.3978494623655915e-08,
      "loss": 0.8819,
      "step": 4690
    },
    {
      "epoch": 1.9993619057747527,
      "grad_norm": 1.470017671585083,
      "learning_rate": 3.2258064516129035e-09,
      "loss": 0.9336,
      "step": 4700
    }
  ],
  "logging_steps": 10,
  "max_steps": 4700,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.60466076696576e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
