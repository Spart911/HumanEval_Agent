{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9995747395279608,
  "eval_steps": 500,
  "global_step": 4702,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00425260472039124,
      "grad_norm": 10.935664176940918,
      "learning_rate": 7.000000000000001e-07,
      "loss": 3.319,
      "step": 10
    },
    {
      "epoch": 0.00850520944078248,
      "grad_norm": 0.5094221830368042,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 2.4931,
      "step": 20
    },
    {
      "epoch": 0.01275781416117372,
      "grad_norm": 4.045973777770996,
      "learning_rate": 2.7000000000000004e-06,
      "loss": 3.0841,
      "step": 30
    },
    {
      "epoch": 0.01701041888156496,
      "grad_norm": 7.367554664611816,
      "learning_rate": 3.7e-06,
      "loss": 3.3262,
      "step": 40
    },
    {
      "epoch": 0.0212630236019562,
      "grad_norm": 0.4984150230884552,
      "learning_rate": 4.7e-06,
      "loss": 2.8029,
      "step": 50
    },
    {
      "epoch": 0.02551562832234744,
      "grad_norm": 5.749065399169922,
      "learning_rate": 4.992476354256235e-06,
      "loss": 3.3696,
      "step": 60
    },
    {
      "epoch": 0.02976823304273868,
      "grad_norm": 8.29130744934082,
      "learning_rate": 4.9817282889079975e-06,
      "loss": 3.018,
      "step": 70
    },
    {
      "epoch": 0.03402083776312992,
      "grad_norm": 5.8276753425598145,
      "learning_rate": 4.9709802235597595e-06,
      "loss": 2.9682,
      "step": 80
    },
    {
      "epoch": 0.03827344248352116,
      "grad_norm": 10.29887580871582,
      "learning_rate": 4.960232158211522e-06,
      "loss": 2.6565,
      "step": 90
    },
    {
      "epoch": 0.0425260472039124,
      "grad_norm": 14.05630111694336,
      "learning_rate": 4.949484092863285e-06,
      "loss": 2.3468,
      "step": 100
    },
    {
      "epoch": 0.04677865192430364,
      "grad_norm": 2.8326971530914307,
      "learning_rate": 4.938736027515048e-06,
      "loss": 2.2064,
      "step": 110
    },
    {
      "epoch": 0.05103125664469488,
      "grad_norm": 12.607732772827148,
      "learning_rate": 4.927987962166811e-06,
      "loss": 2.2245,
      "step": 120
    },
    {
      "epoch": 0.05528386136508612,
      "grad_norm": 1.894316554069519,
      "learning_rate": 4.917239896818573e-06,
      "loss": 2.0443,
      "step": 130
    },
    {
      "epoch": 0.05953646608547736,
      "grad_norm": 9.222661972045898,
      "learning_rate": 4.9064918314703354e-06,
      "loss": 2.1819,
      "step": 140
    },
    {
      "epoch": 0.0637890708058686,
      "grad_norm": 2.329866409301758,
      "learning_rate": 4.895743766122098e-06,
      "loss": 1.9769,
      "step": 150
    },
    {
      "epoch": 0.06804167552625984,
      "grad_norm": 0.6256796717643738,
      "learning_rate": 4.884995700773861e-06,
      "loss": 1.9385,
      "step": 160
    },
    {
      "epoch": 0.07229428024665108,
      "grad_norm": NaN,
      "learning_rate": 4.875322441960447e-06,
      "loss": 1.8491,
      "step": 170
    },
    {
      "epoch": 0.07654688496704232,
      "grad_norm": 3.872101068496704,
      "learning_rate": 4.86457437661221e-06,
      "loss": 1.7336,
      "step": 180
    },
    {
      "epoch": 0.08079948968743356,
      "grad_norm": 3.7559359073638916,
      "learning_rate": 4.853826311263973e-06,
      "loss": 1.8349,
      "step": 190
    },
    {
      "epoch": 0.0850520944078248,
      "grad_norm": 2.370147466659546,
      "learning_rate": 4.843078245915736e-06,
      "loss": 1.6888,
      "step": 200
    },
    {
      "epoch": 0.08930469912821604,
      "grad_norm": 2.135072946548462,
      "learning_rate": 4.832330180567499e-06,
      "loss": 1.6061,
      "step": 210
    },
    {
      "epoch": 0.09355730384860728,
      "grad_norm": 5.358029365539551,
      "learning_rate": 4.821582115219261e-06,
      "loss": 1.5687,
      "step": 220
    },
    {
      "epoch": 0.09780990856899852,
      "grad_norm": 0.7889363765716553,
      "learning_rate": 4.810834049871023e-06,
      "loss": 1.6752,
      "step": 230
    },
    {
      "epoch": 0.10206251328938976,
      "grad_norm": 0.8534307479858398,
      "learning_rate": 4.800085984522786e-06,
      "loss": 1.5077,
      "step": 240
    },
    {
      "epoch": 0.106315118009781,
      "grad_norm": 0.8018249869346619,
      "learning_rate": 4.789337919174549e-06,
      "loss": 1.4545,
      "step": 250
    },
    {
      "epoch": 0.11056772273017224,
      "grad_norm": 0.8088017106056213,
      "learning_rate": 4.778589853826312e-06,
      "loss": 1.7697,
      "step": 260
    },
    {
      "epoch": 0.11482032745056348,
      "grad_norm": 0.6829043030738831,
      "learning_rate": 4.767841788478075e-06,
      "loss": 1.6131,
      "step": 270
    },
    {
      "epoch": 0.11907293217095472,
      "grad_norm": 0.8497843742370605,
      "learning_rate": 4.757093723129837e-06,
      "loss": 1.5807,
      "step": 280
    },
    {
      "epoch": 0.12332553689134595,
      "grad_norm": 0.7860615253448486,
      "learning_rate": 4.746345657781599e-06,
      "loss": 1.5344,
      "step": 290
    },
    {
      "epoch": 0.1275781416117372,
      "grad_norm": 0.6368198394775391,
      "learning_rate": 4.735597592433362e-06,
      "loss": 1.4273,
      "step": 300
    },
    {
      "epoch": 0.13183074633212843,
      "grad_norm": 0.7599575519561768,
      "learning_rate": 4.724849527085125e-06,
      "loss": 1.4861,
      "step": 310
    },
    {
      "epoch": 0.13608335105251967,
      "grad_norm": 0.640021800994873,
      "learning_rate": 4.714101461736888e-06,
      "loss": 1.5175,
      "step": 320
    },
    {
      "epoch": 0.14033595577291091,
      "grad_norm": 2.048405170440674,
      "learning_rate": 4.7033533963886506e-06,
      "loss": 1.4257,
      "step": 330
    },
    {
      "epoch": 0.14458856049330215,
      "grad_norm": 0.9897831678390503,
      "learning_rate": 4.692605331040413e-06,
      "loss": 1.4983,
      "step": 340
    },
    {
      "epoch": 0.1488411652136934,
      "grad_norm": 0.862612247467041,
      "learning_rate": 4.681857265692175e-06,
      "loss": 1.4804,
      "step": 350
    },
    {
      "epoch": 0.15309376993408463,
      "grad_norm": 0.8805267214775085,
      "learning_rate": 4.671109200343938e-06,
      "loss": 1.5466,
      "step": 360
    },
    {
      "epoch": 0.15734637465447587,
      "grad_norm": 0.7064469456672668,
      "learning_rate": 4.660361134995701e-06,
      "loss": 1.5098,
      "step": 370
    },
    {
      "epoch": 0.1615989793748671,
      "grad_norm": 0.8829475045204163,
      "learning_rate": 4.649613069647464e-06,
      "loss": 1.5598,
      "step": 380
    },
    {
      "epoch": 0.16585158409525835,
      "grad_norm": 0.7093785405158997,
      "learning_rate": 4.6388650042992265e-06,
      "loss": 1.4146,
      "step": 390
    },
    {
      "epoch": 0.1701041888156496,
      "grad_norm": 0.909599244594574,
      "learning_rate": 4.628116938950989e-06,
      "loss": 1.3958,
      "step": 400
    },
    {
      "epoch": 0.17435679353604083,
      "grad_norm": 0.6975330114364624,
      "learning_rate": 4.617368873602751e-06,
      "loss": 1.4349,
      "step": 410
    },
    {
      "epoch": 0.17860939825643207,
      "grad_norm": 0.7420578002929688,
      "learning_rate": 4.606620808254514e-06,
      "loss": 1.3255,
      "step": 420
    },
    {
      "epoch": 0.1828620029768233,
      "grad_norm": 0.8749691247940063,
      "learning_rate": 4.595872742906277e-06,
      "loss": 1.538,
      "step": 430
    },
    {
      "epoch": 0.18711460769721455,
      "grad_norm": 0.8941437005996704,
      "learning_rate": 4.58512467755804e-06,
      "loss": 1.4664,
      "step": 440
    },
    {
      "epoch": 0.1913672124176058,
      "grad_norm": 0.8185474276542664,
      "learning_rate": 4.5743766122098025e-06,
      "loss": 1.5294,
      "step": 450
    },
    {
      "epoch": 0.19561981713799703,
      "grad_norm": 0.6878137588500977,
      "learning_rate": 4.563628546861565e-06,
      "loss": 1.3179,
      "step": 460
    },
    {
      "epoch": 0.19987242185838827,
      "grad_norm": 0.7472822070121765,
      "learning_rate": 4.552880481513328e-06,
      "loss": 1.3169,
      "step": 470
    },
    {
      "epoch": 0.2041250265787795,
      "grad_norm": 0.9796614050865173,
      "learning_rate": 4.542132416165091e-06,
      "loss": 1.3921,
      "step": 480
    },
    {
      "epoch": 0.20837763129917075,
      "grad_norm": 0.6791533827781677,
      "learning_rate": 4.531384350816853e-06,
      "loss": 1.409,
      "step": 490
    },
    {
      "epoch": 0.212630236019562,
      "grad_norm": 0.6367195844650269,
      "learning_rate": 4.520636285468616e-06,
      "loss": 1.4331,
      "step": 500
    },
    {
      "epoch": 0.21688284073995323,
      "grad_norm": 0.7307227253913879,
      "learning_rate": 4.5098882201203785e-06,
      "loss": 1.3173,
      "step": 510
    },
    {
      "epoch": 0.22113544546034447,
      "grad_norm": 2.6001803874969482,
      "learning_rate": 4.499140154772141e-06,
      "loss": 1.3725,
      "step": 520
    },
    {
      "epoch": 0.2253880501807357,
      "grad_norm": 3.1894989013671875,
      "learning_rate": 4.488392089423904e-06,
      "loss": 1.3545,
      "step": 530
    },
    {
      "epoch": 0.22964065490112695,
      "grad_norm": 0.8871638178825378,
      "learning_rate": 4.477644024075667e-06,
      "loss": 1.3079,
      "step": 540
    },
    {
      "epoch": 0.2338932596215182,
      "grad_norm": 0.6972763538360596,
      "learning_rate": 4.46689595872743e-06,
      "loss": 1.2987,
      "step": 550
    },
    {
      "epoch": 0.23814586434190943,
      "grad_norm": 0.8072187900543213,
      "learning_rate": 4.456147893379192e-06,
      "loss": 1.4353,
      "step": 560
    },
    {
      "epoch": 0.24239846906230067,
      "grad_norm": 0.9507197737693787,
      "learning_rate": 4.4453998280309544e-06,
      "loss": 1.3435,
      "step": 570
    },
    {
      "epoch": 0.2466510737826919,
      "grad_norm": 1.012758493423462,
      "learning_rate": 4.434651762682717e-06,
      "loss": 1.3903,
      "step": 580
    },
    {
      "epoch": 0.25090367850308315,
      "grad_norm": 0.9572283625602722,
      "learning_rate": 4.42390369733448e-06,
      "loss": 1.2928,
      "step": 590
    },
    {
      "epoch": 0.2551562832234744,
      "grad_norm": 1.0698106288909912,
      "learning_rate": 4.413155631986243e-06,
      "loss": 1.3776,
      "step": 600
    },
    {
      "epoch": 0.25940888794386563,
      "grad_norm": 0.9332467913627625,
      "learning_rate": 4.402407566638006e-06,
      "loss": 1.2987,
      "step": 610
    },
    {
      "epoch": 0.26366149266425687,
      "grad_norm": 1.3106271028518677,
      "learning_rate": 4.3916595012897685e-06,
      "loss": 1.322,
      "step": 620
    },
    {
      "epoch": 0.2679140973846481,
      "grad_norm": 0.7541013360023499,
      "learning_rate": 4.380911435941531e-06,
      "loss": 1.5164,
      "step": 630
    },
    {
      "epoch": 0.27216670210503935,
      "grad_norm": 1.126283884048462,
      "learning_rate": 4.370163370593293e-06,
      "loss": 1.2235,
      "step": 640
    },
    {
      "epoch": 0.2764193068254306,
      "grad_norm": 0.7719714045524597,
      "learning_rate": 4.359415305245056e-06,
      "loss": 1.3018,
      "step": 650
    },
    {
      "epoch": 0.28067191154582183,
      "grad_norm": 0.7227901816368103,
      "learning_rate": 4.348667239896819e-06,
      "loss": 1.2512,
      "step": 660
    },
    {
      "epoch": 0.28492451626621307,
      "grad_norm": 0.8107075095176697,
      "learning_rate": 4.337919174548582e-06,
      "loss": 1.3154,
      "step": 670
    },
    {
      "epoch": 0.2891771209866043,
      "grad_norm": 0.7391876578330994,
      "learning_rate": 4.3271711092003444e-06,
      "loss": 1.0725,
      "step": 680
    },
    {
      "epoch": 0.29342972570699555,
      "grad_norm": 1.1905776262283325,
      "learning_rate": 4.316423043852107e-06,
      "loss": 1.217,
      "step": 690
    },
    {
      "epoch": 0.2976823304273868,
      "grad_norm": 0.774326503276825,
      "learning_rate": 4.30567497850387e-06,
      "loss": 1.2348,
      "step": 700
    },
    {
      "epoch": 0.301934935147778,
      "grad_norm": 0.8778861165046692,
      "learning_rate": 4.294926913155633e-06,
      "loss": 1.4374,
      "step": 710
    },
    {
      "epoch": 0.30618753986816927,
      "grad_norm": 1.1766589879989624,
      "learning_rate": 4.284178847807395e-06,
      "loss": 1.2252,
      "step": 720
    },
    {
      "epoch": 0.3104401445885605,
      "grad_norm": 0.7698457837104797,
      "learning_rate": 4.273430782459158e-06,
      "loss": 1.2638,
      "step": 730
    },
    {
      "epoch": 0.31469274930895175,
      "grad_norm": 0.6856777667999268,
      "learning_rate": 4.26268271711092e-06,
      "loss": 1.2001,
      "step": 740
    },
    {
      "epoch": 0.318945354029343,
      "grad_norm": 0.9085096716880798,
      "learning_rate": 4.251934651762683e-06,
      "loss": 1.1681,
      "step": 750
    },
    {
      "epoch": 0.3231979587497342,
      "grad_norm": 0.862260639667511,
      "learning_rate": 4.241186586414446e-06,
      "loss": 1.2973,
      "step": 760
    },
    {
      "epoch": 0.32745056347012547,
      "grad_norm": 0.9858816266059875,
      "learning_rate": 4.230438521066209e-06,
      "loss": 1.182,
      "step": 770
    },
    {
      "epoch": 0.3317031681905167,
      "grad_norm": 0.9219237565994263,
      "learning_rate": 4.219690455717972e-06,
      "loss": 1.1408,
      "step": 780
    },
    {
      "epoch": 0.33595577291090795,
      "grad_norm": 0.918738603591919,
      "learning_rate": 4.208942390369734e-06,
      "loss": 1.2186,
      "step": 790
    },
    {
      "epoch": 0.3402083776312992,
      "grad_norm": 1.1247689723968506,
      "learning_rate": 4.198194325021496e-06,
      "loss": 1.1998,
      "step": 800
    },
    {
      "epoch": 0.3444609823516904,
      "grad_norm": 0.9849114418029785,
      "learning_rate": 4.187446259673259e-06,
      "loss": 1.1805,
      "step": 810
    },
    {
      "epoch": 0.34871358707208167,
      "grad_norm": 0.9924023151397705,
      "learning_rate": 4.176698194325022e-06,
      "loss": 1.1183,
      "step": 820
    },
    {
      "epoch": 0.3529661917924729,
      "grad_norm": 1.1742628812789917,
      "learning_rate": 4.165950128976785e-06,
      "loss": 1.1932,
      "step": 830
    },
    {
      "epoch": 0.35721879651286415,
      "grad_norm": 1.286995530128479,
      "learning_rate": 4.155202063628548e-06,
      "loss": 1.3072,
      "step": 840
    },
    {
      "epoch": 0.3614714012332554,
      "grad_norm": 1.5858261585235596,
      "learning_rate": 4.14445399828031e-06,
      "loss": 1.1712,
      "step": 850
    },
    {
      "epoch": 0.3657240059536466,
      "grad_norm": 1.1104800701141357,
      "learning_rate": 4.133705932932073e-06,
      "loss": 1.1383,
      "step": 860
    },
    {
      "epoch": 0.36997661067403786,
      "grad_norm": 1.0914952754974365,
      "learning_rate": 4.122957867583835e-06,
      "loss": 1.2141,
      "step": 870
    },
    {
      "epoch": 0.3742292153944291,
      "grad_norm": 0.8506404757499695,
      "learning_rate": 4.112209802235598e-06,
      "loss": 1.1916,
      "step": 880
    },
    {
      "epoch": 0.37848182011482034,
      "grad_norm": 1.1545445919036865,
      "learning_rate": 4.101461736887361e-06,
      "loss": 1.2666,
      "step": 890
    },
    {
      "epoch": 0.3827344248352116,
      "grad_norm": 1.1048473119735718,
      "learning_rate": 4.0907136715391236e-06,
      "loss": 1.175,
      "step": 900
    },
    {
      "epoch": 0.3869870295556028,
      "grad_norm": 1.2625629901885986,
      "learning_rate": 4.079965606190886e-06,
      "loss": 1.2054,
      "step": 910
    },
    {
      "epoch": 0.39123963427599406,
      "grad_norm": 1.0493745803833008,
      "learning_rate": 4.069217540842649e-06,
      "loss": 1.1413,
      "step": 920
    },
    {
      "epoch": 0.3954922389963853,
      "grad_norm": 1.2549998760223389,
      "learning_rate": 4.058469475494411e-06,
      "loss": 1.1965,
      "step": 930
    },
    {
      "epoch": 0.39974484371677654,
      "grad_norm": 1.2769759893417358,
      "learning_rate": 4.047721410146174e-06,
      "loss": 1.2407,
      "step": 940
    },
    {
      "epoch": 0.4039974484371678,
      "grad_norm": 1.0886331796646118,
      "learning_rate": 4.036973344797937e-06,
      "loss": 1.1617,
      "step": 950
    },
    {
      "epoch": 0.408250053157559,
      "grad_norm": 0.9863144159317017,
      "learning_rate": 4.0262252794496995e-06,
      "loss": 1.0935,
      "step": 960
    },
    {
      "epoch": 0.41250265787795026,
      "grad_norm": 1.15212082862854,
      "learning_rate": 4.015477214101462e-06,
      "loss": 1.229,
      "step": 970
    },
    {
      "epoch": 0.4167552625983415,
      "grad_norm": 0.9044834971427917,
      "learning_rate": 4.004729148753225e-06,
      "loss": 1.0566,
      "step": 980
    },
    {
      "epoch": 0.42100786731873274,
      "grad_norm": 1.1841620206832886,
      "learning_rate": 3.993981083404987e-06,
      "loss": 1.1988,
      "step": 990
    },
    {
      "epoch": 0.425260472039124,
      "grad_norm": 1.1231210231781006,
      "learning_rate": 3.98323301805675e-06,
      "loss": 1.133,
      "step": 1000
    },
    {
      "epoch": 0.4295130767595152,
      "grad_norm": 1.0983047485351562,
      "learning_rate": 3.972484952708513e-06,
      "loss": 1.0204,
      "step": 1010
    },
    {
      "epoch": 0.43376568147990646,
      "grad_norm": 1.4868513345718384,
      "learning_rate": 3.9617368873602755e-06,
      "loss": 1.118,
      "step": 1020
    },
    {
      "epoch": 0.4380182862002977,
      "grad_norm": 1.376967191696167,
      "learning_rate": 3.950988822012038e-06,
      "loss": 1.1895,
      "step": 1030
    },
    {
      "epoch": 0.44227089092068894,
      "grad_norm": 1.1548011302947998,
      "learning_rate": 3.9402407566638e-06,
      "loss": 0.9711,
      "step": 1040
    },
    {
      "epoch": 0.4465234956410802,
      "grad_norm": 1.2181812524795532,
      "learning_rate": 3.929492691315563e-06,
      "loss": 1.186,
      "step": 1050
    },
    {
      "epoch": 0.4507761003614714,
      "grad_norm": 1.3569782972335815,
      "learning_rate": 3.918744625967326e-06,
      "loss": 1.0326,
      "step": 1060
    },
    {
      "epoch": 0.45502870508186266,
      "grad_norm": 1.1597418785095215,
      "learning_rate": 3.907996560619089e-06,
      "loss": 1.1714,
      "step": 1070
    },
    {
      "epoch": 0.4592813098022539,
      "grad_norm": 0.998166024684906,
      "learning_rate": 3.8972484952708515e-06,
      "loss": 1.1311,
      "step": 1080
    },
    {
      "epoch": 0.46353391452264514,
      "grad_norm": 1.1213724613189697,
      "learning_rate": 3.886500429922614e-06,
      "loss": 1.2646,
      "step": 1090
    },
    {
      "epoch": 0.4677865192430364,
      "grad_norm": 0.9987063407897949,
      "learning_rate": 3.875752364574377e-06,
      "loss": 1.0174,
      "step": 1100
    },
    {
      "epoch": 0.4720391239634276,
      "grad_norm": 1.2905312776565552,
      "learning_rate": 3.865004299226139e-06,
      "loss": 1.0744,
      "step": 1110
    },
    {
      "epoch": 0.47629172868381886,
      "grad_norm": 1.1500649452209473,
      "learning_rate": 3.854256233877902e-06,
      "loss": 1.0997,
      "step": 1120
    },
    {
      "epoch": 0.4805443334042101,
      "grad_norm": 1.2470557689666748,
      "learning_rate": 3.843508168529665e-06,
      "loss": 1.1118,
      "step": 1130
    },
    {
      "epoch": 0.48479693812460134,
      "grad_norm": 1.1588712930679321,
      "learning_rate": 3.8327601031814274e-06,
      "loss": 1.2088,
      "step": 1140
    },
    {
      "epoch": 0.4890495428449926,
      "grad_norm": 1.2042653560638428,
      "learning_rate": 3.82201203783319e-06,
      "loss": 1.1413,
      "step": 1150
    },
    {
      "epoch": 0.4933021475653838,
      "grad_norm": 1.1954621076583862,
      "learning_rate": 3.811263972484953e-06,
      "loss": 1.1802,
      "step": 1160
    },
    {
      "epoch": 0.49755475228577506,
      "grad_norm": 1.1315772533416748,
      "learning_rate": 3.800515907136716e-06,
      "loss": 1.1083,
      "step": 1170
    },
    {
      "epoch": 0.5018073570061663,
      "grad_norm": 1.2321686744689941,
      "learning_rate": 3.7897678417884782e-06,
      "loss": 1.1043,
      "step": 1180
    },
    {
      "epoch": 0.5060599617265575,
      "grad_norm": 1.0069172382354736,
      "learning_rate": 3.779019776440241e-06,
      "loss": 0.9978,
      "step": 1190
    },
    {
      "epoch": 0.5103125664469488,
      "grad_norm": 1.0874183177947998,
      "learning_rate": 3.7682717110920034e-06,
      "loss": 1.1565,
      "step": 1200
    },
    {
      "epoch": 0.51456517116734,
      "grad_norm": 1.3030509948730469,
      "learning_rate": 3.7575236457437662e-06,
      "loss": 1.0618,
      "step": 1210
    },
    {
      "epoch": 0.5188177758877313,
      "grad_norm": 1.1176122426986694,
      "learning_rate": 3.746775580395529e-06,
      "loss": 1.1717,
      "step": 1220
    },
    {
      "epoch": 0.5230703806081225,
      "grad_norm": 1.2865931987762451,
      "learning_rate": 3.736027515047292e-06,
      "loss": 1.0653,
      "step": 1230
    },
    {
      "epoch": 0.5273229853285137,
      "grad_norm": 1.2509639263153076,
      "learning_rate": 3.7252794496990546e-06,
      "loss": 1.1036,
      "step": 1240
    },
    {
      "epoch": 0.531575590048905,
      "grad_norm": 1.306057333946228,
      "learning_rate": 3.7145313843508174e-06,
      "loss": 1.0058,
      "step": 1250
    },
    {
      "epoch": 0.5358281947692962,
      "grad_norm": 1.4519553184509277,
      "learning_rate": 3.7037833190025794e-06,
      "loss": 1.0768,
      "step": 1260
    },
    {
      "epoch": 0.5400807994896875,
      "grad_norm": 1.338598370552063,
      "learning_rate": 3.693035253654342e-06,
      "loss": 1.1322,
      "step": 1270
    },
    {
      "epoch": 0.5443334042100787,
      "grad_norm": 1.127876877784729,
      "learning_rate": 3.682287188306105e-06,
      "loss": 1.1136,
      "step": 1280
    },
    {
      "epoch": 0.5485860089304699,
      "grad_norm": 1.2535028457641602,
      "learning_rate": 3.671539122957868e-06,
      "loss": 1.1605,
      "step": 1290
    },
    {
      "epoch": 0.5528386136508612,
      "grad_norm": 1.072575569152832,
      "learning_rate": 3.6607910576096306e-06,
      "loss": 1.0928,
      "step": 1300
    },
    {
      "epoch": 0.5570912183712524,
      "grad_norm": 1.363542914390564,
      "learning_rate": 3.6500429922613934e-06,
      "loss": 1.0566,
      "step": 1310
    },
    {
      "epoch": 0.5613438230916437,
      "grad_norm": 1.618896484375,
      "learning_rate": 3.639294926913156e-06,
      "loss": 1.1276,
      "step": 1320
    },
    {
      "epoch": 0.5655964278120349,
      "grad_norm": 1.1560903787612915,
      "learning_rate": 3.628546861564919e-06,
      "loss": 1.3138,
      "step": 1330
    },
    {
      "epoch": 0.5698490325324261,
      "grad_norm": 1.708160400390625,
      "learning_rate": 3.617798796216681e-06,
      "loss": 1.0733,
      "step": 1340
    },
    {
      "epoch": 0.5741016372528174,
      "grad_norm": 1.2055182456970215,
      "learning_rate": 3.6070507308684438e-06,
      "loss": 1.0849,
      "step": 1350
    },
    {
      "epoch": 0.5783542419732086,
      "grad_norm": 1.2406258583068848,
      "learning_rate": 3.5963026655202066e-06,
      "loss": 1.1367,
      "step": 1360
    },
    {
      "epoch": 0.5826068466935999,
      "grad_norm": 1.1660488843917847,
      "learning_rate": 3.5855546001719694e-06,
      "loss": 0.9466,
      "step": 1370
    },
    {
      "epoch": 0.5868594514139911,
      "grad_norm": 1.211012601852417,
      "learning_rate": 3.574806534823732e-06,
      "loss": 0.9068,
      "step": 1380
    },
    {
      "epoch": 0.5911120561343823,
      "grad_norm": 1.110428810119629,
      "learning_rate": 3.564058469475495e-06,
      "loss": 0.9091,
      "step": 1390
    },
    {
      "epoch": 0.5953646608547736,
      "grad_norm": 1.2928519248962402,
      "learning_rate": 3.5533104041272578e-06,
      "loss": 1.1066,
      "step": 1400
    },
    {
      "epoch": 0.5996172655751648,
      "grad_norm": 1.293755292892456,
      "learning_rate": 3.54256233877902e-06,
      "loss": 1.2169,
      "step": 1410
    },
    {
      "epoch": 0.603869870295556,
      "grad_norm": 1.1235984563827515,
      "learning_rate": 3.5318142734307825e-06,
      "loss": 0.9172,
      "step": 1420
    },
    {
      "epoch": 0.6081224750159473,
      "grad_norm": 1.2841589450836182,
      "learning_rate": 3.5210662080825453e-06,
      "loss": 0.9709,
      "step": 1430
    },
    {
      "epoch": 0.6123750797363385,
      "grad_norm": 1.229321002960205,
      "learning_rate": 3.510318142734308e-06,
      "loss": 1.0321,
      "step": 1440
    },
    {
      "epoch": 0.6166276844567298,
      "grad_norm": 1.1407840251922607,
      "learning_rate": 3.499570077386071e-06,
      "loss": 1.1582,
      "step": 1450
    },
    {
      "epoch": 0.620880289177121,
      "grad_norm": 1.2141019105911255,
      "learning_rate": 3.4888220120378338e-06,
      "loss": 1.0717,
      "step": 1460
    },
    {
      "epoch": 0.6251328938975123,
      "grad_norm": 1.2787566184997559,
      "learning_rate": 3.478073946689596e-06,
      "loss": 0.9562,
      "step": 1470
    },
    {
      "epoch": 0.6293854986179035,
      "grad_norm": 1.1850100755691528,
      "learning_rate": 3.467325881341359e-06,
      "loss": 1.1126,
      "step": 1480
    },
    {
      "epoch": 0.6336381033382947,
      "grad_norm": 1.32468581199646,
      "learning_rate": 3.4565778159931213e-06,
      "loss": 0.9498,
      "step": 1490
    },
    {
      "epoch": 0.637890708058686,
      "grad_norm": 1.278361201286316,
      "learning_rate": 3.445829750644884e-06,
      "loss": 1.1064,
      "step": 1500
    },
    {
      "epoch": 0.6421433127790772,
      "grad_norm": 1.295894980430603,
      "learning_rate": 3.435081685296647e-06,
      "loss": 1.0164,
      "step": 1510
    },
    {
      "epoch": 0.6463959174994685,
      "grad_norm": 1.2760825157165527,
      "learning_rate": 3.4243336199484097e-06,
      "loss": 0.9381,
      "step": 1520
    },
    {
      "epoch": 0.6506485222198597,
      "grad_norm": 1.420450210571289,
      "learning_rate": 3.413585554600172e-06,
      "loss": 1.0938,
      "step": 1530
    },
    {
      "epoch": 0.6549011269402509,
      "grad_norm": 1.2370495796203613,
      "learning_rate": 3.402837489251935e-06,
      "loss": 1.0347,
      "step": 1540
    },
    {
      "epoch": 0.6591537316606422,
      "grad_norm": 1.439985752105713,
      "learning_rate": 3.3920894239036977e-06,
      "loss": 1.0584,
      "step": 1550
    },
    {
      "epoch": 0.6634063363810334,
      "grad_norm": 1.458465337753296,
      "learning_rate": 3.3813413585554605e-06,
      "loss": 1.0208,
      "step": 1560
    },
    {
      "epoch": 0.6676589411014247,
      "grad_norm": 1.4996007680892944,
      "learning_rate": 3.370593293207223e-06,
      "loss": 1.0982,
      "step": 1570
    },
    {
      "epoch": 0.6719115458218159,
      "grad_norm": 1.448103666305542,
      "learning_rate": 3.3598452278589853e-06,
      "loss": 1.0174,
      "step": 1580
    },
    {
      "epoch": 0.6761641505422071,
      "grad_norm": 1.392728328704834,
      "learning_rate": 3.349097162510748e-06,
      "loss": 1.0557,
      "step": 1590
    },
    {
      "epoch": 0.6804167552625984,
      "grad_norm": 1.4808363914489746,
      "learning_rate": 3.338349097162511e-06,
      "loss": 1.0971,
      "step": 1600
    },
    {
      "epoch": 0.6846693599829896,
      "grad_norm": 1.3910554647445679,
      "learning_rate": 3.3276010318142737e-06,
      "loss": 0.9889,
      "step": 1610
    },
    {
      "epoch": 0.6889219647033809,
      "grad_norm": 1.3705012798309326,
      "learning_rate": 3.3168529664660365e-06,
      "loss": 1.0785,
      "step": 1620
    },
    {
      "epoch": 0.6931745694237721,
      "grad_norm": 1.5404149293899536,
      "learning_rate": 3.3061049011177993e-06,
      "loss": 1.1344,
      "step": 1630
    },
    {
      "epoch": 0.6974271741441633,
      "grad_norm": 1.4847538471221924,
      "learning_rate": 3.295356835769562e-06,
      "loss": 1.1678,
      "step": 1640
    },
    {
      "epoch": 0.7016797788645546,
      "grad_norm": 1.1512510776519775,
      "learning_rate": 3.284608770421324e-06,
      "loss": 1.1455,
      "step": 1650
    },
    {
      "epoch": 0.7059323835849458,
      "grad_norm": 1.6020052433013916,
      "learning_rate": 3.273860705073087e-06,
      "loss": 0.9322,
      "step": 1660
    },
    {
      "epoch": 0.710184988305337,
      "grad_norm": 1.7328524589538574,
      "learning_rate": 3.2631126397248497e-06,
      "loss": 1.1208,
      "step": 1670
    },
    {
      "epoch": 0.7144375930257283,
      "grad_norm": 1.3465770483016968,
      "learning_rate": 3.2523645743766125e-06,
      "loss": 1.0499,
      "step": 1680
    },
    {
      "epoch": 0.7186901977461195,
      "grad_norm": 1.7242302894592285,
      "learning_rate": 3.2416165090283753e-06,
      "loss": 1.0396,
      "step": 1690
    },
    {
      "epoch": 0.7229428024665108,
      "grad_norm": 1.3057379722595215,
      "learning_rate": 3.230868443680138e-06,
      "loss": 0.8891,
      "step": 1700
    },
    {
      "epoch": 0.727195407186902,
      "grad_norm": 1.4477564096450806,
      "learning_rate": 3.220120378331901e-06,
      "loss": 1.0831,
      "step": 1710
    },
    {
      "epoch": 0.7314480119072932,
      "grad_norm": 1.6252635717391968,
      "learning_rate": 3.2093723129836637e-06,
      "loss": 0.9646,
      "step": 1720
    },
    {
      "epoch": 0.7357006166276845,
      "grad_norm": 1.5366806983947754,
      "learning_rate": 3.1986242476354256e-06,
      "loss": 1.04,
      "step": 1730
    },
    {
      "epoch": 0.7399532213480757,
      "grad_norm": 1.3987008333206177,
      "learning_rate": 3.1878761822871884e-06,
      "loss": 1.0152,
      "step": 1740
    },
    {
      "epoch": 0.744205826068467,
      "grad_norm": 1.7320339679718018,
      "learning_rate": 3.1771281169389512e-06,
      "loss": 1.0405,
      "step": 1750
    },
    {
      "epoch": 0.7484584307888582,
      "grad_norm": 1.3771858215332031,
      "learning_rate": 3.166380051590714e-06,
      "loss": 1.0493,
      "step": 1760
    },
    {
      "epoch": 0.7527110355092494,
      "grad_norm": 1.1644487380981445,
      "learning_rate": 3.155631986242477e-06,
      "loss": 1.0211,
      "step": 1770
    },
    {
      "epoch": 0.7569636402296407,
      "grad_norm": 1.4271421432495117,
      "learning_rate": 3.1448839208942396e-06,
      "loss": 1.091,
      "step": 1780
    },
    {
      "epoch": 0.7612162449500319,
      "grad_norm": 1.303061842918396,
      "learning_rate": 3.134135855546002e-06,
      "loss": 1.0552,
      "step": 1790
    },
    {
      "epoch": 0.7654688496704232,
      "grad_norm": 1.1944206953048706,
      "learning_rate": 3.1233877901977644e-06,
      "loss": 1.1866,
      "step": 1800
    },
    {
      "epoch": 0.7697214543908144,
      "grad_norm": 1.5356836318969727,
      "learning_rate": 3.112639724849527e-06,
      "loss": 0.9459,
      "step": 1810
    },
    {
      "epoch": 0.7739740591112056,
      "grad_norm": 1.205703616142273,
      "learning_rate": 3.10189165950129e-06,
      "loss": 1.0124,
      "step": 1820
    },
    {
      "epoch": 0.7782266638315969,
      "grad_norm": 1.835296392440796,
      "learning_rate": 3.091143594153053e-06,
      "loss": 0.9859,
      "step": 1830
    },
    {
      "epoch": 0.7824792685519881,
      "grad_norm": 1.2092314958572388,
      "learning_rate": 3.0803955288048156e-06,
      "loss": 1.0473,
      "step": 1840
    },
    {
      "epoch": 0.7867318732723794,
      "grad_norm": 1.6154555082321167,
      "learning_rate": 3.069647463456578e-06,
      "loss": 1.0021,
      "step": 1850
    },
    {
      "epoch": 0.7909844779927706,
      "grad_norm": 2.1049909591674805,
      "learning_rate": 3.058899398108341e-06,
      "loss": 0.9967,
      "step": 1860
    },
    {
      "epoch": 0.7952370827131618,
      "grad_norm": 1.2805968523025513,
      "learning_rate": 3.0481513327601036e-06,
      "loss": 0.9742,
      "step": 1870
    },
    {
      "epoch": 0.7994896874335531,
      "grad_norm": 1.1567225456237793,
      "learning_rate": 3.037403267411866e-06,
      "loss": 1.002,
      "step": 1880
    },
    {
      "epoch": 0.8037422921539443,
      "grad_norm": 1.4333062171936035,
      "learning_rate": 3.0266552020636288e-06,
      "loss": 1.0878,
      "step": 1890
    },
    {
      "epoch": 0.8079948968743356,
      "grad_norm": 1.1671009063720703,
      "learning_rate": 3.015907136715391e-06,
      "loss": 1.1275,
      "step": 1900
    },
    {
      "epoch": 0.8122475015947268,
      "grad_norm": 1.458569884300232,
      "learning_rate": 3.005159071367154e-06,
      "loss": 1.0045,
      "step": 1910
    },
    {
      "epoch": 0.816500106315118,
      "grad_norm": 1.5267349481582642,
      "learning_rate": 2.9944110060189168e-06,
      "loss": 1.1274,
      "step": 1920
    },
    {
      "epoch": 0.8207527110355093,
      "grad_norm": 1.7844315767288208,
      "learning_rate": 2.9836629406706796e-06,
      "loss": 1.091,
      "step": 1930
    },
    {
      "epoch": 0.8250053157559005,
      "grad_norm": 1.991843819618225,
      "learning_rate": 2.9729148753224424e-06,
      "loss": 1.0683,
      "step": 1940
    },
    {
      "epoch": 0.8292579204762918,
      "grad_norm": 1.1731414794921875,
      "learning_rate": 2.962166809974205e-06,
      "loss": 1.0914,
      "step": 1950
    },
    {
      "epoch": 0.833510525196683,
      "grad_norm": 1.9924342632293701,
      "learning_rate": 2.951418744625967e-06,
      "loss": 0.9321,
      "step": 1960
    },
    {
      "epoch": 0.8377631299170742,
      "grad_norm": 1.5405653715133667,
      "learning_rate": 2.94067067927773e-06,
      "loss": 0.974,
      "step": 1970
    },
    {
      "epoch": 0.8420157346374655,
      "grad_norm": 1.966017246246338,
      "learning_rate": 2.9299226139294927e-06,
      "loss": 0.9658,
      "step": 1980
    },
    {
      "epoch": 0.8462683393578567,
      "grad_norm": 1.1213072538375854,
      "learning_rate": 2.9191745485812555e-06,
      "loss": 0.9887,
      "step": 1990
    },
    {
      "epoch": 0.850520944078248,
      "grad_norm": 1.5959392786026,
      "learning_rate": 2.9084264832330183e-06,
      "loss": 1.0199,
      "step": 2000
    },
    {
      "epoch": 0.8547735487986392,
      "grad_norm": 1.9439414739608765,
      "learning_rate": 2.897678417884781e-06,
      "loss": 0.899,
      "step": 2010
    },
    {
      "epoch": 0.8590261535190304,
      "grad_norm": 1.18765127658844,
      "learning_rate": 2.886930352536544e-06,
      "loss": 1.0474,
      "step": 2020
    },
    {
      "epoch": 0.8632787582394217,
      "grad_norm": 1.6145261526107788,
      "learning_rate": 2.8761822871883068e-06,
      "loss": 0.9589,
      "step": 2030
    },
    {
      "epoch": 0.8675313629598129,
      "grad_norm": 1.5840023756027222,
      "learning_rate": 2.8654342218400687e-06,
      "loss": 0.982,
      "step": 2040
    },
    {
      "epoch": 0.8717839676802042,
      "grad_norm": 2.717893123626709,
      "learning_rate": 2.8546861564918315e-06,
      "loss": 1.0722,
      "step": 2050
    },
    {
      "epoch": 0.8760365724005954,
      "grad_norm": 2.0425798892974854,
      "learning_rate": 2.8439380911435943e-06,
      "loss": 0.9147,
      "step": 2060
    },
    {
      "epoch": 0.8802891771209866,
      "grad_norm": 2.264101505279541,
      "learning_rate": 2.833190025795357e-06,
      "loss": 0.9485,
      "step": 2070
    },
    {
      "epoch": 0.8845417818413779,
      "grad_norm": 1.3722807168960571,
      "learning_rate": 2.82244196044712e-06,
      "loss": 1.0305,
      "step": 2080
    },
    {
      "epoch": 0.8887943865617691,
      "grad_norm": 1.402430772781372,
      "learning_rate": 2.8116938950988827e-06,
      "loss": 0.8707,
      "step": 2090
    },
    {
      "epoch": 0.8930469912821604,
      "grad_norm": 1.2915900945663452,
      "learning_rate": 2.8009458297506455e-06,
      "loss": 1.0224,
      "step": 2100
    },
    {
      "epoch": 0.8972995960025516,
      "grad_norm": 2.9852347373962402,
      "learning_rate": 2.7901977644024075e-06,
      "loss": 0.9306,
      "step": 2110
    },
    {
      "epoch": 0.9015522007229428,
      "grad_norm": 1.3256852626800537,
      "learning_rate": 2.7794496990541703e-06,
      "loss": 1.1582,
      "step": 2120
    },
    {
      "epoch": 0.9058048054433341,
      "grad_norm": 1.3966871500015259,
      "learning_rate": 2.768701633705933e-06,
      "loss": 1.0642,
      "step": 2130
    },
    {
      "epoch": 0.9100574101637253,
      "grad_norm": 1.4233269691467285,
      "learning_rate": 2.757953568357696e-06,
      "loss": 0.9122,
      "step": 2140
    },
    {
      "epoch": 0.9143100148841166,
      "grad_norm": 1.360426664352417,
      "learning_rate": 2.7472055030094587e-06,
      "loss": 0.9796,
      "step": 2150
    },
    {
      "epoch": 0.9185626196045078,
      "grad_norm": 1.2996522188186646,
      "learning_rate": 2.7364574376612215e-06,
      "loss": 1.1237,
      "step": 2160
    },
    {
      "epoch": 0.922815224324899,
      "grad_norm": 1.7336751222610474,
      "learning_rate": 2.725709372312984e-06,
      "loss": 1.1127,
      "step": 2170
    },
    {
      "epoch": 0.9270678290452903,
      "grad_norm": 1.449715495109558,
      "learning_rate": 2.7149613069647467e-06,
      "loss": 0.9417,
      "step": 2180
    },
    {
      "epoch": 0.9313204337656815,
      "grad_norm": 1.4904404878616333,
      "learning_rate": 2.704213241616509e-06,
      "loss": 1.0247,
      "step": 2190
    },
    {
      "epoch": 0.9355730384860728,
      "grad_norm": 1.2407495975494385,
      "learning_rate": 2.693465176268272e-06,
      "loss": 0.9928,
      "step": 2200
    },
    {
      "epoch": 0.939825643206464,
      "grad_norm": 1.3020522594451904,
      "learning_rate": 2.6827171109200347e-06,
      "loss": 1.0267,
      "step": 2210
    },
    {
      "epoch": 0.9440782479268552,
      "grad_norm": 1.7596776485443115,
      "learning_rate": 2.671969045571797e-06,
      "loss": 1.0881,
      "step": 2220
    },
    {
      "epoch": 0.9483308526472465,
      "grad_norm": 1.754442572593689,
      "learning_rate": 2.66122098022356e-06,
      "loss": 1.1099,
      "step": 2230
    },
    {
      "epoch": 0.9525834573676377,
      "grad_norm": 1.4423383474349976,
      "learning_rate": 2.6504729148753227e-06,
      "loss": 0.9526,
      "step": 2240
    },
    {
      "epoch": 0.956836062088029,
      "grad_norm": 2.0882155895233154,
      "learning_rate": 2.6397248495270855e-06,
      "loss": 1.0922,
      "step": 2250
    },
    {
      "epoch": 0.9610886668084202,
      "grad_norm": 1.3675830364227295,
      "learning_rate": 2.6289767841788483e-06,
      "loss": 0.9034,
      "step": 2260
    },
    {
      "epoch": 0.9653412715288114,
      "grad_norm": 1.2076600790023804,
      "learning_rate": 2.6182287188306106e-06,
      "loss": 0.9422,
      "step": 2270
    },
    {
      "epoch": 0.9695938762492027,
      "grad_norm": 1.6866154670715332,
      "learning_rate": 2.607480653482373e-06,
      "loss": 0.918,
      "step": 2280
    },
    {
      "epoch": 0.9738464809695939,
      "grad_norm": 2.717249631881714,
      "learning_rate": 2.596732588134136e-06,
      "loss": 0.892,
      "step": 2290
    },
    {
      "epoch": 0.9780990856899852,
      "grad_norm": 1.5299654006958008,
      "learning_rate": 2.5859845227858986e-06,
      "loss": 0.985,
      "step": 2300
    },
    {
      "epoch": 0.9823516904103764,
      "grad_norm": 1.342376708984375,
      "learning_rate": 2.5752364574376614e-06,
      "loss": 0.9764,
      "step": 2310
    },
    {
      "epoch": 0.9866042951307676,
      "grad_norm": 1.7525407075881958,
      "learning_rate": 2.5644883920894242e-06,
      "loss": 0.8834,
      "step": 2320
    },
    {
      "epoch": 0.9908568998511589,
      "grad_norm": 1.4590052366256714,
      "learning_rate": 2.553740326741187e-06,
      "loss": 1.0183,
      "step": 2330
    },
    {
      "epoch": 0.9951095045715501,
      "grad_norm": 1.8850669860839844,
      "learning_rate": 2.54299226139295e-06,
      "loss": 0.9867,
      "step": 2340
    },
    {
      "epoch": 0.9993621092919414,
      "grad_norm": 3.0287256240844727,
      "learning_rate": 2.532244196044712e-06,
      "loss": 0.9951,
      "step": 2350
    },
    {
      "epoch": 1.0036147140123326,
      "grad_norm": 1.2149666547775269,
      "learning_rate": 2.5214961306964746e-06,
      "loss": 1.0306,
      "step": 2360
    },
    {
      "epoch": 1.0078673187327238,
      "grad_norm": 1.3420071601867676,
      "learning_rate": 2.5107480653482374e-06,
      "loss": 0.9875,
      "step": 2370
    },
    {
      "epoch": 1.012119923453115,
      "grad_norm": 1.5649285316467285,
      "learning_rate": 2.5e-06,
      "loss": 0.9845,
      "step": 2380
    },
    {
      "epoch": 1.0163725281735063,
      "grad_norm": 1.392877221107483,
      "learning_rate": 2.489251934651763e-06,
      "loss": 0.9885,
      "step": 2390
    },
    {
      "epoch": 1.0206251328938976,
      "grad_norm": 1.4931155443191528,
      "learning_rate": 2.4785038693035254e-06,
      "loss": 0.9314,
      "step": 2400
    },
    {
      "epoch": 1.0248777376142888,
      "grad_norm": 1.3178669214248657,
      "learning_rate": 2.467755803955288e-06,
      "loss": 0.9394,
      "step": 2410
    },
    {
      "epoch": 1.02913034233468,
      "grad_norm": 1.3298419713974,
      "learning_rate": 2.457007738607051e-06,
      "loss": 0.9241,
      "step": 2420
    },
    {
      "epoch": 1.0333829470550713,
      "grad_norm": 1.6034588813781738,
      "learning_rate": 2.446259673258814e-06,
      "loss": 0.9632,
      "step": 2430
    },
    {
      "epoch": 1.0376355517754625,
      "grad_norm": 1.4393059015274048,
      "learning_rate": 2.435511607910576e-06,
      "loss": 1.034,
      "step": 2440
    },
    {
      "epoch": 1.0418881564958538,
      "grad_norm": 1.4727674722671509,
      "learning_rate": 2.424763542562339e-06,
      "loss": 1.1133,
      "step": 2450
    },
    {
      "epoch": 1.046140761216245,
      "grad_norm": 1.4437830448150635,
      "learning_rate": 2.4140154772141018e-06,
      "loss": 0.9628,
      "step": 2460
    },
    {
      "epoch": 1.0503933659366362,
      "grad_norm": 1.5536085367202759,
      "learning_rate": 2.4032674118658646e-06,
      "loss": 0.932,
      "step": 2470
    },
    {
      "epoch": 1.0546459706570275,
      "grad_norm": 1.3257462978363037,
      "learning_rate": 2.392519346517627e-06,
      "loss": 1.1522,
      "step": 2480
    },
    {
      "epoch": 1.0588985753774187,
      "grad_norm": 1.2802554368972778,
      "learning_rate": 2.3817712811693898e-06,
      "loss": 0.9367,
      "step": 2490
    },
    {
      "epoch": 1.06315118009781,
      "grad_norm": 1.266373872756958,
      "learning_rate": 2.3710232158211526e-06,
      "loss": 1.0236,
      "step": 2500
    },
    {
      "epoch": 1.0674037848182012,
      "grad_norm": 1.6188766956329346,
      "learning_rate": 2.3602751504729154e-06,
      "loss": 1.0432,
      "step": 2510
    },
    {
      "epoch": 1.0716563895385924,
      "grad_norm": 1.7325422763824463,
      "learning_rate": 2.3495270851246778e-06,
      "loss": 0.9515,
      "step": 2520
    },
    {
      "epoch": 1.0759089942589837,
      "grad_norm": 1.2430994510650635,
      "learning_rate": 2.3387790197764406e-06,
      "loss": 1.0164,
      "step": 2530
    },
    {
      "epoch": 1.080161598979375,
      "grad_norm": 1.7503633499145508,
      "learning_rate": 2.3280309544282034e-06,
      "loss": 1.0003,
      "step": 2540
    },
    {
      "epoch": 1.0844142036997662,
      "grad_norm": 1.883783221244812,
      "learning_rate": 2.3172828890799657e-06,
      "loss": 1.0094,
      "step": 2550
    },
    {
      "epoch": 1.0886668084201574,
      "grad_norm": 1.7863792181015015,
      "learning_rate": 2.3065348237317285e-06,
      "loss": 1.0393,
      "step": 2560
    },
    {
      "epoch": 1.0929194131405486,
      "grad_norm": 1.3729830980300903,
      "learning_rate": 2.295786758383491e-06,
      "loss": 0.973,
      "step": 2570
    },
    {
      "epoch": 1.0971720178609399,
      "grad_norm": 2.4355523586273193,
      "learning_rate": 2.2850386930352537e-06,
      "loss": 0.9605,
      "step": 2580
    },
    {
      "epoch": 1.1014246225813311,
      "grad_norm": 1.5216963291168213,
      "learning_rate": 2.2742906276870165e-06,
      "loss": 1.0212,
      "step": 2590
    },
    {
      "epoch": 1.1056772273017224,
      "grad_norm": 1.4391719102859497,
      "learning_rate": 2.263542562338779e-06,
      "loss": 0.9243,
      "step": 2600
    },
    {
      "epoch": 1.1099298320221136,
      "grad_norm": 1.3761900663375854,
      "learning_rate": 2.2527944969905417e-06,
      "loss": 0.9978,
      "step": 2610
    },
    {
      "epoch": 1.1141824367425048,
      "grad_norm": 1.3639401197433472,
      "learning_rate": 2.2420464316423045e-06,
      "loss": 1.07,
      "step": 2620
    },
    {
      "epoch": 1.118435041462896,
      "grad_norm": 1.3674675226211548,
      "learning_rate": 2.2312983662940673e-06,
      "loss": 1.1352,
      "step": 2630
    },
    {
      "epoch": 1.1226876461832873,
      "grad_norm": 1.2290161848068237,
      "learning_rate": 2.2205503009458297e-06,
      "loss": 1.0117,
      "step": 2640
    },
    {
      "epoch": 1.1269402509036786,
      "grad_norm": 1.4193332195281982,
      "learning_rate": 2.2098022355975925e-06,
      "loss": 1.1924,
      "step": 2650
    },
    {
      "epoch": 1.1311928556240698,
      "grad_norm": 2.4167542457580566,
      "learning_rate": 2.1990541702493553e-06,
      "loss": 1.1155,
      "step": 2660
    },
    {
      "epoch": 1.135445460344461,
      "grad_norm": 1.5931766033172607,
      "learning_rate": 2.188306104901118e-06,
      "loss": 1.164,
      "step": 2670
    },
    {
      "epoch": 1.1396980650648523,
      "grad_norm": 2.730811834335327,
      "learning_rate": 2.1775580395528805e-06,
      "loss": 1.0765,
      "step": 2680
    },
    {
      "epoch": 1.1439506697852435,
      "grad_norm": 1.4461387395858765,
      "learning_rate": 2.1668099742046433e-06,
      "loss": 1.0072,
      "step": 2690
    },
    {
      "epoch": 1.1482032745056348,
      "grad_norm": 1.7233009338378906,
      "learning_rate": 2.156061908856406e-06,
      "loss": 1.1272,
      "step": 2700
    },
    {
      "epoch": 1.152455879226026,
      "grad_norm": 1.411820888519287,
      "learning_rate": 2.1453138435081685e-06,
      "loss": 1.2233,
      "step": 2710
    },
    {
      "epoch": 1.1567084839464172,
      "grad_norm": 1.5155045986175537,
      "learning_rate": 2.1345657781599313e-06,
      "loss": 1.0324,
      "step": 2720
    },
    {
      "epoch": 1.1609610886668085,
      "grad_norm": 1.6780898571014404,
      "learning_rate": 2.123817712811694e-06,
      "loss": 0.9655,
      "step": 2730
    },
    {
      "epoch": 1.1652136933871997,
      "grad_norm": 1.4939903020858765,
      "learning_rate": 2.113069647463457e-06,
      "loss": 1.0072,
      "step": 2740
    },
    {
      "epoch": 1.169466298107591,
      "grad_norm": 1.617834448814392,
      "learning_rate": 2.1023215821152193e-06,
      "loss": 1.0337,
      "step": 2750
    },
    {
      "epoch": 1.1737189028279822,
      "grad_norm": 1.8560512065887451,
      "learning_rate": 2.091573516766982e-06,
      "loss": 0.9175,
      "step": 2760
    },
    {
      "epoch": 1.1779715075483734,
      "grad_norm": 1.467940330505371,
      "learning_rate": 2.080825451418745e-06,
      "loss": 1.0384,
      "step": 2770
    },
    {
      "epoch": 1.1822241122687647,
      "grad_norm": 1.4115936756134033,
      "learning_rate": 2.0700773860705077e-06,
      "loss": 0.9918,
      "step": 2780
    },
    {
      "epoch": 1.186476716989156,
      "grad_norm": 1.433537483215332,
      "learning_rate": 2.05932932072227e-06,
      "loss": 1.0446,
      "step": 2790
    },
    {
      "epoch": 1.1907293217095472,
      "grad_norm": 1.2687065601348877,
      "learning_rate": 2.048581255374033e-06,
      "loss": 1.0282,
      "step": 2800
    },
    {
      "epoch": 1.1949819264299384,
      "grad_norm": 1.296276330947876,
      "learning_rate": 2.0378331900257957e-06,
      "loss": 1.035,
      "step": 2810
    },
    {
      "epoch": 1.1992345311503296,
      "grad_norm": 1.4691842794418335,
      "learning_rate": 2.0270851246775585e-06,
      "loss": 0.9614,
      "step": 2820
    },
    {
      "epoch": 1.2034871358707209,
      "grad_norm": 1.323941946029663,
      "learning_rate": 2.016337059329321e-06,
      "loss": 0.9552,
      "step": 2830
    },
    {
      "epoch": 1.207739740591112,
      "grad_norm": 1.3758753538131714,
      "learning_rate": 2.0055889939810836e-06,
      "loss": 1.1206,
      "step": 2840
    },
    {
      "epoch": 1.2119923453115033,
      "grad_norm": 1.3175621032714844,
      "learning_rate": 1.9948409286328464e-06,
      "loss": 0.8727,
      "step": 2850
    },
    {
      "epoch": 1.2162449500318946,
      "grad_norm": 1.561316967010498,
      "learning_rate": 1.9840928632846092e-06,
      "loss": 1.0301,
      "step": 2860
    },
    {
      "epoch": 1.2204975547522858,
      "grad_norm": 1.5591446161270142,
      "learning_rate": 1.9733447979363716e-06,
      "loss": 0.8939,
      "step": 2870
    },
    {
      "epoch": 1.224750159472677,
      "grad_norm": 1.5170550346374512,
      "learning_rate": 1.9625967325881344e-06,
      "loss": 0.9307,
      "step": 2880
    },
    {
      "epoch": 1.2290027641930683,
      "grad_norm": 1.7244610786437988,
      "learning_rate": 1.951848667239897e-06,
      "loss": 0.9908,
      "step": 2890
    },
    {
      "epoch": 1.2332553689134595,
      "grad_norm": 1.5412750244140625,
      "learning_rate": 1.9411006018916596e-06,
      "loss": 0.9672,
      "step": 2900
    },
    {
      "epoch": 1.2375079736338508,
      "grad_norm": 1.2979854345321655,
      "learning_rate": 1.9303525365434224e-06,
      "loss": 0.9706,
      "step": 2910
    },
    {
      "epoch": 1.241760578354242,
      "grad_norm": 1.6226060390472412,
      "learning_rate": 1.919604471195185e-06,
      "loss": 0.8635,
      "step": 2920
    },
    {
      "epoch": 1.2460131830746333,
      "grad_norm": 1.4394071102142334,
      "learning_rate": 1.9088564058469476e-06,
      "loss": 1.0055,
      "step": 2930
    },
    {
      "epoch": 1.2502657877950245,
      "grad_norm": 1.7930508852005005,
      "learning_rate": 1.8981083404987106e-06,
      "loss": 0.923,
      "step": 2940
    },
    {
      "epoch": 1.2545183925154157,
      "grad_norm": 1.902482032775879,
      "learning_rate": 1.887360275150473e-06,
      "loss": 1.2077,
      "step": 2950
    },
    {
      "epoch": 1.258770997235807,
      "grad_norm": 1.2195649147033691,
      "learning_rate": 1.8766122098022358e-06,
      "loss": 0.9822,
      "step": 2960
    },
    {
      "epoch": 1.2630236019561982,
      "grad_norm": 1.377030849456787,
      "learning_rate": 1.8658641444539986e-06,
      "loss": 1.0919,
      "step": 2970
    },
    {
      "epoch": 1.2672762066765895,
      "grad_norm": 1.6835565567016602,
      "learning_rate": 1.8551160791057612e-06,
      "loss": 0.8829,
      "step": 2980
    },
    {
      "epoch": 1.2715288113969807,
      "grad_norm": 1.2820864915847778,
      "learning_rate": 1.8443680137575238e-06,
      "loss": 0.9527,
      "step": 2990
    },
    {
      "epoch": 1.275781416117372,
      "grad_norm": 1.5378886461257935,
      "learning_rate": 1.8336199484092864e-06,
      "loss": 1.1363,
      "step": 3000
    },
    {
      "epoch": 1.2800340208377632,
      "grad_norm": 1.7446327209472656,
      "learning_rate": 1.8228718830610492e-06,
      "loss": 1.0706,
      "step": 3010
    },
    {
      "epoch": 1.2842866255581544,
      "grad_norm": 1.3089995384216309,
      "learning_rate": 1.8121238177128118e-06,
      "loss": 0.9586,
      "step": 3020
    },
    {
      "epoch": 1.2885392302785457,
      "grad_norm": 1.2894452810287476,
      "learning_rate": 1.8013757523645744e-06,
      "loss": 0.9308,
      "step": 3030
    },
    {
      "epoch": 1.292791834998937,
      "grad_norm": 1.407439947128296,
      "learning_rate": 1.7906276870163372e-06,
      "loss": 1.0592,
      "step": 3040
    },
    {
      "epoch": 1.2970444397193281,
      "grad_norm": 1.930071234703064,
      "learning_rate": 1.7798796216681e-06,
      "loss": 0.9245,
      "step": 3050
    },
    {
      "epoch": 1.3012970444397194,
      "grad_norm": 1.5816007852554321,
      "learning_rate": 1.7691315563198623e-06,
      "loss": 0.9414,
      "step": 3060
    },
    {
      "epoch": 1.3055496491601106,
      "grad_norm": 2.54315447807312,
      "learning_rate": 1.7583834909716251e-06,
      "loss": 1.0807,
      "step": 3070
    },
    {
      "epoch": 1.3098022538805019,
      "grad_norm": 1.7038352489471436,
      "learning_rate": 1.747635425623388e-06,
      "loss": 0.8777,
      "step": 3080
    },
    {
      "epoch": 1.314054858600893,
      "grad_norm": 1.7983787059783936,
      "learning_rate": 1.7368873602751508e-06,
      "loss": 0.997,
      "step": 3090
    },
    {
      "epoch": 1.3183074633212843,
      "grad_norm": 1.3142404556274414,
      "learning_rate": 1.7261392949269131e-06,
      "loss": 0.9635,
      "step": 3100
    },
    {
      "epoch": 1.3225600680416756,
      "grad_norm": 1.872833251953125,
      "learning_rate": 1.715391229578676e-06,
      "loss": 1.0013,
      "step": 3110
    },
    {
      "epoch": 1.3268126727620668,
      "grad_norm": 1.4787882566452026,
      "learning_rate": 1.7046431642304387e-06,
      "loss": 0.9918,
      "step": 3120
    },
    {
      "epoch": 1.331065277482458,
      "grad_norm": 1.398829698562622,
      "learning_rate": 1.6938950988822015e-06,
      "loss": 0.887,
      "step": 3130
    },
    {
      "epoch": 1.3353178822028493,
      "grad_norm": 2.268230676651001,
      "learning_rate": 1.683147033533964e-06,
      "loss": 1.0408,
      "step": 3140
    },
    {
      "epoch": 1.3395704869232405,
      "grad_norm": 2.263721227645874,
      "learning_rate": 1.6723989681857267e-06,
      "loss": 1.0217,
      "step": 3150
    },
    {
      "epoch": 1.3438230916436318,
      "grad_norm": 1.3448642492294312,
      "learning_rate": 1.6616509028374893e-06,
      "loss": 0.844,
      "step": 3160
    },
    {
      "epoch": 1.348075696364023,
      "grad_norm": 1.4318172931671143,
      "learning_rate": 1.6509028374892521e-06,
      "loss": 0.9819,
      "step": 3170
    },
    {
      "epoch": 1.3523283010844143,
      "grad_norm": 7.801759243011475,
      "learning_rate": 1.6401547721410147e-06,
      "loss": 0.9098,
      "step": 3180
    },
    {
      "epoch": 1.3565809058048055,
      "grad_norm": 1.4793040752410889,
      "learning_rate": 1.6294067067927773e-06,
      "loss": 0.9802,
      "step": 3190
    },
    {
      "epoch": 1.3608335105251967,
      "grad_norm": 1.1508584022521973,
      "learning_rate": 1.6186586414445401e-06,
      "loss": 0.9157,
      "step": 3200
    },
    {
      "epoch": 1.365086115245588,
      "grad_norm": 1.5101956129074097,
      "learning_rate": 1.607910576096303e-06,
      "loss": 1.0504,
      "step": 3210
    },
    {
      "epoch": 1.3693387199659792,
      "grad_norm": 1.6027525663375854,
      "learning_rate": 1.5971625107480653e-06,
      "loss": 0.9443,
      "step": 3220
    },
    {
      "epoch": 1.3735913246863705,
      "grad_norm": 1.5778106451034546,
      "learning_rate": 1.586414445399828e-06,
      "loss": 0.9405,
      "step": 3230
    },
    {
      "epoch": 1.3778439294067617,
      "grad_norm": 1.698789119720459,
      "learning_rate": 1.575666380051591e-06,
      "loss": 0.9957,
      "step": 3240
    },
    {
      "epoch": 1.382096534127153,
      "grad_norm": 1.649829387664795,
      "learning_rate": 1.5649183147033537e-06,
      "loss": 1.1108,
      "step": 3250
    },
    {
      "epoch": 1.3863491388475442,
      "grad_norm": 1.759988784790039,
      "learning_rate": 1.554170249355116e-06,
      "loss": 0.9769,
      "step": 3260
    },
    {
      "epoch": 1.3906017435679354,
      "grad_norm": 1.5956087112426758,
      "learning_rate": 1.5434221840068789e-06,
      "loss": 1.0469,
      "step": 3270
    },
    {
      "epoch": 1.3948543482883267,
      "grad_norm": 1.5723100900650024,
      "learning_rate": 1.5326741186586417e-06,
      "loss": 0.8747,
      "step": 3280
    },
    {
      "epoch": 1.399106953008718,
      "grad_norm": 1.4569915533065796,
      "learning_rate": 1.5219260533104045e-06,
      "loss": 1.0919,
      "step": 3290
    },
    {
      "epoch": 1.4033595577291091,
      "grad_norm": 2.4111475944519043,
      "learning_rate": 1.5111779879621669e-06,
      "loss": 0.9763,
      "step": 3300
    },
    {
      "epoch": 1.4076121624495004,
      "grad_norm": 1.5326216220855713,
      "learning_rate": 1.5004299226139297e-06,
      "loss": 0.8708,
      "step": 3310
    },
    {
      "epoch": 1.4118647671698916,
      "grad_norm": 1.7427352666854858,
      "learning_rate": 1.4896818572656923e-06,
      "loss": 1.2239,
      "step": 3320
    },
    {
      "epoch": 1.4161173718902829,
      "grad_norm": 1.528483271598816,
      "learning_rate": 1.4789337919174549e-06,
      "loss": 0.8637,
      "step": 3330
    },
    {
      "epoch": 1.420369976610674,
      "grad_norm": 1.5154283046722412,
      "learning_rate": 1.4681857265692177e-06,
      "loss": 1.0052,
      "step": 3340
    },
    {
      "epoch": 1.4246225813310653,
      "grad_norm": 1.46908700466156,
      "learning_rate": 1.4574376612209802e-06,
      "loss": 1.0308,
      "step": 3350
    },
    {
      "epoch": 1.4288751860514566,
      "grad_norm": 1.3951436281204224,
      "learning_rate": 1.446689595872743e-06,
      "loss": 0.9535,
      "step": 3360
    },
    {
      "epoch": 1.4331277907718478,
      "grad_norm": 1.589127540588379,
      "learning_rate": 1.4359415305245056e-06,
      "loss": 0.9959,
      "step": 3370
    },
    {
      "epoch": 1.437380395492239,
      "grad_norm": 1.228898525238037,
      "learning_rate": 1.4251934651762682e-06,
      "loss": 0.9871,
      "step": 3380
    },
    {
      "epoch": 1.4416330002126303,
      "grad_norm": 1.9320921897888184,
      "learning_rate": 1.414445399828031e-06,
      "loss": 0.951,
      "step": 3390
    },
    {
      "epoch": 1.4458856049330215,
      "grad_norm": 1.7307136058807373,
      "learning_rate": 1.4036973344797938e-06,
      "loss": 0.8733,
      "step": 3400
    },
    {
      "epoch": 1.4501382096534128,
      "grad_norm": 1.6700987815856934,
      "learning_rate": 1.3929492691315562e-06,
      "loss": 0.9346,
      "step": 3410
    },
    {
      "epoch": 1.454390814373804,
      "grad_norm": 1.4160131216049194,
      "learning_rate": 1.382201203783319e-06,
      "loss": 0.9286,
      "step": 3420
    },
    {
      "epoch": 1.4586434190941953,
      "grad_norm": 1.5732083320617676,
      "learning_rate": 1.3714531384350818e-06,
      "loss": 0.9846,
      "step": 3430
    },
    {
      "epoch": 1.4628960238145865,
      "grad_norm": 2.2969796657562256,
      "learning_rate": 1.3607050730868446e-06,
      "loss": 0.9253,
      "step": 3440
    },
    {
      "epoch": 1.4671486285349777,
      "grad_norm": 1.5147937536239624,
      "learning_rate": 1.349957007738607e-06,
      "loss": 0.9964,
      "step": 3450
    },
    {
      "epoch": 1.471401233255369,
      "grad_norm": 4.542707443237305,
      "learning_rate": 1.3392089423903698e-06,
      "loss": 0.9775,
      "step": 3460
    },
    {
      "epoch": 1.4756538379757602,
      "grad_norm": 1.3579835891723633,
      "learning_rate": 1.3284608770421326e-06,
      "loss": 0.8872,
      "step": 3470
    },
    {
      "epoch": 1.4799064426961515,
      "grad_norm": 1.4123613834381104,
      "learning_rate": 1.3177128116938952e-06,
      "loss": 0.9653,
      "step": 3480
    },
    {
      "epoch": 1.4841590474165427,
      "grad_norm": 2.069103240966797,
      "learning_rate": 1.3069647463456578e-06,
      "loss": 0.941,
      "step": 3490
    },
    {
      "epoch": 1.488411652136934,
      "grad_norm": 1.3783169984817505,
      "learning_rate": 1.2962166809974206e-06,
      "loss": 0.9148,
      "step": 3500
    },
    {
      "epoch": 1.4926642568573252,
      "grad_norm": 1.5446046590805054,
      "learning_rate": 1.2854686156491832e-06,
      "loss": 1.1052,
      "step": 3510
    },
    {
      "epoch": 1.4969168615777164,
      "grad_norm": 3.144028425216675,
      "learning_rate": 1.274720550300946e-06,
      "loss": 0.8469,
      "step": 3520
    },
    {
      "epoch": 1.5011694662981077,
      "grad_norm": 1.4095379114151,
      "learning_rate": 1.2639724849527086e-06,
      "loss": 1.0759,
      "step": 3530
    },
    {
      "epoch": 1.505422071018499,
      "grad_norm": 1.7476601600646973,
      "learning_rate": 1.2532244196044712e-06,
      "loss": 0.9483,
      "step": 3540
    },
    {
      "epoch": 1.5096746757388901,
      "grad_norm": 2.1252191066741943,
      "learning_rate": 1.242476354256234e-06,
      "loss": 0.8918,
      "step": 3550
    },
    {
      "epoch": 1.5139272804592814,
      "grad_norm": 1.8176875114440918,
      "learning_rate": 1.2317282889079966e-06,
      "loss": 1.0013,
      "step": 3560
    },
    {
      "epoch": 1.5181798851796726,
      "grad_norm": 1.8837480545043945,
      "learning_rate": 1.2209802235597594e-06,
      "loss": 0.9072,
      "step": 3570
    },
    {
      "epoch": 1.5224324899000639,
      "grad_norm": 1.6903249025344849,
      "learning_rate": 1.210232158211522e-06,
      "loss": 1.014,
      "step": 3580
    },
    {
      "epoch": 1.526685094620455,
      "grad_norm": 1.474450945854187,
      "learning_rate": 1.1994840928632848e-06,
      "loss": 0.9896,
      "step": 3590
    },
    {
      "epoch": 1.5309376993408463,
      "grad_norm": 1.3615893125534058,
      "learning_rate": 1.1887360275150474e-06,
      "loss": 0.9567,
      "step": 3600
    },
    {
      "epoch": 1.5351903040612376,
      "grad_norm": 2.3294107913970947,
      "learning_rate": 1.1779879621668102e-06,
      "loss": 0.9454,
      "step": 3610
    },
    {
      "epoch": 1.5394429087816288,
      "grad_norm": 1.6980220079421997,
      "learning_rate": 1.1672398968185728e-06,
      "loss": 0.9677,
      "step": 3620
    },
    {
      "epoch": 1.54369551350202,
      "grad_norm": 1.4640250205993652,
      "learning_rate": 1.1564918314703356e-06,
      "loss": 1.0213,
      "step": 3630
    },
    {
      "epoch": 1.5479481182224113,
      "grad_norm": 1.9600410461425781,
      "learning_rate": 1.1457437661220981e-06,
      "loss": 0.8573,
      "step": 3640
    },
    {
      "epoch": 1.5522007229428025,
      "grad_norm": 1.7157752513885498,
      "learning_rate": 1.1349957007738607e-06,
      "loss": 1.0802,
      "step": 3650
    },
    {
      "epoch": 1.5564533276631938,
      "grad_norm": 1.7602790594100952,
      "learning_rate": 1.1242476354256235e-06,
      "loss": 0.9638,
      "step": 3660
    },
    {
      "epoch": 1.560705932383585,
      "grad_norm": 1.9524339437484741,
      "learning_rate": 1.1134995700773861e-06,
      "loss": 1.0954,
      "step": 3670
    },
    {
      "epoch": 1.5649585371039763,
      "grad_norm": 1.4548205137252808,
      "learning_rate": 1.102751504729149e-06,
      "loss": 0.9402,
      "step": 3680
    },
    {
      "epoch": 1.5692111418243675,
      "grad_norm": 1.691915512084961,
      "learning_rate": 1.0920034393809115e-06,
      "loss": 1.034,
      "step": 3690
    },
    {
      "epoch": 1.5734637465447587,
      "grad_norm": 1.3535759449005127,
      "learning_rate": 1.0812553740326741e-06,
      "loss": 0.9818,
      "step": 3700
    },
    {
      "epoch": 1.57771635126515,
      "grad_norm": 1.7225452661514282,
      "learning_rate": 1.070507308684437e-06,
      "loss": 0.9798,
      "step": 3710
    },
    {
      "epoch": 1.5819689559855412,
      "grad_norm": 1.345123529434204,
      "learning_rate": 1.0597592433361995e-06,
      "loss": 0.9754,
      "step": 3720
    },
    {
      "epoch": 1.5862215607059325,
      "grad_norm": 8.304523468017578,
      "learning_rate": 1.0490111779879621e-06,
      "loss": 1.0332,
      "step": 3730
    },
    {
      "epoch": 1.5904741654263237,
      "grad_norm": 1.5375206470489502,
      "learning_rate": 1.038263112639725e-06,
      "loss": 0.853,
      "step": 3740
    },
    {
      "epoch": 1.594726770146715,
      "grad_norm": 1.3879014253616333,
      "learning_rate": 1.0275150472914875e-06,
      "loss": 1.0205,
      "step": 3750
    },
    {
      "epoch": 1.5989793748671062,
      "grad_norm": 1.735459566116333,
      "learning_rate": 1.0167669819432503e-06,
      "loss": 1.0278,
      "step": 3760
    },
    {
      "epoch": 1.6032319795874974,
      "grad_norm": 1.3345351219177246,
      "learning_rate": 1.006018916595013e-06,
      "loss": 0.8665,
      "step": 3770
    },
    {
      "epoch": 1.6074845843078887,
      "grad_norm": 1.8745843172073364,
      "learning_rate": 9.952708512467757e-07,
      "loss": 0.9908,
      "step": 3780
    },
    {
      "epoch": 1.61173718902828,
      "grad_norm": 1.767360806465149,
      "learning_rate": 9.845227858985383e-07,
      "loss": 0.7852,
      "step": 3790
    },
    {
      "epoch": 1.6159897937486711,
      "grad_norm": 1.501900553703308,
      "learning_rate": 9.73774720550301e-07,
      "loss": 0.9586,
      "step": 3800
    },
    {
      "epoch": 1.6202423984690624,
      "grad_norm": 1.4393938779830933,
      "learning_rate": 9.630266552020637e-07,
      "loss": 0.9292,
      "step": 3810
    },
    {
      "epoch": 1.6244950031894536,
      "grad_norm": 1.4520814418792725,
      "learning_rate": 9.522785898538264e-07,
      "loss": 0.8793,
      "step": 3820
    },
    {
      "epoch": 1.6287476079098449,
      "grad_norm": 1.6602604389190674,
      "learning_rate": 9.415305245055891e-07,
      "loss": 0.8907,
      "step": 3830
    },
    {
      "epoch": 1.633000212630236,
      "grad_norm": 1.5946242809295654,
      "learning_rate": 9.307824591573518e-07,
      "loss": 0.8761,
      "step": 3840
    },
    {
      "epoch": 1.6372528173506273,
      "grad_norm": 1.617666244506836,
      "learning_rate": 9.200343938091144e-07,
      "loss": 1.0318,
      "step": 3850
    },
    {
      "epoch": 1.6415054220710186,
      "grad_norm": 1.321639060974121,
      "learning_rate": 9.092863284608772e-07,
      "loss": 1.0094,
      "step": 3860
    },
    {
      "epoch": 1.6457580267914098,
      "grad_norm": 1.3987364768981934,
      "learning_rate": 8.985382631126398e-07,
      "loss": 0.8785,
      "step": 3870
    },
    {
      "epoch": 1.650010631511801,
      "grad_norm": 1.3593353033065796,
      "learning_rate": 8.877901977644026e-07,
      "loss": 0.8762,
      "step": 3880
    },
    {
      "epoch": 1.6542632362321923,
      "grad_norm": 1.7206809520721436,
      "learning_rate": 8.770421324161652e-07,
      "loss": 1.0828,
      "step": 3890
    },
    {
      "epoch": 1.6585158409525835,
      "grad_norm": 1.3659262657165527,
      "learning_rate": 8.662940670679279e-07,
      "loss": 0.9339,
      "step": 3900
    },
    {
      "epoch": 1.6627684456729748,
      "grad_norm": 1.6886954307556152,
      "learning_rate": 8.555460017196906e-07,
      "loss": 0.9686,
      "step": 3910
    },
    {
      "epoch": 1.667021050393366,
      "grad_norm": 1.6662136316299438,
      "learning_rate": 8.447979363714532e-07,
      "loss": 0.9475,
      "step": 3920
    },
    {
      "epoch": 1.6712736551137573,
      "grad_norm": 1.6479873657226562,
      "learning_rate": 8.340498710232158e-07,
      "loss": 1.0727,
      "step": 3930
    },
    {
      "epoch": 1.6755262598341485,
      "grad_norm": 1.6547415256500244,
      "learning_rate": 8.233018056749786e-07,
      "loss": 0.7827,
      "step": 3940
    },
    {
      "epoch": 1.6797788645545397,
      "grad_norm": 1.9899368286132812,
      "learning_rate": 8.125537403267412e-07,
      "loss": 0.9918,
      "step": 3950
    },
    {
      "epoch": 1.684031469274931,
      "grad_norm": 1.5952167510986328,
      "learning_rate": 8.018056749785038e-07,
      "loss": 0.8572,
      "step": 3960
    },
    {
      "epoch": 1.6882840739953222,
      "grad_norm": 1.5207351446151733,
      "learning_rate": 7.910576096302666e-07,
      "loss": 1.0033,
      "step": 3970
    },
    {
      "epoch": 1.6925366787157134,
      "grad_norm": 1.4404758214950562,
      "learning_rate": 7.803095442820292e-07,
      "loss": 1.0334,
      "step": 3980
    },
    {
      "epoch": 1.6967892834361047,
      "grad_norm": 1.4936792850494385,
      "learning_rate": 7.69561478933792e-07,
      "loss": 0.8759,
      "step": 3990
    },
    {
      "epoch": 1.701041888156496,
      "grad_norm": 1.6111656427383423,
      "learning_rate": 7.588134135855546e-07,
      "loss": 1.0011,
      "step": 4000
    },
    {
      "epoch": 1.7052944928768872,
      "grad_norm": 1.5022116899490356,
      "learning_rate": 7.480653482373173e-07,
      "loss": 1.024,
      "step": 4010
    },
    {
      "epoch": 1.7095470975972784,
      "grad_norm": 1.5959672927856445,
      "learning_rate": 7.3731728288908e-07,
      "loss": 0.9845,
      "step": 4020
    },
    {
      "epoch": 1.7137997023176696,
      "grad_norm": 1.2814629077911377,
      "learning_rate": 7.265692175408427e-07,
      "loss": 0.9156,
      "step": 4030
    },
    {
      "epoch": 1.7180523070380609,
      "grad_norm": 1.236923098564148,
      "learning_rate": 7.158211521926053e-07,
      "loss": 0.8712,
      "step": 4040
    },
    {
      "epoch": 1.7223049117584521,
      "grad_norm": 1.8119351863861084,
      "learning_rate": 7.050730868443681e-07,
      "loss": 0.9947,
      "step": 4050
    },
    {
      "epoch": 1.7265575164788434,
      "grad_norm": 2.264273166656494,
      "learning_rate": 6.943250214961307e-07,
      "loss": 0.9214,
      "step": 4060
    },
    {
      "epoch": 1.7308101211992346,
      "grad_norm": 1.281649112701416,
      "learning_rate": 6.835769561478935e-07,
      "loss": 0.9805,
      "step": 4070
    },
    {
      "epoch": 1.7350627259196258,
      "grad_norm": 1.8138867616653442,
      "learning_rate": 6.728288907996561e-07,
      "loss": 1.0991,
      "step": 4080
    },
    {
      "epoch": 1.739315330640017,
      "grad_norm": 1.5580331087112427,
      "learning_rate": 6.620808254514188e-07,
      "loss": 0.8859,
      "step": 4090
    },
    {
      "epoch": 1.7435679353604083,
      "grad_norm": 1.8609890937805176,
      "learning_rate": 6.513327601031815e-07,
      "loss": 1.0202,
      "step": 4100
    },
    {
      "epoch": 1.7478205400807996,
      "grad_norm": 1.3044061660766602,
      "learning_rate": 6.405846947549442e-07,
      "loss": 0.8348,
      "step": 4110
    },
    {
      "epoch": 1.7520731448011908,
      "grad_norm": 1.85099458694458,
      "learning_rate": 6.298366294067068e-07,
      "loss": 0.9492,
      "step": 4120
    },
    {
      "epoch": 1.756325749521582,
      "grad_norm": 1.2027620077133179,
      "learning_rate": 6.190885640584695e-07,
      "loss": 0.9594,
      "step": 4130
    },
    {
      "epoch": 1.7605783542419733,
      "grad_norm": 1.6729265451431274,
      "learning_rate": 6.083404987102322e-07,
      "loss": 0.8416,
      "step": 4140
    },
    {
      "epoch": 1.7648309589623645,
      "grad_norm": 1.5136055946350098,
      "learning_rate": 5.975924333619949e-07,
      "loss": 0.9122,
      "step": 4150
    },
    {
      "epoch": 1.7690835636827558,
      "grad_norm": 1.3854923248291016,
      "learning_rate": 5.868443680137576e-07,
      "loss": 0.9106,
      "step": 4160
    },
    {
      "epoch": 1.773336168403147,
      "grad_norm": 1.5651670694351196,
      "learning_rate": 5.760963026655203e-07,
      "loss": 0.8971,
      "step": 4170
    },
    {
      "epoch": 1.7775887731235382,
      "grad_norm": 9.048185348510742,
      "learning_rate": 5.65348237317283e-07,
      "loss": 0.799,
      "step": 4180
    },
    {
      "epoch": 1.7818413778439295,
      "grad_norm": 1.7110873460769653,
      "learning_rate": 5.546001719690456e-07,
      "loss": 0.9942,
      "step": 4190
    },
    {
      "epoch": 1.7860939825643207,
      "grad_norm": 1.6559580564498901,
      "learning_rate": 5.438521066208082e-07,
      "loss": 0.8708,
      "step": 4200
    },
    {
      "epoch": 1.790346587284712,
      "grad_norm": 1.2340373992919922,
      "learning_rate": 5.331040412725709e-07,
      "loss": 1.037,
      "step": 4210
    },
    {
      "epoch": 1.7945991920051032,
      "grad_norm": 1.3801536560058594,
      "learning_rate": 5.223559759243336e-07,
      "loss": 0.9188,
      "step": 4220
    },
    {
      "epoch": 1.7988517967254944,
      "grad_norm": 2.2161214351654053,
      "learning_rate": 5.116079105760963e-07,
      "loss": 0.984,
      "step": 4230
    },
    {
      "epoch": 1.8031044014458857,
      "grad_norm": 1.4954283237457275,
      "learning_rate": 5.00859845227859e-07,
      "loss": 1.1264,
      "step": 4240
    },
    {
      "epoch": 1.807357006166277,
      "grad_norm": 2.407176971435547,
      "learning_rate": 4.901117798796217e-07,
      "loss": 0.8084,
      "step": 4250
    },
    {
      "epoch": 1.8116096108866682,
      "grad_norm": 1.5291213989257812,
      "learning_rate": 4.793637145313844e-07,
      "loss": 0.975,
      "step": 4260
    },
    {
      "epoch": 1.8158622156070594,
      "grad_norm": 1.448801875114441,
      "learning_rate": 4.6861564918314707e-07,
      "loss": 1.0371,
      "step": 4270
    },
    {
      "epoch": 1.8201148203274506,
      "grad_norm": 1.5390878915786743,
      "learning_rate": 4.5786758383490977e-07,
      "loss": 0.9105,
      "step": 4280
    },
    {
      "epoch": 1.8243674250478419,
      "grad_norm": 1.9247814416885376,
      "learning_rate": 4.4711951848667246e-07,
      "loss": 1.0441,
      "step": 4290
    },
    {
      "epoch": 1.8286200297682331,
      "grad_norm": 1.8494752645492554,
      "learning_rate": 4.363714531384351e-07,
      "loss": 0.9093,
      "step": 4300
    },
    {
      "epoch": 1.8328726344886244,
      "grad_norm": 1.2683863639831543,
      "learning_rate": 4.256233877901978e-07,
      "loss": 0.9159,
      "step": 4310
    },
    {
      "epoch": 1.8371252392090156,
      "grad_norm": 1.80696439743042,
      "learning_rate": 4.148753224419605e-07,
      "loss": 0.9148,
      "step": 4320
    },
    {
      "epoch": 1.8413778439294068,
      "grad_norm": 1.2626075744628906,
      "learning_rate": 4.041272570937232e-07,
      "loss": 0.9574,
      "step": 4330
    },
    {
      "epoch": 1.845630448649798,
      "grad_norm": 1.5570592880249023,
      "learning_rate": 3.9337919174548584e-07,
      "loss": 0.8988,
      "step": 4340
    },
    {
      "epoch": 1.8498830533701893,
      "grad_norm": 1.3914422988891602,
      "learning_rate": 3.8263112639724854e-07,
      "loss": 0.9651,
      "step": 4350
    },
    {
      "epoch": 1.8541356580905806,
      "grad_norm": 1.3893778324127197,
      "learning_rate": 3.7188306104901124e-07,
      "loss": 0.9338,
      "step": 4360
    },
    {
      "epoch": 1.8583882628109718,
      "grad_norm": 1.761635661125183,
      "learning_rate": 3.6113499570077393e-07,
      "loss": 0.9508,
      "step": 4370
    },
    {
      "epoch": 1.862640867531363,
      "grad_norm": 1.5152661800384521,
      "learning_rate": 3.503869303525366e-07,
      "loss": 0.812,
      "step": 4380
    },
    {
      "epoch": 1.8668934722517543,
      "grad_norm": 2.07291316986084,
      "learning_rate": 3.396388650042993e-07,
      "loss": 1.0664,
      "step": 4390
    },
    {
      "epoch": 1.8711460769721455,
      "grad_norm": 1.4721473455429077,
      "learning_rate": 3.2889079965606197e-07,
      "loss": 0.9957,
      "step": 4400
    },
    {
      "epoch": 1.8753986816925368,
      "grad_norm": 1.3821461200714111,
      "learning_rate": 3.1814273430782467e-07,
      "loss": 1.1404,
      "step": 4410
    },
    {
      "epoch": 1.879651286412928,
      "grad_norm": 1.432856798171997,
      "learning_rate": 3.073946689595873e-07,
      "loss": 1.0052,
      "step": 4420
    },
    {
      "epoch": 1.8839038911333192,
      "grad_norm": 1.6785396337509155,
      "learning_rate": 2.9664660361134996e-07,
      "loss": 1.0408,
      "step": 4430
    },
    {
      "epoch": 1.8881564958537105,
      "grad_norm": 1.7781394720077515,
      "learning_rate": 2.8589853826311266e-07,
      "loss": 1.0442,
      "step": 4440
    },
    {
      "epoch": 1.8924091005741017,
      "grad_norm": 1.476549744606018,
      "learning_rate": 2.7515047291487535e-07,
      "loss": 1.0671,
      "step": 4450
    },
    {
      "epoch": 1.896661705294493,
      "grad_norm": 1.619120478630066,
      "learning_rate": 2.6440240756663805e-07,
      "loss": 1.1353,
      "step": 4460
    },
    {
      "epoch": 1.9009143100148842,
      "grad_norm": 2.2543396949768066,
      "learning_rate": 2.536543422184007e-07,
      "loss": 0.841,
      "step": 4470
    },
    {
      "epoch": 1.9051669147352754,
      "grad_norm": 1.6270688772201538,
      "learning_rate": 2.429062768701634e-07,
      "loss": 1.0594,
      "step": 4480
    },
    {
      "epoch": 1.9094195194556667,
      "grad_norm": 1.5027445554733276,
      "learning_rate": 2.321582115219261e-07,
      "loss": 0.9794,
      "step": 4490
    },
    {
      "epoch": 1.913672124176058,
      "grad_norm": 1.7243256568908691,
      "learning_rate": 2.2141014617368876e-07,
      "loss": 1.1105,
      "step": 4500
    },
    {
      "epoch": 1.9179247288964492,
      "grad_norm": 2.0413265228271484,
      "learning_rate": 2.1066208082545143e-07,
      "loss": 1.1351,
      "step": 4510
    },
    {
      "epoch": 1.9221773336168404,
      "grad_norm": 1.4429795742034912,
      "learning_rate": 1.999140154772141e-07,
      "loss": 0.9292,
      "step": 4520
    },
    {
      "epoch": 1.9264299383372316,
      "grad_norm": 1.4349234104156494,
      "learning_rate": 1.891659501289768e-07,
      "loss": 0.9106,
      "step": 4530
    },
    {
      "epoch": 1.9306825430576229,
      "grad_norm": 1.8514044284820557,
      "learning_rate": 1.7841788478073947e-07,
      "loss": 1.0167,
      "step": 4540
    },
    {
      "epoch": 1.9349351477780141,
      "grad_norm": 1.3695228099822998,
      "learning_rate": 1.6766981943250217e-07,
      "loss": 0.8232,
      "step": 4550
    },
    {
      "epoch": 1.9391877524984054,
      "grad_norm": 1.6903694868087769,
      "learning_rate": 1.5692175408426484e-07,
      "loss": 0.847,
      "step": 4560
    },
    {
      "epoch": 1.9434403572187966,
      "grad_norm": 1.3639529943466187,
      "learning_rate": 1.4617368873602753e-07,
      "loss": 1.0056,
      "step": 4570
    },
    {
      "epoch": 1.9476929619391878,
      "grad_norm": 1.5749311447143555,
      "learning_rate": 1.354256233877902e-07,
      "loss": 0.9138,
      "step": 4580
    },
    {
      "epoch": 1.951945566659579,
      "grad_norm": 2.3607659339904785,
      "learning_rate": 1.246775580395529e-07,
      "loss": 0.8744,
      "step": 4590
    },
    {
      "epoch": 1.9561981713799703,
      "grad_norm": 1.5424456596374512,
      "learning_rate": 1.1392949269131557e-07,
      "loss": 1.036,
      "step": 4600
    },
    {
      "epoch": 1.9604507761003616,
      "grad_norm": 1.413935661315918,
      "learning_rate": 1.0318142734307826e-07,
      "loss": 1.017,
      "step": 4610
    },
    {
      "epoch": 1.9647033808207528,
      "grad_norm": 1.9706467390060425,
      "learning_rate": 9.243336199484093e-08,
      "loss": 1.0016,
      "step": 4620
    },
    {
      "epoch": 1.968955985541144,
      "grad_norm": 1.771653175354004,
      "learning_rate": 8.168529664660361e-08,
      "loss": 0.9486,
      "step": 4630
    },
    {
      "epoch": 1.9732085902615353,
      "grad_norm": 1.430317759513855,
      "learning_rate": 7.09372312983663e-08,
      "loss": 1.023,
      "step": 4640
    },
    {
      "epoch": 1.9774611949819265,
      "grad_norm": 1.5239413976669312,
      "learning_rate": 6.018916595012898e-08,
      "loss": 1.1048,
      "step": 4650
    },
    {
      "epoch": 1.9817137997023178,
      "grad_norm": 1.7959420680999756,
      "learning_rate": 4.9441100601891664e-08,
      "loss": 0.8963,
      "step": 4660
    },
    {
      "epoch": 1.985966404422709,
      "grad_norm": 1.9004325866699219,
      "learning_rate": 3.869303525365434e-08,
      "loss": 0.8133,
      "step": 4670
    },
    {
      "epoch": 1.9902190091431002,
      "grad_norm": 1.3611100912094116,
      "learning_rate": 2.794496990541703e-08,
      "loss": 0.8373,
      "step": 4680
    },
    {
      "epoch": 1.9944716138634915,
      "grad_norm": 1.4089275598526,
      "learning_rate": 1.719690455717971e-08,
      "loss": 0.9227,
      "step": 4690
    },
    {
      "epoch": 1.9987242185838827,
      "grad_norm": 1.6821610927581787,
      "learning_rate": 6.448839208942391e-09,
      "loss": 0.961,
      "step": 4700
    }
  ],
  "logging_steps": 10,
  "max_steps": 4702,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.6053436013346816e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
